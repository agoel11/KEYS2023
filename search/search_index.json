{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Author: Atharva Goel Advisor: Nirav C Merchant About \u00b6 Atharva Goel is a rising Senior at Paradise Valley High School in Phoenix, Arizona. During the summer of 2023 he is an intern at The University of Arizona through the BIO5 Institute's KEYS program. This summer he is an intern working at The University of Arizona in Dr. Nirav C Merchant's lab: a member of CyVerse , a cutting edge cyberinfrastructure funded by the National Science Foundation that is designed for research and committed to the principles of open science. This website follows the FAIR and CARE data principles and hopes to help further open science. Table of Contents \u00b6 Introduction Logbook Assignments Jupyter Notebooks GitHub Actions Project Notes KEYS 2023 Main Project Results References GitHub Actions \u00b6 GitHub Education Access Project Notes \u00b6 UNIX Shell Version Control with Git Containers KEYS 2023 Main Project \u00b6 My Project Prior Research/ Fact Finding Literature Search Results \u00b6 Poster","title":"Introduction"},{"location":"#introduction","text":"Author: Atharva Goel Advisor: Nirav C Merchant","title":"Introduction"},{"location":"#about","text":"Atharva Goel is a rising Senior at Paradise Valley High School in Phoenix, Arizona. During the summer of 2023 he is an intern at The University of Arizona through the BIO5 Institute's KEYS program. This summer he is an intern working at The University of Arizona in Dr. Nirav C Merchant's lab: a member of CyVerse , a cutting edge cyberinfrastructure funded by the National Science Foundation that is designed for research and committed to the principles of open science. This website follows the FAIR and CARE data principles and hopes to help further open science.","title":"About"},{"location":"#table-of-contents","text":"Introduction Logbook Assignments Jupyter Notebooks GitHub Actions Project Notes KEYS 2023 Main Project Results References","title":"Table of Contents"},{"location":"#github-actions","text":"GitHub Education Access","title":"GitHub Actions"},{"location":"#project-notes","text":"UNIX Shell Version Control with Git Containers","title":"Project Notes"},{"location":"#keys-2023-main-project","text":"My Project Prior Research/ Fact Finding Literature Search","title":"KEYS 2023 Main Project"},{"location":"#results","text":"Poster","title":"Results"},{"location":"containers/","text":"Containers \u00b6 Links \u00b6 Containers Explanation What are Contaiers & How do They Work Container Image Architecture Notes \u00b6 Containers are software packages built to run in any environment and simplify movement and application of software. They include all the elements required to run an application and can be devloped or deployed on any OS, VM, server, or cloud. Containerization also helps distribute the workload of development and deployment since applications are no longer environment specific. Containers rely on OS virtualization which means that they run on a single host OS and have the ability to emulate (or virtualize) any other OS or environment. This is in contrast to VMs which run on a single OS and attempt to virtualize the hardware of a machine or system. Containerization, the process of building software applications for containers, requires a container image. A container image includes the application code, config files, software dependencies, libraries, and environment variables. The image includes everything needed to run containerized applications irrespective of the infrastructure that hosts them. The benefits of containerization include the use of fewer system resources, high interoperability, optimized resource usage, high portability and scaling, as well as no hardware or implementation worries. Containers are also mainly used because of how they lend themselves to Agile development, and because they are high-efficency and present future ready solutions. Companies like using containers because they provide a environment that allows for easy migration in the future because of container-native development. Application refactoring or modernization also makes containerization very attractive. If any applications need to be shifted then they can simply be lifted and shifted to the cloud or re-factored for deployment. Continuous Intetegration & Continuous Deployment (CI/CD) allows for the concept of DevOps to flourish. It makes it so that developers can develop, deploy, scale, and integrate in a streamlined process. Containers are also ideal for batch processes since containers enable sharing of operating systems, libraries, and other dependencies among similar applications. Containerization aslo lends itself to the architecture of microservices. This architecture utilizes multiple contaienrs to deploy one app, called a container cluster (a group of containers in a containerized enviroment). Distributed cloud architectures generally utilize a multi-cloud or hybrid-cloud environment. Using containers helps support this architecture because of the portability and interoperability required by a distributed cloud system. Two softwares, Docker and Kubernetes, are leading software in container development and management. While Docker is a containerization platform, Kubernetes allows the management of multiple containers. Docker is a famous runtime environment for containers and allows an execution space for applications in containers. Kubernetes can store multiple containers to form a cluster while providing a managed environment for containers\u2019 collaboration. A container image is a static file with executable code that can create a container on a computing system. A container image is immutable\u2014meaning it cannot be changed, and can be deployed consistently in any environment. It is the core component of a containerized architecture. Container images have everything a container needs to run the container engine (Docker or CoreOS), system libraries, utilities, configuration settings, and specific workloads that should run on the container. The image also shares the OS kernel of the host rather than having its own. A container image is composed of layers, added on to a parent image. Layers make it possible to reuse components and configurations across images. Layers can also help reduce container size and improve performance. Docker \u00b6 Links \u00b6 Official Docker Website Docker Overview Getting Started Overview \u00b6 Docker is a platform used for developing, shipping, and running applications. Docker allows users to develop and package and then run applications in a isolated environment called a container. Docker helps manage and control the container lifecycle through developing an application and its components using containers, using the container for distributing and testing the application, and deploying the container into any production environment. Docker helps streamline the development cycle as well. Developers can create applications locally and share their work with other developers using Docker containers, they can then use Docker to push the application into the testing eviornment and find and fix bugs through the development environment as needed, finally they can push their image into a production environment. Docker takes advantage of all the benefits of containerized development, allowing users to do more with less resources and actively scale their applications. Docker uses a client-server architecture. The Docker client talks to the Docker daemon which is responsible for building, running, and distributing Docker containers. The client and daemon can either run on the same system or communicate by using a remote daemon. They communicate using a REST API, across UNIX sockets, or a network interface. The Docker daemon ( dockerd ) listens for Docker API requests and manages Docker images, containers, networks, and volumes. A daemon can also communicate with other daemons to manage services. The Docker client ( docker ) is the main way users interact with Docker. Commands such as docker run aresent by the client to dockerd which then carries out the command. The docker command uses the Docker API and the client can communicate with more than one daemon. The Docker registry stores Docker images. Docker Hub is the public registry that anyone can use and Docker looks for images on Docker Hub by default. When you use docker pull or docker run commands, the images are pulled from the configured registry. Docker images are read-only templates with instructions for creating a Docker container. Often an image is based, or built, upon another previous image with additional customization or modifications (this is also because images are immutable). You can create your own image or use pre-built ones as well. To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed to create the image and run it. Each instruction in a Dockerfile creates a layer in the image. When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt. This is part of what makes images so lightweight, small, and fast, when compared to other virtualization technologies. Docker containers are runnable instances of Docker images. You can create, start, stop, move, or delete a container using the Docker API or CLI. You can connect a container to one or more networks, attach storage to it, or even create a new image based on its current state. A container is isolated from the host machine and other containers, but you can control how isolated you would like the container to be. A container is defined by its image as well as any configuration options you provide to it when you create or start it. When a container is removed, any changes to its state that are not stored in persistent storage disappear.","title":"Containers"},{"location":"containers/#containers","text":"","title":"Containers"},{"location":"containers/#links","text":"Containers Explanation What are Contaiers & How do They Work Container Image Architecture","title":"Links"},{"location":"containers/#notes","text":"Containers are software packages built to run in any environment and simplify movement and application of software. They include all the elements required to run an application and can be devloped or deployed on any OS, VM, server, or cloud. Containerization also helps distribute the workload of development and deployment since applications are no longer environment specific. Containers rely on OS virtualization which means that they run on a single host OS and have the ability to emulate (or virtualize) any other OS or environment. This is in contrast to VMs which run on a single OS and attempt to virtualize the hardware of a machine or system. Containerization, the process of building software applications for containers, requires a container image. A container image includes the application code, config files, software dependencies, libraries, and environment variables. The image includes everything needed to run containerized applications irrespective of the infrastructure that hosts them. The benefits of containerization include the use of fewer system resources, high interoperability, optimized resource usage, high portability and scaling, as well as no hardware or implementation worries. Containers are also mainly used because of how they lend themselves to Agile development, and because they are high-efficency and present future ready solutions. Companies like using containers because they provide a environment that allows for easy migration in the future because of container-native development. Application refactoring or modernization also makes containerization very attractive. If any applications need to be shifted then they can simply be lifted and shifted to the cloud or re-factored for deployment. Continuous Intetegration & Continuous Deployment (CI/CD) allows for the concept of DevOps to flourish. It makes it so that developers can develop, deploy, scale, and integrate in a streamlined process. Containers are also ideal for batch processes since containers enable sharing of operating systems, libraries, and other dependencies among similar applications. Containerization aslo lends itself to the architecture of microservices. This architecture utilizes multiple contaienrs to deploy one app, called a container cluster (a group of containers in a containerized enviroment). Distributed cloud architectures generally utilize a multi-cloud or hybrid-cloud environment. Using containers helps support this architecture because of the portability and interoperability required by a distributed cloud system. Two softwares, Docker and Kubernetes, are leading software in container development and management. While Docker is a containerization platform, Kubernetes allows the management of multiple containers. Docker is a famous runtime environment for containers and allows an execution space for applications in containers. Kubernetes can store multiple containers to form a cluster while providing a managed environment for containers\u2019 collaboration. A container image is a static file with executable code that can create a container on a computing system. A container image is immutable\u2014meaning it cannot be changed, and can be deployed consistently in any environment. It is the core component of a containerized architecture. Container images have everything a container needs to run the container engine (Docker or CoreOS), system libraries, utilities, configuration settings, and specific workloads that should run on the container. The image also shares the OS kernel of the host rather than having its own. A container image is composed of layers, added on to a parent image. Layers make it possible to reuse components and configurations across images. Layers can also help reduce container size and improve performance.","title":"Notes"},{"location":"containers/#docker","text":"","title":"Docker"},{"location":"containers/#links_1","text":"Official Docker Website Docker Overview Getting Started","title":"Links"},{"location":"containers/#overview","text":"Docker is a platform used for developing, shipping, and running applications. Docker allows users to develop and package and then run applications in a isolated environment called a container. Docker helps manage and control the container lifecycle through developing an application and its components using containers, using the container for distributing and testing the application, and deploying the container into any production environment. Docker helps streamline the development cycle as well. Developers can create applications locally and share their work with other developers using Docker containers, they can then use Docker to push the application into the testing eviornment and find and fix bugs through the development environment as needed, finally they can push their image into a production environment. Docker takes advantage of all the benefits of containerized development, allowing users to do more with less resources and actively scale their applications. Docker uses a client-server architecture. The Docker client talks to the Docker daemon which is responsible for building, running, and distributing Docker containers. The client and daemon can either run on the same system or communicate by using a remote daemon. They communicate using a REST API, across UNIX sockets, or a network interface. The Docker daemon ( dockerd ) listens for Docker API requests and manages Docker images, containers, networks, and volumes. A daemon can also communicate with other daemons to manage services. The Docker client ( docker ) is the main way users interact with Docker. Commands such as docker run aresent by the client to dockerd which then carries out the command. The docker command uses the Docker API and the client can communicate with more than one daemon. The Docker registry stores Docker images. Docker Hub is the public registry that anyone can use and Docker looks for images on Docker Hub by default. When you use docker pull or docker run commands, the images are pulled from the configured registry. Docker images are read-only templates with instructions for creating a Docker container. Often an image is based, or built, upon another previous image with additional customization or modifications (this is also because images are immutable). You can create your own image or use pre-built ones as well. To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed to create the image and run it. Each instruction in a Dockerfile creates a layer in the image. When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt. This is part of what makes images so lightweight, small, and fast, when compared to other virtualization technologies. Docker containers are runnable instances of Docker images. You can create, start, stop, move, or delete a container using the Docker API or CLI. You can connect a container to one or more networks, attach storage to it, or even create a new image based on its current state. A container is isolated from the host machine and other containers, but you can control how isolated you would like the container to be. A container is defined by its image as well as any configuration options you provide to it when you create or start it. When a container is removed, any changes to its state that are not stored in persistent storage disappear.","title":"Overview"},{"location":"cyverse/","text":"CyVerse \u00b6 Introduction \u00b6 CyVerse serves as a way for life scientists to share their data with others around the world through a common cyberinfrastructure. It is funded by the National Science Foudation's Directorate for Biological Science and is currently led by the University of Arizona. CyVerse's cyberinfrastructure allows scientists to store their data as well as share it with others through cloud computing for further analysis. Acting as a way to complete complex analyses and share large datasets, CyVerse furthers open science and enables a collaborative workspace. VICE Apps \u00b6 The Visual and Interactive Computing Environment(VICE) allows scientists to run interactive applications through CyVerse. Through this scientists can open their interactive applications(Jupyter Lab, RStudio, Shiny, WebGL, HTML5, VNC, and XPRA), transfer data into containters, analyze this data, and send their results to the cloud.","title":"CyVerse"},{"location":"cyverse/#cyverse","text":"","title":"CyVerse"},{"location":"cyverse/#introduction","text":"CyVerse serves as a way for life scientists to share their data with others around the world through a common cyberinfrastructure. It is funded by the National Science Foudation's Directorate for Biological Science and is currently led by the University of Arizona. CyVerse's cyberinfrastructure allows scientists to store their data as well as share it with others through cloud computing for further analysis. Acting as a way to complete complex analyses and share large datasets, CyVerse furthers open science and enables a collaborative workspace.","title":"Introduction"},{"location":"cyverse/#vice-apps","text":"The Visual and Interactive Computing Environment(VICE) allows scientists to run interactive applications through CyVerse. Through this scientists can open their interactive applications(Jupyter Lab, RStudio, Shiny, WebGL, HTML5, VNC, and XPRA), transfer data into containters, analyze this data, and send their results to the cloud.","title":"VICE Apps"},{"location":"githubactions/","text":"Introduction \u00b6 GitHub Actions allows for the automation of tasks within your software development life cycle. Through GitHub actions users can automatically run their software testing scripts. Key Vocabulary \u00b6 Workflows \u00b6 Workflows can be used to design, test, package, release, or deploy a project on GitHub. A workflow can be added to a repository in GitHub using the file name .github/workflows. Workflows consist of one or more jobs that are scheduled/triggered by an event. Events \u00b6 Events are the activities that trigger the start of a workflow. Workflows can be triggered using pus or pull requests. Runners \u00b6 Runners can be hosted by GitHub or own on your own and is basically considered as a server that has GitHub Actions runner application installed. A runner runs one job at a time and reports the progress to GitHub. Jobs \u00b6 A job is a sequence of instructions that run on the same runner. A workflow containing multiple jobs will perform them in parallel by default. You may also set up a pipeline to conduct jobs in a specific order. Steps \u00b6 A step is a single task that can be used to execute commands in a job. An action or a shell command can be used as a step. Each step of a task runs on the same runner, allowing all of the activities in that job to exchange data. Actions \u00b6 Actions are commands that are used to join steps to create a job. Actions can be created or found on the GitHub community. Figure credit : GitHub Docs The components of GitHub Actions that work together to run jobs","title":"Githubactions"},{"location":"githubactions/#introduction","text":"GitHub Actions allows for the automation of tasks within your software development life cycle. Through GitHub actions users can automatically run their software testing scripts.","title":"Introduction"},{"location":"githubactions/#key-vocabulary","text":"","title":"Key Vocabulary"},{"location":"githubactions/#workflows","text":"Workflows can be used to design, test, package, release, or deploy a project on GitHub. A workflow can be added to a repository in GitHub using the file name .github/workflows. Workflows consist of one or more jobs that are scheduled/triggered by an event.","title":"Workflows"},{"location":"githubactions/#events","text":"Events are the activities that trigger the start of a workflow. Workflows can be triggered using pus or pull requests.","title":"Events"},{"location":"githubactions/#runners","text":"Runners can be hosted by GitHub or own on your own and is basically considered as a server that has GitHub Actions runner application installed. A runner runs one job at a time and reports the progress to GitHub.","title":"Runners"},{"location":"githubactions/#jobs","text":"A job is a sequence of instructions that run on the same runner. A workflow containing multiple jobs will perform them in parallel by default. You may also set up a pipeline to conduct jobs in a specific order.","title":"Jobs"},{"location":"githubactions/#steps","text":"A step is a single task that can be used to execute commands in a job. An action or a shell command can be used as a step. Each step of a task runs on the same runner, allowing all of the activities in that job to exchange data.","title":"Steps"},{"location":"githubactions/#actions","text":"Actions are commands that are used to join steps to create a job. Actions can be created or found on the GitHub community. Figure credit : GitHub Docs The components of GitHub Actions that work together to run jobs","title":"Actions"},{"location":"githubed/","text":"GitHub Education \u00b6 Steps to Enroll \u00b6 Go to the GitHub Education Site and enter your education status as student From here your school email and dated documentation of your enrollment is required After this is approved you have access to the GitHub education student developer pack! What's Included and Functionality \u00b6 As part of the GitHub education student developer pack and GitHub global campus students and faculty are granted access to forums, Campus TV, exclusive events, and free software and subscriptions such as Canva Pro, Microsoft Azure, and VS code. note: the student developer pack doesn't include access to GitHub CodeSpaces","title":"GitHub Education Access"},{"location":"githubed/#github-education","text":"","title":"GitHub Education"},{"location":"githubed/#steps-to-enroll","text":"Go to the GitHub Education Site and enter your education status as student From here your school email and dated documentation of your enrollment is required After this is approved you have access to the GitHub education student developer pack!","title":"Steps to Enroll"},{"location":"githubed/#whats-included-and-functionality","text":"As part of the GitHub education student developer pack and GitHub global campus students and faculty are granted access to forums, Campus TV, exclusive events, and free software and subscriptions such as Canva Pro, Microsoft Azure, and VS code. note: the student developer pack doesn't include access to GitHub CodeSpaces","title":"What's Included and Functionality"},{"location":"jupyter/","text":"Jupyter Notebooks \u00b6 What is it? \u00b6 seamless way to write and iterate python code to perform data analysis Notebooks allows code, figures, diagrams, charts, and explanations to all be stored in one location This allows developer's work to be shared, leading to collaboration and improving reproducibility Additionally, Jupyter Notebooks is free of charge, improving equity in data analysis and software development When new notebooks are created prebuilt docker containers are used to put the notebooks at their own path How does it work? \u00b6 The basis for Jupyter Notebooks is IPython, a command line shell for writing code in the terminal Juputer Notebooks allows this code to be written and iterated in the browser through the use of the ipykernel lines of code can be run all at once or one at a time Jupyter Notebooks supports mulitple languages, most common is python Allows storage of code and inclusion of Markdown files for notes and documentation when new notebooks are created prebuilt docker containers are used to place the notebooks on their own paths Structure \u00b6 Kernel \u00b6 the kernel is specific to the programming language, this project will be in python process that supports the notebook to execute the written code Juptyer team maintains the ipykernel, but other user maintained kernels are available for use Cell \u00b6 the cells are the main contents of the notebooks and are where code is written markdown cells can be created to store notes and info on code green = code running grey = code not running","title":"Jupyter Notebooks"},{"location":"jupyter/#jupyter-notebooks","text":"","title":"Jupyter Notebooks"},{"location":"jupyter/#what-is-it","text":"seamless way to write and iterate python code to perform data analysis Notebooks allows code, figures, diagrams, charts, and explanations to all be stored in one location This allows developer's work to be shared, leading to collaboration and improving reproducibility Additionally, Jupyter Notebooks is free of charge, improving equity in data analysis and software development When new notebooks are created prebuilt docker containers are used to put the notebooks at their own path","title":"What is it?"},{"location":"jupyter/#how-does-it-work","text":"The basis for Jupyter Notebooks is IPython, a command line shell for writing code in the terminal Juputer Notebooks allows this code to be written and iterated in the browser through the use of the ipykernel lines of code can be run all at once or one at a time Jupyter Notebooks supports mulitple languages, most common is python Allows storage of code and inclusion of Markdown files for notes and documentation when new notebooks are created prebuilt docker containers are used to place the notebooks on their own paths","title":"How does it work?"},{"location":"jupyter/#structure","text":"","title":"Structure"},{"location":"jupyter/#kernel","text":"the kernel is specific to the programming language, this project will be in python process that supports the notebook to execute the written code Juptyer team maintains the ipykernel, but other user maintained kernels are available for use","title":"Kernel"},{"location":"jupyter/#cell","text":"the cells are the main contents of the notebooks and are where code is written markdown cells can be created to store notes and info on code green = code running grey = code not running","title":"Cell"},{"location":"keysassignments/","text":"Keys Assignments \u00b6 Poster Assignments: \u00b6 Assignment 1: Internship Description Assignment 2: Introduction to your Research Assignment 3: Materials and Methods Assignment 4: Results Assignment 5: Long Abstract Assignment 6: Title and Short Abstract Reflections: \u00b6 Reflection 1 Reflection 2 Reflection 3 Reflection 4 Reflection 5","title":"Assignments"},{"location":"keysassignments/#keys-assignments","text":"","title":"Keys Assignments"},{"location":"keysassignments/#poster-assignments","text":"Assignment 1: Internship Description Assignment 2: Introduction to your Research Assignment 3: Materials and Methods Assignment 4: Results Assignment 5: Long Abstract Assignment 6: Title and Short Abstract","title":"Poster Assignments:"},{"location":"keysassignments/#reflections","text":"Reflection 1 Reflection 2 Reflection 3 Reflection 4 Reflection 5","title":"Reflections:"},{"location":"litsearch/","text":"Method Search & Software: \u00b6 Links: \u00b6 DeepLabCut Official DeepLabCut GitHub Track-Anything GitHub XMem GitHub MaskFreeVIS Official MaskFreeVIS GitHub YOLO8 Official DeepLabCut: \u00b6 DeepLabCut is a software package designed for 2D and 3D markerless pose estimation based on transfer learning with deep neural networks. DeepLabCut is very accurate and efficient and requires minimal training data as well. The versatility of this framework is demonstrated by tracking various body parts in multiple species across a broad collection of behaviors. The package is open source, fast, robust, and can be used to compute 3D pose estimates or for multi-animals. This package is collaboratively developed by the Mathis Group & Mathis Lab at EPFL (releases prior to 2.1.9 were developed at Harvard University). To use DeepLabCut you can use their own GUI, their Jupyter Notebook, their Google Colab, or your own terminal. They also provide lots of data that helps you demo the package and test installation. DeepLabCut has been used for trail tracking, reaching in mice, and various Drosophila behaviours during egg-laying. But this toolbox has a variety of applications and it has already been applied to rats, humans, various fish species, bacteria, leeches, various robots, cheetahs, mouse whiskers and race horses. DeepLabCut utilizes the feature detectors (ResNets + readout layers) of one of the state-of-the-art algorithms for human pose estimation by Insafutdinov et al., called DeeperCut. They have improved the inference speed and provided both additional and novel augmentation methods, added real-time, and multi-animal support and currently provide state-of-the-art performance for animal pose estimation. Because of transfer learning, the package requires little training data for multiple challenging behaviors. The feature detectors are robust to video compression. It allows 3D pose estimation with a single network and camera. It allows 3D pose estimation with a single network trained on data from multiple cameras together with standard triangulation methods. DeepLabCut is embedding in a larger open-source eco-system, providing behavioral tracking for neuroscience, ecology, medical, and technical applications. DeepLabCut: Markerless Pose Estimation of User-Defined Body Parts With Deep Learning : \u00b6 New methods for markerless tracking using deep learning and neural networks. Excellent performance, comparable to human accuracy. Minimal training data also yields excellent results across various species and behaviors. Quantification of behavior essential for understanding brain. Computer vision much easier and more efficient than manual analysis. Markers, used traditionally, invade space and aren't cost effective and are distracting to subjects. Deep learning architecture greatly improve accuracy of pose estimation. Large datasets can be tackled by using transfer learning. Used feature detection architecture from DeeperCut (best pose estimation algorithms, can acheieve human-level labeling accuracy with minimal data). Result of transfer learning, feature detectors are based on extremely deep neural networks, which were pretrained on ImageNet, a massive dataset for object recognition. DeeperCut achieves outstanding performance on multi-human pose detection benchmarks, trained on thousands of labeled images. DeepLabCut is DeeperCut but focuses on feature detectors, which are variations of deep residual neural networks (ResNet) with readout layers that predict the location of a body part. DeepLabCut is a deep convolutional network combining two key ingredients from object recognition and semantic segmentation: pretrained ResNets and deconvolutional layers. The network consists of a variant of ResNets, whose weights were trained on a popular, large-scale object recognition benchmark called ImageNet. Deconvolutional layers are used to up-sample the visual information and produce spatial probability densities. For each body part, its probability density represents the \u2018evidence\u2019 that a body part is in a particular location. To fine-tune the network for a particular task, its weights are trained on labeled data, which consist of frames and the accompanying annotated body part locations. The weights are adjusted in an iterative fashion such that for a given frame the network assigns high probabilities to labeled body part locations and low probabilities elsewhere. The network is rewired and \u2018learns\u2019 feature detectors for the labeled body parts. Average variability to ground truth was found to be very low and small. To quantidy accuracy, datasets were split and a certain percentage was used to train while the rest was used to test. When trained with 80% of the data the algorithm achieved human-level accuracy. After varying training and testing percentages, it was found that even 100 frames were enough to achieve excellent generalization. Data augmentation (such as rotations or translations) also resulted in minimal differences, demonstrating the data-efficiency of DeepLabCut. The feature detectors were able to translate pretty well to novel mouse behaviors as well as videos with multiple mice. Although it wasn't error free, this can be improved by simply training on that data or on data for multiple mice. End-to-end training allows the model to facilitate the localization of one body part based on other labeled body parts. The network that was trained with all body part labels simultaneously outperforms the specialized networks nearly twofold. Temporal information could indeed be beneficial in certain contexts, challenges remain to using end-to-end-trained deep architectures for video data to extract postures. Because of the curse of dimensionality, deep architectures on videos must rely on input images with lower spatial resolution, and thus the best-performing action recognition algorithms still rely on frame-by-frame analysis with deep networks pretrained on ImageNet as a result of hardware limitations. Therefore currently, in situations where occlusions are very common, such as in social behaviors, pairwise interactions could also be added to improve performance. Using DeepLabCut for 3D Markerless Pose Estimation Across Species and Behaviors : \u00b6 Transfer learning, the ability to take a network, which was trained on a task with a large supervised data set, and utilize it for another task with a small supervised data set, is beginning to allow users to broadly apply deep learning methods. DeepLabCut provides tools to create annotated training sets, train robust feature detectors, and utilize them to analyze novel behavioral videos. The major motivation for developing the DeepLabCut toolbox was to provide a reliable and efficient tool for high-throughput video analysis, where powerful feature detectors of user-defined body parts need to be learned for a specific situation. The toolbox is aimed to solve the problem of detecting body parts in dynamic visual environments where varying background, reflective walls or motion blur hinder the performance of common techniques, such as thresholding or regression based on visual features. The user starts by creating a new project based on a project and username as well as some (initial) videos, which are required to create the training dataset (additional videos can also be added after the creation of the project). Next, DeepLabCut extracts frames, which reflect the diversity of the behavior with respect to postures, animal identities, etc. Then the user can label the points of interest in the extracted frames. These annotated frames can be visually checked for accuracy, and corrected if necessary. Eventually, a training dataset is created by merging all the extracted labeled frames and splitting them into subsets of test and train frames. Then, a pre-trained network (ResNet) is trained end-to-end to adapt its weights in order to predict the desired features. The performance of the trained network can then be evaluated on the training and test frames. The trained network can be used to analyze videos yielding extracted pose files. In case the trained network does not generalize well to unseen data in the evaluation and analysis step, then additional frames with poor results can be extracted and the predicted labels can be manually shifted to their ideal location. This refinement step, if needed, creates an additional set of annotated images that can then be merged with the original training dataset to create a new training dataset. This larger training set can then be used to re-train the feature detectors for better results. This active learning loop can be done iteratively to robustly and accurately analyze videos with potentially large variability- i.e. experiments that include many individuals, and run over long time periods. Furthermore, the user can add additional body parts/labels at later stages during a project as well as correct user-defined labels. However, if a user aims to track (adult) human poses, many excellent options exist, including DeeperCut, ArtTrack, DeepPose, OpenPose, and OpenPose-Plus (better for humans). DeepLabCut does not support occlusions, requires HPC and GPUs, and also workers faster with smaller images. Track-Anything: \u00b6 Track-Anything is a flexible and interactive tool for video object tracking and segmentation. It is developed upon Segment Anything, can specify anything to track and segment via user clicks only. During tracking, users can flexibly change the objects they wanna track or correct the region of interest if there are any ambiguities. Track-Anything is suitable for video object tracking and segmentation with shot changes, visualized development and data annotation for video object tracking and segmentation, object-centric downstream video tasks (such as video inpainting and editing). Track Anything: Segment Anything Meets Videos : \u00b6 Segment Anything Model (SAM) displays impressive performance on images, but it performs poorly on video segmentation. The proposed Track Anything Model (TAM), achieves high-performance interactive tracking and segmentation in videos. Current state-of-the-art video trackers/segmenters are trained on large-scale manually-annotated datasets and initialized by a bounding box or a segmentation mask. Moreover, current initialization settings, especially the semi-supervised VOS, need specific object mask groundtruth for model initialization. Large amouns of human labor is also required for huge amounts of annotated and labeled data. Recently, Segment-Anything Model (SAM) has been proposed, which is a large foundation model for image segmentation. It supports flexible prompts and computes masks in real-time, thus allowing interactive use. Trained on 11 million images and 1.1 billion masks, SAM can produce high-quality masks and do zero-shot segmentation in generic scenarios. With input user-friendly prompts of points, boxes, or language, SAM can give satisfactory segmentation masks on specific image areas. However, using SAM in videos directly does not give an impressive performance due to its deficiency in temporal correspondence. But tracking or segmenting in videos faces challenges from scale variation, target deformation, motion blur, camera motion, similar objects, and so on. In this paper, the Track-Anything toolkit is proposed for high-performance object tracking and segmentation in videos. With a user-friendly interface, the Track Anything Model (TAM) can track and segment any objects in a given video with only one-pass inference. TAM combines SAM, a large segmentation model, and XMem, an advanced VOS model. Firstly, users can interactively initialize the SAM, i.e., clicking on the object, to define a target object; then, XMem is used to give a mask prediction of the object in the next frame according to both temporal and spatial correspondence; next, SAM is utilized to give a more precise mask description; during the tracking process, users can pause and correct as soon as they notice tracking failures. TAM is able to promote the SAM applications to the video level to achieve interactive video object tracking and segmentation. Rather than separately using SAM per frame, it integrates SAM into the process of temporal correspondence construction. The Track Anything task aims for flexible object tracking in arbitrary videos. The target objects can be flexibly selected, added, or removed in any way according to the users\u2019 interests. Also, the video length and types can be arbitrary rather than limited to trimmed or natural videos. With such settings, diverse downstream tasks can be achieved, including single/multiple object tracking, short-/long-term object tracking, unsupervised VOS, semi-supervised VOS, referring VOS, interactive VOS, long-term VOS, and more. As a foundation model for image segmentation (and the genesis for TAM), SAM is based on ViT and trained on the large-scale dataset SA-1B. Obviously, SAM shows promising segmentation ability on images, especially on zero-shot segmentation tasks. Unfortunately, SAM only shows superior performance on image segmentation, while it cannot deal with complex video segmentation. Given the mask description of the target object at the first frame, XMem can track the object and generate corresponding masks in the subsequent frames. Inspired by the Atkinson-Shiffrin memory model, it aims to solve the difficulties in long-term videos with unified feature memory stores. The drawbacks of XMem are also obvious: as a semi-supervised VOS model, it requires a precise mask to initialize; for long videos, it is difficult for XMem to recover from tracking or segmentation failure. In this paper, we solve both difficulties by importing interactive tracking with SAM. The Track-Anything process is divided into the following processes: Initialization with SAM - As SAM provides an opportunity to segment a region of interest with weak prompts, e.g., points, and bounding boxes, we use it to give an initial mask of the target object. Following SAM, users can get a mask description of the interested object by a click or modify the object mask with several clicks to get a satisfactory initialization. Tracking with XMem - Given the initialized mask, XMem performs semi-supervised VOS on the following frames. Since XMem is an advanced VOS method that can output satisfactory results on simple scenarios, we output the predicted masks of XMem on most occasions. When the mask quality is not as good, we save the XMem predictions and corresponding intermediate parameters, i.e., probes and affinities, and skip to the next step. Refinement with SAM - During the inference of VOS models, keep predicting consistent and precise masks are challenging. In fact, most state-of-the-art VOS models tend to segment more and more coarsely over time during inference. Therefore, we utilize SAM to refine the masks predicted by XMem when its quality assessment is not satisfactory. Specifically, we project the probes and affinities to be point prompts for SAM, and the predicted mask from Step 2 is used as a mask prompt for SAM. Then, with these prompts, SAM is able to produce a refined segmentation mask. Such refined masks will also be added to the temporal correspondence of XMem to refine all subsequent object discrimination. Correction with human participation - After the above three steps, the TAM can now successfully solve some common challenges and predict segmentation masks. However, it is still difficult to accurately distinguish the objects in some extremely challenging scenarios, especially when processing long videos. Therefore, we propose to add human correction during inference, which can bring a qualitative leap in performance with only very small human efforts. In detail, users can compulsively stop the TAM process and correct the mask of the current frame with positive and negative clicks. TAM can handle multi-object separation, target deformation, scale change, and camera motion well, which demonstrates its superior tracking and segmentation abilities within only click initialization and one-round inference. Some failed cases: The failed cases typically appear on the following two occasions. Current VOS models are mostly designed for short videos, which focus more on maintaining short-term memory rather than long-term memory. This leads to mask shrinkage or lacking refinement in long-term videos, as shown in seq (a). Essentially, this is solved in step 3 by the refinement ability of SAM, while its effectiveness is lower than expected in realistic applications. It indicates that the ability of SAM refinement based on multiple prompts can be further improved in the future. On the other hand, human participation/interaction in TAM can be an approach to solving such difficulties, while too much interaction will also result in low efficiency. Thus, the mechanism of long-term memory preserving and transient memory updating is still important. When the object structure is complex, e.g., the bicycle wheels in seq (b) contain many cavities in groundtruth masks. It is very difficult to get a fine-grained initialized mask by propagating the clicks. Thus, the coarse initialized masks may have side effects on the subsequent frames and lead to poor predictions. This suggests that SAM is still struggling with complex and precision structures. XMem: \u00b6 XMem frames Video Object Segmentation (VOS) as a memory problem. Prior works mostly use a single type of feature memory. This can be in the form of network weights (i.e., online learning), last frame segmentation (e.g., MaskTrack), spatial hidden representation (e.g., Conv-RNN-based methods), spatial-attentional features (e.g., STM, STCN, AOT), or some sort of long-term compact features (e.g., AFB-URR). Methods with a short memory span are not robust to changes, while those with a large memory bank are subject to a catastrophic increase in computation and GPU memory usage. Attempts at long-term attentional VOS like AFB-URR compress features eagerly as soon as they are generated, leading to a loss of feature resolution. XMem is inspired by the Atkinson-Shiffrin human memory model, which has a sensory memory, a working memory, and a long-term memory. These memory stores have different temporal scales and complement each other in our memory reading mechanism. It performs well in both short-term and long-term video datasets, handling videos with more than 10,000 frames with ease. XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model : \u00b6 XMem is a video object segmentation architecture for long videos with unified feature memory stores inspired by the Atkinson-Shiffrin memory model. Prior work on video object segmentation typically only uses one type of feature memory. For videos longer than a minute, a single feature memory model tightly links memory consumption and accuracy. XMem uses an architecture that incorporates multiple independent yet deeply-connected feature memory stores: a rapidly updated sensory memory, a high-resolution working memory, and a compact thus sustained long-term memory. A memory potentiation algorithm that routinely consolidates actively used working memory elements into the long-term memory, which avoids memory explosion and minimizes performance decay for long-term prediction is developed. Combined with a new memory reading mechanism, XMem greatly exceeds state-of-the-art performance on long-video datasets while being on par with state-of-theart methods (that do not work on long videos) on short-video datasets. XMem relies on semi-supervised VOS by using a first-frame annotation, provided by the user, and segments objects in all other frames as accurately as possible while preferably running in real-time, online, and while having a small memory footprint even when processing long videos. VOS methods employ a feature memory to store relevant deep-net representations of an object to propogate annotations to other frames. Online learning methods use the weights of a network as their feature memory and recurrent methods propagate information often from the most recent frames, either via a mask or via a hidden representation. But these methods are prone to drifting and struggle with occlusions. Recent VOS methods store representations of past frames in the feature memory with features extracted from the newly observed query frame which needs to be segmented. Although these methods are high-performance, they require a large amount of GPU memory and struggle to handle videos longer than a minute. Methods designed for longer videos, however, often sacrifice on segmentation quality. It is proposed that this connection of performance and GPU memory consumption is a direct consequence of using a single feature memory type. Instead a unified memory architecture, dubbed XMem, is proposed. XMem maintains three independent yet deeply-connected feature memory stores: a rapidly updated sensory memory, a high-resolution working memory, and a compact thus sustained long-term memory. The sensory memory corresponds to the hidden representation of a GRU which is updated every frame. It provides temporal smoothness but fails for long-term prediction due to representation drift. To complement, the working memory is agglomerated from a subset of historical frames and considers them equally without drifting over time. To control the size of the working memory, XMem routinely consolidates its representations into the long-term memory, inspired by the consolidation mechanism in the human memory. XMem stores long-term memory as a set of highly compact prototypes. For this, they develop a memory potentiation algorithm that aggregates richer information into these prototypes to prevent aliasing due to sub-sampling. To read from the working and long-term memory, they devise a space-time memory reading operation. The three feature memory stores combined permit handling long videos with high accuracy while keeping GPU memory usage low. Given the image and target object mask at the first frame, XMem tracks the object and generates corresponding masks for subsequent query frames. For this, it first initializes the different feature memory stores using the inputs. For each subsequent query frame, it performs memory reading from long-term memory, working memory, and sensory memory respectively. The readout features are used to generate a segmentation mask. Then, it updates each of the feature memory stores at different frequencies. It updates the sensory memory every frame and inserts features into the working memory at every r-th frame. When the working memory reaches a pre-defined maximum of Tmax frames, it consolidates features from the working memory into the long-term memory in a highly compact form. When the long-term memory is also full (which only happens after processing thousands of frames), it discards obsolete features to bound the maximum GPU memory usage. These feature memory stores work in conjunction to provide high-quality features with low GPU memory usage even for very long videos. XMem consists of three end-to-end trainable convolutional networks as shown in Figure 3: a query encoder that extracts query-specific image features, a decoder that takes the output of the memory reading step to generate an object mask, and a value encoder that combines the image with the object mask to extract new memory features. See Section 3.6 for details of these networks. In the following, we will first describe the memory reading operation before discussing each feature memory store in detail. The method sometimes fails when the target object moves too quickly or has severe motion blur as even the fastest updating sensory memory cannot catch up. Comparisons: Failure Cases: MaskFreeVIS: \u00b6 The recent advancement in Video Instance Segmentation (VIS) has largely been driven by the use of deeper and increasingly data-hungry transformer-based models. However, video masks are tedious and expensive to annotate, limiting the scale and diversity of existing VIS datasets. The solution proposed is MaskFreeVIS, which aims to remove the mask-annotation requirement, achieving highly competitive VIS performance, while only using bounding box annotations for the object state. It leverages the rich temporal mask consistency constraints in videos by introducing the Temporal KNN-patch Loss (TK-Loss), providing strong mask supervision without any labels. TK-Loss finds one-to-many matches across frames, through an efficient patch-matching step followed by a K-nearest neighbor selection. A consistency loss is then enforced on the found matches. The mask-free objective is simple to implement, has no trainable parameters, is computationally efficient, yet outperforms baselines employing, e.g., state-of-the-art optical flow to enforce temporal mask consistency. MaskFreeVIS is validated on the YouTube-VIS 2019/2021, OVIS and BDD100K MOTS benchmarks. The results clearly demonstrate the efficacy of the method by drastically narrowing the gap between fully and weakly-supervised VIS performance. MaskFreeVIS has high-performing video instance segmentation without using any video masks or even image mask labels. Using SwinL and built on Mask2Former, MaskFreeVIS achieved 56.0 AP on YTVIS without using any video masks labels. Using ResNet-101, MaskFreeVIS achieves 49.1 AP without using video masks, and 47.3 AP only using COCO mask initialized model. A new parameter-free Temporal KNN-patch Loss (TK-Loss), which leverages temporal masks consistency using unsupervised one-to-k patch correspondence is what MaskFreeVIS uses. TK-Loss is flexible to intergrated with state-of-the-art transformer-based VIS models, with no trainable parameters. Mask-Free Video Instance Segmentation : \u00b6 Video Instance Segmentation (VIS) requires jointly detecting, tracking and segmenting all objects in a video from a given set of categories. State-of-the-art VIS models are trained with complete video annotations from VIS datasets. However, video annotation is costly, in particular regarding objectmask labels. Even coarse polygon-based mask annotation is multiple times slower than annotating video bounding boxes. This makes existing VIS benchmarks difficult to scale, limiting the number of object categories covered, especially for recent transformer based VIS models. Current box-supervised instance segmentation models are created for images and achieve low accuracy when applied to videos because they do not utilize temporal cues. Instead, the MaskFreeVIS method is proposed, for high performance VIS without any mask annotations. To leverage temporal mask consistency, the Temporal KNN-patch Loss (TK-Loss) is introduced. To find regions corresponding to the same underlying video object, TK-Loss first builds correspondences across frames by patch-wise matching. For each target patch, only the top K matches in the neighboring frame with high enough matching score are selected. A temporal consistency loss is then applied to all found matches to promote the mask consistency. Specifically, the surrogate objective function not only promotes the one-to-k matched regions to reach the same mask probabilities, but also commits their mask prediction to a confident foreground or background prediction by entropy minimization. TK-Loss simply replaces the conventional video mask losses in supervising video mask generation. To further enforce temporal consistency through the video clip, TK-Loss is employed in a cyclic manner instead of using dense frame-wise connections. This greatly reduces memory cost with negligible performance drop. MaskFreeVIS achieves competitive VIS performance without using any video masks or even image mask labels on all datasets. Validated on various methods and backbones, MaskFreeVIS achieves 91.25% performance of its fully supervised counterparts, even outperforming a few recent fully-supervised methods on the popular YTVIS benchmark. YOLO8: \u00b6 Ultralytics YOLOv8 is the latest version of the acclaimed real-time object detection and image segmentation model. YOLOv8 is built on cutting-edge advancements in deep learning and computer vision, offering unparalleled performance in terms of speed and accuracy. Its streamlined design makes it suitable for various applications and easily adaptable to different hardware platforms, from edge devices to cloud APIs. Using YOLOv8 is as simple as installing ultralytics with pip and get it up and running in minutes. YOLOv8 helps you predict new images and videos. You can also train a new YOLOv8 model on your own custom dataset or use a pre-trained model. YOLOv8 also provides tasks like segment, classify, pose and track. YOLO's History: YOLO (You Only Look Once), a popular object detection and image segmentation model, was developed by Joseph Redmon and Ali Farhadi at the University of Washington. Launched in 2015, YOLO quickly gained popularity for its high speed and accuracy. YOLOv2, released in 2016, improved the original model by incorporating batch normalization, anchor boxes, and dimension clusters. YOLOv3, launched in 2018, further enhanced the model's performance using a more efficient backbone network, multiple anchors and spatial pyramid pooling. YOLOv4 was released in 2020, introducing innovations like Mosaic data augmentation, a new anchor-free detection head, and a new loss function. YOLOv5 further improved the model's performance and added new features such as hyperparameter optimization, integrated experiment tracking and automatic export to popular export formats. YOLOv6 was open-sourced by Meituan in 2022 and is in use in many of the company's autonomous delivery robots. YOLOv7 added additional tasks such as pose estimation on the COCO keypoints dataset. YOLOv8 is the latest version of YOLO by Ultralytics. As a cutting-edge, state-of-the-art (SOTA) model, YOLOv8 builds on the success of previous versions, introducing new features and improvements for enhanced performance, flexibility, and efficiency. YOLOv8 supports a full range of vision AI tasks, including detection, segmentation, pose estimation, tracking, and classification. This versatility allows users to leverage YOLOv8's capabilities across diverse applications and domains. YOLOv8 is an AI framework that supports multiple computer vision tasks. The framework can be used to perform detection, segmentation, classification, and pose estimation. Each of these tasks has a different objective and use case. Detection is the primary task supported by YOLOv8. It involves detecting objects in an image or video frame and drawing bounding boxes around them. The detected objects are classified into different categories based on their features. YOLOv8 can detect multiple objects in a single image or video frame with high accuracy and speed. Segmentation is a task that involves segmenting an image into different regions based on the content of the image. Each region is assigned a label based on its content. This task is useful in applications such as image segmentation and medical imaging. YOLOv8 uses a variant of the U-Net architecture to perform segmentation. Classification is a task that involves classifying an image into different categories. YOLOv8 can be used to classify images based on their content. It uses a variant of the EfficientNet architecture to perform classification. Pose/keypoint detection is a task that involves detecting specific points in an image or video frame. These points are referred to as keypoints and are used to track movement or pose estimation. YOLOv8 can detect keypoints in an image or video frame with high accuracy and speed. Matrix & Comparison \u00b6 All qualitative fields will be marked on a scale of 1-5 with 1 representing a worse rating and 5 representing a great rating. The videos used to test are uploaded below. Single-Human Video (SH, 2.84 MB, 28 sec.) DeepLabCut Track-Anything YOLOv8 Multi-Human Video (MH, 2.07 MB, 14 sec.) DeepLabCut Track-Anything YOLOv8 Single-Animal Video (SA, 742 KB, 12 sec.) DeepLabCut Track-Anything YOLOv8 Multi-Animal Video (MA, 6.35 MB, 13 sec.) DeepLabCut Track-Anything YOLOv8 Method Use Case Test Link Number of Papers Using Tool Activeness Total Time to Test Processing Time Ease of Use Efficiency of Single-Human Segmentation Efficiency of Multi-Human Segmentation Efficiency of Single-Animal Segmentation Efficiency of Multi-Animal Segmentation DeepLabCut Trained Pose Estimation/ Tracking http://bit.ly/DLCMZ 3310 5 SH - 6m MH - 5m SA - 5m MA - 5m SH - 1m MH - 1m SA - 56s MA - 56s 4 4 2 3 3 Track-Anything Semi-supervised Segmentation/ Tracking https://bit.ly/TrAn 3 4 SH - 3m MH - 2m SA - 2m MA - 3m SH - 2m MH - 1m SA - 52s MA - 1m 5 4 5 5 4 XMem Trained Segmentation/ Tracking https://xxxxxx/xxxx 1500 1 SH - 0m MH - 0m SA - 0m MA - 0m SH - 0m MH - 0m SA - 0m MA - 0m 1 1 1 1 1 MaskFreeVIS Trained Segmentation/ Tracking https://xxxxxx/xxxx 2 1 SH - 0m MH - 0m SA - 0m MA - 0m SH - 0m MH - 0m SA - 0m MA - 0m 1 1 1 1 1 YOLOv8 Trained Pose Estimation/ Tracking https://bit.ly/YOLO8 382 5 SH - 2m MH - 2m SA - 3m MA - 5m SH - 1m MH - 45s SA - 1m MA - 1m 4 4 4 2 1","title":"Literature Search"},{"location":"litsearch/#method-search-software","text":"","title":"Method Search &amp; Software:"},{"location":"litsearch/#links","text":"DeepLabCut Official DeepLabCut GitHub Track-Anything GitHub XMem GitHub MaskFreeVIS Official MaskFreeVIS GitHub YOLO8 Official","title":"Links:"},{"location":"litsearch/#deeplabcut","text":"DeepLabCut is a software package designed for 2D and 3D markerless pose estimation based on transfer learning with deep neural networks. DeepLabCut is very accurate and efficient and requires minimal training data as well. The versatility of this framework is demonstrated by tracking various body parts in multiple species across a broad collection of behaviors. The package is open source, fast, robust, and can be used to compute 3D pose estimates or for multi-animals. This package is collaboratively developed by the Mathis Group & Mathis Lab at EPFL (releases prior to 2.1.9 were developed at Harvard University). To use DeepLabCut you can use their own GUI, their Jupyter Notebook, their Google Colab, or your own terminal. They also provide lots of data that helps you demo the package and test installation. DeepLabCut has been used for trail tracking, reaching in mice, and various Drosophila behaviours during egg-laying. But this toolbox has a variety of applications and it has already been applied to rats, humans, various fish species, bacteria, leeches, various robots, cheetahs, mouse whiskers and race horses. DeepLabCut utilizes the feature detectors (ResNets + readout layers) of one of the state-of-the-art algorithms for human pose estimation by Insafutdinov et al., called DeeperCut. They have improved the inference speed and provided both additional and novel augmentation methods, added real-time, and multi-animal support and currently provide state-of-the-art performance for animal pose estimation. Because of transfer learning, the package requires little training data for multiple challenging behaviors. The feature detectors are robust to video compression. It allows 3D pose estimation with a single network and camera. It allows 3D pose estimation with a single network trained on data from multiple cameras together with standard triangulation methods. DeepLabCut is embedding in a larger open-source eco-system, providing behavioral tracking for neuroscience, ecology, medical, and technical applications.","title":"DeepLabCut:"},{"location":"litsearch/#deeplabcut-markerless-pose-estimation-of-user-defined-body-parts-with-deep-learning","text":"New methods for markerless tracking using deep learning and neural networks. Excellent performance, comparable to human accuracy. Minimal training data also yields excellent results across various species and behaviors. Quantification of behavior essential for understanding brain. Computer vision much easier and more efficient than manual analysis. Markers, used traditionally, invade space and aren't cost effective and are distracting to subjects. Deep learning architecture greatly improve accuracy of pose estimation. Large datasets can be tackled by using transfer learning. Used feature detection architecture from DeeperCut (best pose estimation algorithms, can acheieve human-level labeling accuracy with minimal data). Result of transfer learning, feature detectors are based on extremely deep neural networks, which were pretrained on ImageNet, a massive dataset for object recognition. DeeperCut achieves outstanding performance on multi-human pose detection benchmarks, trained on thousands of labeled images. DeepLabCut is DeeperCut but focuses on feature detectors, which are variations of deep residual neural networks (ResNet) with readout layers that predict the location of a body part. DeepLabCut is a deep convolutional network combining two key ingredients from object recognition and semantic segmentation: pretrained ResNets and deconvolutional layers. The network consists of a variant of ResNets, whose weights were trained on a popular, large-scale object recognition benchmark called ImageNet. Deconvolutional layers are used to up-sample the visual information and produce spatial probability densities. For each body part, its probability density represents the \u2018evidence\u2019 that a body part is in a particular location. To fine-tune the network for a particular task, its weights are trained on labeled data, which consist of frames and the accompanying annotated body part locations. The weights are adjusted in an iterative fashion such that for a given frame the network assigns high probabilities to labeled body part locations and low probabilities elsewhere. The network is rewired and \u2018learns\u2019 feature detectors for the labeled body parts. Average variability to ground truth was found to be very low and small. To quantidy accuracy, datasets were split and a certain percentage was used to train while the rest was used to test. When trained with 80% of the data the algorithm achieved human-level accuracy. After varying training and testing percentages, it was found that even 100 frames were enough to achieve excellent generalization. Data augmentation (such as rotations or translations) also resulted in minimal differences, demonstrating the data-efficiency of DeepLabCut. The feature detectors were able to translate pretty well to novel mouse behaviors as well as videos with multiple mice. Although it wasn't error free, this can be improved by simply training on that data or on data for multiple mice. End-to-end training allows the model to facilitate the localization of one body part based on other labeled body parts. The network that was trained with all body part labels simultaneously outperforms the specialized networks nearly twofold. Temporal information could indeed be beneficial in certain contexts, challenges remain to using end-to-end-trained deep architectures for video data to extract postures. Because of the curse of dimensionality, deep architectures on videos must rely on input images with lower spatial resolution, and thus the best-performing action recognition algorithms still rely on frame-by-frame analysis with deep networks pretrained on ImageNet as a result of hardware limitations. Therefore currently, in situations where occlusions are very common, such as in social behaviors, pairwise interactions could also be added to improve performance.","title":"DeepLabCut: Markerless Pose Estimation of User-Defined Body Parts With Deep Learning:"},{"location":"litsearch/#using-deeplabcut-for-3d-markerless-pose-estimation-across-species-and-behaviors","text":"Transfer learning, the ability to take a network, which was trained on a task with a large supervised data set, and utilize it for another task with a small supervised data set, is beginning to allow users to broadly apply deep learning methods. DeepLabCut provides tools to create annotated training sets, train robust feature detectors, and utilize them to analyze novel behavioral videos. The major motivation for developing the DeepLabCut toolbox was to provide a reliable and efficient tool for high-throughput video analysis, where powerful feature detectors of user-defined body parts need to be learned for a specific situation. The toolbox is aimed to solve the problem of detecting body parts in dynamic visual environments where varying background, reflective walls or motion blur hinder the performance of common techniques, such as thresholding or regression based on visual features. The user starts by creating a new project based on a project and username as well as some (initial) videos, which are required to create the training dataset (additional videos can also be added after the creation of the project). Next, DeepLabCut extracts frames, which reflect the diversity of the behavior with respect to postures, animal identities, etc. Then the user can label the points of interest in the extracted frames. These annotated frames can be visually checked for accuracy, and corrected if necessary. Eventually, a training dataset is created by merging all the extracted labeled frames and splitting them into subsets of test and train frames. Then, a pre-trained network (ResNet) is trained end-to-end to adapt its weights in order to predict the desired features. The performance of the trained network can then be evaluated on the training and test frames. The trained network can be used to analyze videos yielding extracted pose files. In case the trained network does not generalize well to unseen data in the evaluation and analysis step, then additional frames with poor results can be extracted and the predicted labels can be manually shifted to their ideal location. This refinement step, if needed, creates an additional set of annotated images that can then be merged with the original training dataset to create a new training dataset. This larger training set can then be used to re-train the feature detectors for better results. This active learning loop can be done iteratively to robustly and accurately analyze videos with potentially large variability- i.e. experiments that include many individuals, and run over long time periods. Furthermore, the user can add additional body parts/labels at later stages during a project as well as correct user-defined labels. However, if a user aims to track (adult) human poses, many excellent options exist, including DeeperCut, ArtTrack, DeepPose, OpenPose, and OpenPose-Plus (better for humans). DeepLabCut does not support occlusions, requires HPC and GPUs, and also workers faster with smaller images.","title":"Using DeepLabCut for 3D Markerless Pose Estimation Across Species and Behaviors:"},{"location":"litsearch/#track-anything","text":"Track-Anything is a flexible and interactive tool for video object tracking and segmentation. It is developed upon Segment Anything, can specify anything to track and segment via user clicks only. During tracking, users can flexibly change the objects they wanna track or correct the region of interest if there are any ambiguities. Track-Anything is suitable for video object tracking and segmentation with shot changes, visualized development and data annotation for video object tracking and segmentation, object-centric downstream video tasks (such as video inpainting and editing).","title":"Track-Anything:"},{"location":"litsearch/#track-anything-segment-anything-meets-videos","text":"Segment Anything Model (SAM) displays impressive performance on images, but it performs poorly on video segmentation. The proposed Track Anything Model (TAM), achieves high-performance interactive tracking and segmentation in videos. Current state-of-the-art video trackers/segmenters are trained on large-scale manually-annotated datasets and initialized by a bounding box or a segmentation mask. Moreover, current initialization settings, especially the semi-supervised VOS, need specific object mask groundtruth for model initialization. Large amouns of human labor is also required for huge amounts of annotated and labeled data. Recently, Segment-Anything Model (SAM) has been proposed, which is a large foundation model for image segmentation. It supports flexible prompts and computes masks in real-time, thus allowing interactive use. Trained on 11 million images and 1.1 billion masks, SAM can produce high-quality masks and do zero-shot segmentation in generic scenarios. With input user-friendly prompts of points, boxes, or language, SAM can give satisfactory segmentation masks on specific image areas. However, using SAM in videos directly does not give an impressive performance due to its deficiency in temporal correspondence. But tracking or segmenting in videos faces challenges from scale variation, target deformation, motion blur, camera motion, similar objects, and so on. In this paper, the Track-Anything toolkit is proposed for high-performance object tracking and segmentation in videos. With a user-friendly interface, the Track Anything Model (TAM) can track and segment any objects in a given video with only one-pass inference. TAM combines SAM, a large segmentation model, and XMem, an advanced VOS model. Firstly, users can interactively initialize the SAM, i.e., clicking on the object, to define a target object; then, XMem is used to give a mask prediction of the object in the next frame according to both temporal and spatial correspondence; next, SAM is utilized to give a more precise mask description; during the tracking process, users can pause and correct as soon as they notice tracking failures. TAM is able to promote the SAM applications to the video level to achieve interactive video object tracking and segmentation. Rather than separately using SAM per frame, it integrates SAM into the process of temporal correspondence construction. The Track Anything task aims for flexible object tracking in arbitrary videos. The target objects can be flexibly selected, added, or removed in any way according to the users\u2019 interests. Also, the video length and types can be arbitrary rather than limited to trimmed or natural videos. With such settings, diverse downstream tasks can be achieved, including single/multiple object tracking, short-/long-term object tracking, unsupervised VOS, semi-supervised VOS, referring VOS, interactive VOS, long-term VOS, and more. As a foundation model for image segmentation (and the genesis for TAM), SAM is based on ViT and trained on the large-scale dataset SA-1B. Obviously, SAM shows promising segmentation ability on images, especially on zero-shot segmentation tasks. Unfortunately, SAM only shows superior performance on image segmentation, while it cannot deal with complex video segmentation. Given the mask description of the target object at the first frame, XMem can track the object and generate corresponding masks in the subsequent frames. Inspired by the Atkinson-Shiffrin memory model, it aims to solve the difficulties in long-term videos with unified feature memory stores. The drawbacks of XMem are also obvious: as a semi-supervised VOS model, it requires a precise mask to initialize; for long videos, it is difficult for XMem to recover from tracking or segmentation failure. In this paper, we solve both difficulties by importing interactive tracking with SAM. The Track-Anything process is divided into the following processes: Initialization with SAM - As SAM provides an opportunity to segment a region of interest with weak prompts, e.g., points, and bounding boxes, we use it to give an initial mask of the target object. Following SAM, users can get a mask description of the interested object by a click or modify the object mask with several clicks to get a satisfactory initialization. Tracking with XMem - Given the initialized mask, XMem performs semi-supervised VOS on the following frames. Since XMem is an advanced VOS method that can output satisfactory results on simple scenarios, we output the predicted masks of XMem on most occasions. When the mask quality is not as good, we save the XMem predictions and corresponding intermediate parameters, i.e., probes and affinities, and skip to the next step. Refinement with SAM - During the inference of VOS models, keep predicting consistent and precise masks are challenging. In fact, most state-of-the-art VOS models tend to segment more and more coarsely over time during inference. Therefore, we utilize SAM to refine the masks predicted by XMem when its quality assessment is not satisfactory. Specifically, we project the probes and affinities to be point prompts for SAM, and the predicted mask from Step 2 is used as a mask prompt for SAM. Then, with these prompts, SAM is able to produce a refined segmentation mask. Such refined masks will also be added to the temporal correspondence of XMem to refine all subsequent object discrimination. Correction with human participation - After the above three steps, the TAM can now successfully solve some common challenges and predict segmentation masks. However, it is still difficult to accurately distinguish the objects in some extremely challenging scenarios, especially when processing long videos. Therefore, we propose to add human correction during inference, which can bring a qualitative leap in performance with only very small human efforts. In detail, users can compulsively stop the TAM process and correct the mask of the current frame with positive and negative clicks. TAM can handle multi-object separation, target deformation, scale change, and camera motion well, which demonstrates its superior tracking and segmentation abilities within only click initialization and one-round inference. Some failed cases: The failed cases typically appear on the following two occasions. Current VOS models are mostly designed for short videos, which focus more on maintaining short-term memory rather than long-term memory. This leads to mask shrinkage or lacking refinement in long-term videos, as shown in seq (a). Essentially, this is solved in step 3 by the refinement ability of SAM, while its effectiveness is lower than expected in realistic applications. It indicates that the ability of SAM refinement based on multiple prompts can be further improved in the future. On the other hand, human participation/interaction in TAM can be an approach to solving such difficulties, while too much interaction will also result in low efficiency. Thus, the mechanism of long-term memory preserving and transient memory updating is still important. When the object structure is complex, e.g., the bicycle wheels in seq (b) contain many cavities in groundtruth masks. It is very difficult to get a fine-grained initialized mask by propagating the clicks. Thus, the coarse initialized masks may have side effects on the subsequent frames and lead to poor predictions. This suggests that SAM is still struggling with complex and precision structures.","title":"Track Anything: Segment Anything Meets Videos:"},{"location":"litsearch/#xmem","text":"XMem frames Video Object Segmentation (VOS) as a memory problem. Prior works mostly use a single type of feature memory. This can be in the form of network weights (i.e., online learning), last frame segmentation (e.g., MaskTrack), spatial hidden representation (e.g., Conv-RNN-based methods), spatial-attentional features (e.g., STM, STCN, AOT), or some sort of long-term compact features (e.g., AFB-URR). Methods with a short memory span are not robust to changes, while those with a large memory bank are subject to a catastrophic increase in computation and GPU memory usage. Attempts at long-term attentional VOS like AFB-URR compress features eagerly as soon as they are generated, leading to a loss of feature resolution. XMem is inspired by the Atkinson-Shiffrin human memory model, which has a sensory memory, a working memory, and a long-term memory. These memory stores have different temporal scales and complement each other in our memory reading mechanism. It performs well in both short-term and long-term video datasets, handling videos with more than 10,000 frames with ease.","title":"XMem:"},{"location":"litsearch/#xmem-long-term-video-object-segmentation-with-an-atkinson-shiffrin-memory-model","text":"XMem is a video object segmentation architecture for long videos with unified feature memory stores inspired by the Atkinson-Shiffrin memory model. Prior work on video object segmentation typically only uses one type of feature memory. For videos longer than a minute, a single feature memory model tightly links memory consumption and accuracy. XMem uses an architecture that incorporates multiple independent yet deeply-connected feature memory stores: a rapidly updated sensory memory, a high-resolution working memory, and a compact thus sustained long-term memory. A memory potentiation algorithm that routinely consolidates actively used working memory elements into the long-term memory, which avoids memory explosion and minimizes performance decay for long-term prediction is developed. Combined with a new memory reading mechanism, XMem greatly exceeds state-of-the-art performance on long-video datasets while being on par with state-of-theart methods (that do not work on long videos) on short-video datasets. XMem relies on semi-supervised VOS by using a first-frame annotation, provided by the user, and segments objects in all other frames as accurately as possible while preferably running in real-time, online, and while having a small memory footprint even when processing long videos. VOS methods employ a feature memory to store relevant deep-net representations of an object to propogate annotations to other frames. Online learning methods use the weights of a network as their feature memory and recurrent methods propagate information often from the most recent frames, either via a mask or via a hidden representation. But these methods are prone to drifting and struggle with occlusions. Recent VOS methods store representations of past frames in the feature memory with features extracted from the newly observed query frame which needs to be segmented. Although these methods are high-performance, they require a large amount of GPU memory and struggle to handle videos longer than a minute. Methods designed for longer videos, however, often sacrifice on segmentation quality. It is proposed that this connection of performance and GPU memory consumption is a direct consequence of using a single feature memory type. Instead a unified memory architecture, dubbed XMem, is proposed. XMem maintains three independent yet deeply-connected feature memory stores: a rapidly updated sensory memory, a high-resolution working memory, and a compact thus sustained long-term memory. The sensory memory corresponds to the hidden representation of a GRU which is updated every frame. It provides temporal smoothness but fails for long-term prediction due to representation drift. To complement, the working memory is agglomerated from a subset of historical frames and considers them equally without drifting over time. To control the size of the working memory, XMem routinely consolidates its representations into the long-term memory, inspired by the consolidation mechanism in the human memory. XMem stores long-term memory as a set of highly compact prototypes. For this, they develop a memory potentiation algorithm that aggregates richer information into these prototypes to prevent aliasing due to sub-sampling. To read from the working and long-term memory, they devise a space-time memory reading operation. The three feature memory stores combined permit handling long videos with high accuracy while keeping GPU memory usage low. Given the image and target object mask at the first frame, XMem tracks the object and generates corresponding masks for subsequent query frames. For this, it first initializes the different feature memory stores using the inputs. For each subsequent query frame, it performs memory reading from long-term memory, working memory, and sensory memory respectively. The readout features are used to generate a segmentation mask. Then, it updates each of the feature memory stores at different frequencies. It updates the sensory memory every frame and inserts features into the working memory at every r-th frame. When the working memory reaches a pre-defined maximum of Tmax frames, it consolidates features from the working memory into the long-term memory in a highly compact form. When the long-term memory is also full (which only happens after processing thousands of frames), it discards obsolete features to bound the maximum GPU memory usage. These feature memory stores work in conjunction to provide high-quality features with low GPU memory usage even for very long videos. XMem consists of three end-to-end trainable convolutional networks as shown in Figure 3: a query encoder that extracts query-specific image features, a decoder that takes the output of the memory reading step to generate an object mask, and a value encoder that combines the image with the object mask to extract new memory features. See Section 3.6 for details of these networks. In the following, we will first describe the memory reading operation before discussing each feature memory store in detail. The method sometimes fails when the target object moves too quickly or has severe motion blur as even the fastest updating sensory memory cannot catch up. Comparisons: Failure Cases:","title":"XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model:"},{"location":"litsearch/#maskfreevis","text":"The recent advancement in Video Instance Segmentation (VIS) has largely been driven by the use of deeper and increasingly data-hungry transformer-based models. However, video masks are tedious and expensive to annotate, limiting the scale and diversity of existing VIS datasets. The solution proposed is MaskFreeVIS, which aims to remove the mask-annotation requirement, achieving highly competitive VIS performance, while only using bounding box annotations for the object state. It leverages the rich temporal mask consistency constraints in videos by introducing the Temporal KNN-patch Loss (TK-Loss), providing strong mask supervision without any labels. TK-Loss finds one-to-many matches across frames, through an efficient patch-matching step followed by a K-nearest neighbor selection. A consistency loss is then enforced on the found matches. The mask-free objective is simple to implement, has no trainable parameters, is computationally efficient, yet outperforms baselines employing, e.g., state-of-the-art optical flow to enforce temporal mask consistency. MaskFreeVIS is validated on the YouTube-VIS 2019/2021, OVIS and BDD100K MOTS benchmarks. The results clearly demonstrate the efficacy of the method by drastically narrowing the gap between fully and weakly-supervised VIS performance. MaskFreeVIS has high-performing video instance segmentation without using any video masks or even image mask labels. Using SwinL and built on Mask2Former, MaskFreeVIS achieved 56.0 AP on YTVIS without using any video masks labels. Using ResNet-101, MaskFreeVIS achieves 49.1 AP without using video masks, and 47.3 AP only using COCO mask initialized model. A new parameter-free Temporal KNN-patch Loss (TK-Loss), which leverages temporal masks consistency using unsupervised one-to-k patch correspondence is what MaskFreeVIS uses. TK-Loss is flexible to intergrated with state-of-the-art transformer-based VIS models, with no trainable parameters.","title":"MaskFreeVIS:"},{"location":"litsearch/#mask-free-video-instance-segmentation","text":"Video Instance Segmentation (VIS) requires jointly detecting, tracking and segmenting all objects in a video from a given set of categories. State-of-the-art VIS models are trained with complete video annotations from VIS datasets. However, video annotation is costly, in particular regarding objectmask labels. Even coarse polygon-based mask annotation is multiple times slower than annotating video bounding boxes. This makes existing VIS benchmarks difficult to scale, limiting the number of object categories covered, especially for recent transformer based VIS models. Current box-supervised instance segmentation models are created for images and achieve low accuracy when applied to videos because they do not utilize temporal cues. Instead, the MaskFreeVIS method is proposed, for high performance VIS without any mask annotations. To leverage temporal mask consistency, the Temporal KNN-patch Loss (TK-Loss) is introduced. To find regions corresponding to the same underlying video object, TK-Loss first builds correspondences across frames by patch-wise matching. For each target patch, only the top K matches in the neighboring frame with high enough matching score are selected. A temporal consistency loss is then applied to all found matches to promote the mask consistency. Specifically, the surrogate objective function not only promotes the one-to-k matched regions to reach the same mask probabilities, but also commits their mask prediction to a confident foreground or background prediction by entropy minimization. TK-Loss simply replaces the conventional video mask losses in supervising video mask generation. To further enforce temporal consistency through the video clip, TK-Loss is employed in a cyclic manner instead of using dense frame-wise connections. This greatly reduces memory cost with negligible performance drop. MaskFreeVIS achieves competitive VIS performance without using any video masks or even image mask labels on all datasets. Validated on various methods and backbones, MaskFreeVIS achieves 91.25% performance of its fully supervised counterparts, even outperforming a few recent fully-supervised methods on the popular YTVIS benchmark.","title":"Mask-Free Video Instance Segmentation:"},{"location":"litsearch/#yolo8","text":"Ultralytics YOLOv8 is the latest version of the acclaimed real-time object detection and image segmentation model. YOLOv8 is built on cutting-edge advancements in deep learning and computer vision, offering unparalleled performance in terms of speed and accuracy. Its streamlined design makes it suitable for various applications and easily adaptable to different hardware platforms, from edge devices to cloud APIs. Using YOLOv8 is as simple as installing ultralytics with pip and get it up and running in minutes. YOLOv8 helps you predict new images and videos. You can also train a new YOLOv8 model on your own custom dataset or use a pre-trained model. YOLOv8 also provides tasks like segment, classify, pose and track. YOLO's History: YOLO (You Only Look Once), a popular object detection and image segmentation model, was developed by Joseph Redmon and Ali Farhadi at the University of Washington. Launched in 2015, YOLO quickly gained popularity for its high speed and accuracy. YOLOv2, released in 2016, improved the original model by incorporating batch normalization, anchor boxes, and dimension clusters. YOLOv3, launched in 2018, further enhanced the model's performance using a more efficient backbone network, multiple anchors and spatial pyramid pooling. YOLOv4 was released in 2020, introducing innovations like Mosaic data augmentation, a new anchor-free detection head, and a new loss function. YOLOv5 further improved the model's performance and added new features such as hyperparameter optimization, integrated experiment tracking and automatic export to popular export formats. YOLOv6 was open-sourced by Meituan in 2022 and is in use in many of the company's autonomous delivery robots. YOLOv7 added additional tasks such as pose estimation on the COCO keypoints dataset. YOLOv8 is the latest version of YOLO by Ultralytics. As a cutting-edge, state-of-the-art (SOTA) model, YOLOv8 builds on the success of previous versions, introducing new features and improvements for enhanced performance, flexibility, and efficiency. YOLOv8 supports a full range of vision AI tasks, including detection, segmentation, pose estimation, tracking, and classification. This versatility allows users to leverage YOLOv8's capabilities across diverse applications and domains. YOLOv8 is an AI framework that supports multiple computer vision tasks. The framework can be used to perform detection, segmentation, classification, and pose estimation. Each of these tasks has a different objective and use case. Detection is the primary task supported by YOLOv8. It involves detecting objects in an image or video frame and drawing bounding boxes around them. The detected objects are classified into different categories based on their features. YOLOv8 can detect multiple objects in a single image or video frame with high accuracy and speed. Segmentation is a task that involves segmenting an image into different regions based on the content of the image. Each region is assigned a label based on its content. This task is useful in applications such as image segmentation and medical imaging. YOLOv8 uses a variant of the U-Net architecture to perform segmentation. Classification is a task that involves classifying an image into different categories. YOLOv8 can be used to classify images based on their content. It uses a variant of the EfficientNet architecture to perform classification. Pose/keypoint detection is a task that involves detecting specific points in an image or video frame. These points are referred to as keypoints and are used to track movement or pose estimation. YOLOv8 can detect keypoints in an image or video frame with high accuracy and speed.","title":"YOLO8:"},{"location":"litsearch/#matrix-comparison","text":"All qualitative fields will be marked on a scale of 1-5 with 1 representing a worse rating and 5 representing a great rating. The videos used to test are uploaded below. Single-Human Video (SH, 2.84 MB, 28 sec.) DeepLabCut Track-Anything YOLOv8 Multi-Human Video (MH, 2.07 MB, 14 sec.) DeepLabCut Track-Anything YOLOv8 Single-Animal Video (SA, 742 KB, 12 sec.) DeepLabCut Track-Anything YOLOv8 Multi-Animal Video (MA, 6.35 MB, 13 sec.) DeepLabCut Track-Anything YOLOv8 Method Use Case Test Link Number of Papers Using Tool Activeness Total Time to Test Processing Time Ease of Use Efficiency of Single-Human Segmentation Efficiency of Multi-Human Segmentation Efficiency of Single-Animal Segmentation Efficiency of Multi-Animal Segmentation DeepLabCut Trained Pose Estimation/ Tracking http://bit.ly/DLCMZ 3310 5 SH - 6m MH - 5m SA - 5m MA - 5m SH - 1m MH - 1m SA - 56s MA - 56s 4 4 2 3 3 Track-Anything Semi-supervised Segmentation/ Tracking https://bit.ly/TrAn 3 4 SH - 3m MH - 2m SA - 2m MA - 3m SH - 2m MH - 1m SA - 52s MA - 1m 5 4 5 5 4 XMem Trained Segmentation/ Tracking https://xxxxxx/xxxx 1500 1 SH - 0m MH - 0m SA - 0m MA - 0m SH - 0m MH - 0m SA - 0m MA - 0m 1 1 1 1 1 MaskFreeVIS Trained Segmentation/ Tracking https://xxxxxx/xxxx 2 1 SH - 0m MH - 0m SA - 0m MA - 0m SH - 0m MH - 0m SA - 0m MA - 0m 1 1 1 1 1 YOLOv8 Trained Pose Estimation/ Tracking https://bit.ly/YOLO8 382 5 SH - 2m MH - 2m SA - 3m MA - 5m SH - 1m MH - 45s SA - 1m MA - 1m 4 4 4 2 1","title":"Matrix &amp; Comparison"},{"location":"logbook/","text":"Logbook \u00b6 Day 1 (4/10) : \u00b6 Met with Dr. Merchant and discussed current standing and overall goals for the summer. Discussed possible interests and options and what can be started right now. Dr. Merchant talked about write-ups and possible publication, and recommended exploring Docker. Discussed previous projects as well and considered possible projects and pathways. Day 2 (6/7) : \u00b6 Second meeting with Dr. Merchant. Project was communicated over email along with lab agreement form. Possible project options were also discusses over email previously. Asked questions and clarified the project with Dr. Merchant as well as what could be done starting now. Learned about weekly meetings and standups as well as daily check-ins and check-outs and logs. Periodic demos were also mentioned. Dr. Merchant recommended learning UNIX and getting familiar with Linux Cl, Python, Jupyter Notebooks, Docker, and create a Git website. Prior research for video segmentation was also discussed for an understanding before the project. Day 3 (6/8) : \u00b6 Created and worked on Git website with some struggle on the Actions tab and getting it to auto-update and re-build the website. Also worked on UNIX and Git Bash commands. Will continue to go deeper into that tomorrow. Day 4 (6/9) : \u00b6 Finished up UNIX learning and testing as well as notes and documentation of learning on website. Learned version control with Git and took extensive notes on website, tried commands and created sample repository as well. Installed and started looking into Docker as well as Singularity and reading up on both softwares and learning the concept of containers. Concept of containerization and virtualization is still confusing and difference between VMs is unclear as well. Day 5 (6/12) : \u00b6 Solidified basic understanding of containers and containerization including use cases, advantages, differences from VMs, container clustering, and container images. Used a variety of sources and recorded notes and important information on website. Understood basic inner-workings and uses of Docker. Explored some commands and different aspects of Docker such as client, daemon, API, registry, and object. Took notes and recorded important and useful information on website. Recapped deep learning and deep learning concepts as well as benefits and use cases. Recorded important information on website and prepared for prior fact finding for actual project content (video segmentation). Day 6 (6/13) : \u00b6 Continued work on understanding video segmentation and the methods and models associated with it. Did not go into any specific research yet, but started gaining basic overview. Learned about VOS and VSS as well as the many different networks used in both methods. Finished learning and writing about VOS and main methods used in VOS. Started learning and working on VSS and got through one main method. Recorded learning on website with extensive notes as well as diagrams and images. Some technical concepts and technical words are still unclear, but are not needed for a basic understanding of methods. Can go into detail of the methods in the future if needed. Starting point and foundation has been established. Day 7 (6/14) : \u00b6 Continued work on understanding video segmentation and the methods and models associated with it. Finished learning about VSS methods and recorded findings and learnings on website. Also worked through challenges associated with video segmentation models. Started looking into video segmentation models/ packages such as DeepLabCut. Also started looking into literature search for such packages and software as well as reading through current papers on the topic. Got basic understanding of what DeepLabCut is, will continue understanding how it works and testing it as well. Day 8 (6/15) : \u00b6 Continued reading up on DeepLabCut and continued literature search. Started finding some papers and storing them for future study and possible use of their methods. Also researched what pipelines have been created in the past, if any, and what their use cases are. Also looked into point tracking and different methods for that. Started gathering questions for meeting with Dr. Merchant on Friday. Day 9 (6/16) : \u00b6 Continued literature search and reading up on current research and methods for video segmentation. Met with Dr. Merchant early in the day to discuss progress so far as well as upcoming timeline. Got many questions answered on actual tasks and use cases as well as clarified projection of project and tasks associated with it. Also created Trello board for timeline and planning of upcoming days and progress. Started getting research question down and moving forward with information on website as well. Day 10 (6/20) : \u00b6 Had to go back to DeepLabCut after conversation with Dr. Merchant. Read into it more and analyzed use cases as well as different research papers on the package. Understood how video segmentation (and software packages in general) work and how they can be used. Updated logbook with information and website with findings on DeepLabCut. Day 11 (6/21) : \u00b6 Finished reading both papers on DeepLabCut and gained working understanding of the package. Recorded everything on website along with visual models of how DeepLabCut works and what it does. Started searching for second and third method in order to research and learn more about. Found the package Track-Anything and started looking into it and seeing if it meets the needs of the project. Day 12 (6/22) : \u00b6 Read all about Track-Anything and learned how it works and what it is. Learned about segment anything models and how they differ from traditional video segmentation models like DeepLabCut. Also learned how models can be compounded and built off each other. For example, Track-Anything is based off SAM (Segment Anything Model) and XMem and therefore creates its own new output. Read the paper on Track-Anything and recorded findings on website. Also updated Trello board and started planning for next week. Day 13 (6/23) : \u00b6 Research the third package that could be used and evaluated against Track-Anything and DeepLabCut. Found a package called PaddleSeg and initially started reading through that, but realised it was only meant for image segmentation not video segmentation. Found another package called XMem (same one Track-Anything uses and is built off) and started researching and learning about it. Also asked Dr. Merchant some clarification questions and plans for next week after initial research has been done. Updated Trello board with plans for next week and shared link with Dr. Merchant. Finished reading and learning about XMem and its innovative memory allocation method and recorded all learnings on website. Day 14 (6/26) : \u00b6 Researched another package after starting to build the amatrix and realising that another method would be helpful. Found MaskFreeVIS which is unsupervised video segmentation. Researched MaskFreeVIS and read about how it works using TK-Loss and recorded learnings as well as findings on website. Also created criteria for matrix in order to find best method to use. Created table with multiple criteria and also found videos to use for testing all 4 methods. Found videos of single and multiple humans dancing and single and multiple dogs as well. Also started playing with DeepLabCut demo in Jupyter Notebook. Day 15 (6/27) : \u00b6 Started testing for all the criteria in the matrix. Tested DeepLabCut with all four videos and recorded all results in matrix as accurately as possible. Also repeated testing when unsure of something. Also tested Track-Anything and recorded all findings in matrix. Again, re-tested when in doubt of a recorded metric. So far, Track-Anything greatly outperformed DeepLabCut even though they have different ways of tracking (DeepLabCut has pose estimation, Track-Anything has traditional segmentation). Used DeepLabCut Model Zoo Jupyter Notebook for testing. Used HuggingFace demo of Track-Anything for testing. Also uploaded all 4 videos to website as well as segmented results of each video (segmented video from each method i.e. segmented dancing video that Track-Anything produced). Day 16 (6/28) : \u00b6 Started testing XMem package on Google Colab demo. Tested with the singular human dancing video first, but had to create a mask for XMem to work. Created mask using image segmentation software on Hugging Face and then through image editing in order to get the background black and the objects that needed to be detected in one solid color. Once the mask was created, plugged it into the Colab file along with the video but it did not work. Tried again and again by refining the mask and trying to make it in different ways and different colors and trying different image segmentation methods. XMem software still did not work by end of day. Decided to move on and come back later. Day 17 (6/29) : \u00b6 Tried to test MaskFreeVIS software today. Looked all through the GitHub repo but did not find any instructions on what to do and how to do it. Looked through all three different repos (PyTorch, Tensorflow, and Cuda) but to no avail. No instructions or references for testing were to be found. Looked through different websites and publications and the official MaskFreeVIS website as well. Also looked at each folder to try and see .txt files or if there are instructions in code files. Finally tried cloning the repo into Google Colab and trying something that way, but there were no instructions and ways to execute the code. Turned out to be a dead-end. Day 18 (6/30) : \u00b6 Re-tried both methods after countless failures the past two days. Starting to get very frustrated now. Retried XMem with different videos and different masks. Tried the single dog video and mask with different colors for the dog and different refined and unrefined masks, but still did not work. Even tried to remove an object from the example mask and try it, but still did not work. Tried a simple rectangle and oval on the mask and even no object on the mask (just a black image) but still did not get any results. Tried to find some sort of instruction or reference for MaskFreeVIS as well, looking through different publications as well as searching the internet, but found nothing. Had a meeting with Dr. Merchant and discussed progress so far as well as future steps. Next steps are to test the current methods with Brian Carter's data as well as some mice data and see how well they work and then report results. Dr. Merchant instructed to not waste time on XMem and MaskFreeVIS anymore since they did not work, instead add YOLO to the matrix next week. Also discussed opportunity after KEYS. Day 19 (7/3) : \u00b6 Read and researched information on YOLOv8 and recorded findings on website. Also looked at different sources to see the history of YOLO as well as the activeness and usefulness in the community. Conversed with Dr. Merchant and recieved some mice videos for further testing of methods as well as directions on how to apply YOLO to pose estimation. Looked at multiple videos and documentation sources for YOLOv8 in order to come up with a Colab file that can estimate poses in videos. Added to matrix and conducting testing on all 4 types of videos (single and multi human, single and multi animal) with good results for human pose estimation and worse results for animal pose estimation. Also used Brian Carter's video and conducted testing on the Track-Anything algorithm. Had to split the video into 4 parts for easy and efficient processing by the Track-Anything model. recorded all results on website and plan to continue testing Brian Carter's videos and mice videos with the 3 models (DLC, TA, YOLO). Day 20 (7/5) : \u00b6 Continued testing on Brian Carter videos with each method. Read up on new method in the field provided by Dr. Merchant (SAM w/ Point Tracking). No repo or source code for this new method yet, will continue to monitor, maybe it can be added into the matrix and tested as well. Brian Carter's Children of the Row Solo video yielded good segmentation results with Track-Anything and DeepLabCut but not with YOLO. Videos were run through all three models and saved locally for now and then added to a Google Drive folder for sharing, will be added to the website later with private link attachments to ensure the data is secured and only accesible to certain individuals. Started working on next video as well (American Mo). Had to segment video into seperate clips first, as did with Children of the Row Solo as well. Started plugging into Track Anything but started running into processing issues and long processing times. Will resume tomorrow after clearing cache and history to try and improve performance and possibly make video clips smaller as well. Day 21 (7/6) : \u00b6 Continued testing and segmenting American Mo videos using all three methods. Track-Anything still proved to have processing issues, divided videos up unti 10 second clips again and tried to process them. Processing was still slow but worked when given enough time. Processed videos through DLC and YOLO as well. Track-Anything yielded good results, DLC yielded moderate, and YOLOv8 did not yield good results for American Mo. Overall it was a difficult video to segment and took large processing times as well. All videos were tested and downloaded locally first, and then uploaded to Google Drive and shared with Dr. Merchant. Day 22 (7/7) : \u00b6 All videos that had been segmented in parts were combined into original length videos as intended by the Dr. Carter. Finished processing some leftover videos for Track-Anything from yesterday for American Mo since processing was taking too long. Finished processing and combining American Mo and uploaded finished products to Google Drive. Started learning about workflow managers such as SnakeMake and Nextflow. Also started looking into UA HPC and what the options in it are. Started learning about parallel computing on different cores and how to run processes faster using parallel computing. Day 23 (7/10) : \u00b6 Started working on data pipeline aftter discussion with Dr. Merchant. Used early part of the day to converse and clarify exact goals and specifications of the pipeline as well as what tools we would use and for what purpose. Also started research all three workflow managers/ engines (Snakemake, Nextflow, Makeflow). Started working on installation and looking through guides for each. Installed Snakemake using WSL (Windoes Subsystem Linux) but ran into lots of issues and found the process as well as the setup of linux on a windows machine to be too much of a hassle. Tried Nextflow installation using WSL as well, but failed due to some Java issues that I could not find solutions to. Tried to install Makeflow as well using conda and installer, but ran into issues with conda not being able to find the package needed (ndcctools). Tried to do seperate research to find a different and easier to use workflow manager and found Luigi as well as Parsl, but am not sure if they meet the requirements. Also worked on poster throughout the day. Ended day by posting questions to Dr. Merchant in slack to see what are some other options. Day 24 (7/11) : \u00b6 Continued work on data pipeline. Spent a majority of the day trying to figure out how Snakemake works and how workflows are created. After talking with Dr. Merchant, started using CyVerse cloud shell at first, but then that started maintennance, so switched to UA HPC Interactive Desktop (which has native linux) in order to work with Snakemake easily. Read up on how to use UA HPC along with how to allocate resources and work with Linux. Created first Snakemake pipeline for splitting video into 10 second chunks. Did lots of research on existing solutions, but had to create new solution using ffmpeg command in shell ran through Snakemake workflow. Started working on DLC integration with pipeline, trying to see if Snakemake works with Google Colab. Day 25 (7/12) : \u00b6 Kept working on data pipeline. Created a python script to run DLC modelzoo demo locally instead of on Google Colab. Started using script for analysis and processing of videos. Created a way to run the python script using the shell and by inputing a video file of the user's choice. Created a snakemake rule for DLC processing and started trying to connect that to the Snakemake split rule. Goal is to use the output from the Snakemake split rule as input for the DLC processing rule and produce segmented clips. Kept working and researchign for ways to do this, tried to use wildcards etc but was not able to find any way to make it work. Also had to switch back to CyVerse Desktop because UA HPC was not working with DLC. Continued research and work on poster. Day 26 (7/13) : \u00b6 Continued research and work on combining DLC and splitting script. Did significant work on poster in order to make sure it is ready for presentation time. Added information from website and segmented testing videos to display research effectively. Scheduled meeting with Zi Deng to get help on Snakemake script and try and get it to work before the poster is due. Also attended meeting with Dr. Merchant, Dr. Mike Hammer, and Collin K. Discussed what Dr. Hammer needs for mouse tracking and seizure detection and started discussing what solutions we can provide to them. Need an automated way to detect and log seizures in mice. Possible using DLC and pose estimation (based on the crescent shape). Recieved videos for testing and training from Collin, ready to start work on this new issue once KEYS is sorted. Day 27 (7/14) : \u00b6 Finished poster by adding final touches and submitted for approval for KEYS Crew. Recieved edits to make from Alexis and Robyn and made those edits on the poster and then resubmitted for final approval. After confirmation from KEYS Crew, submitted poster and slideshow for approval to Dr. Merchant and Jordan. Recieved feedback from Dr. Merchant and made changes accordingly and also recieved feedback from Jordan and made changes accordingly. Submitted for final approval over the weekend after getting requested changes as well. Continued work on the pipeline with not much success, will continue spending some more time moving forward, but not much since it is not a priority. Will try and spend time rehearsing for KEYS as well as spend time analyzing Dr. Hammer's videos if time permits. Day 28 (7/17) : \u00b6 Made final edits to poster and recieved approval from Jordan. Did final reviews over poster and then submitted to KEYS for printing. Submitted presentation as well after adding final citations and small updates. Continued work on Snakemake pipeline with no success. Tried to look at documentation and tried various different methods to no avail. Tried to create a new analysis in CyVerse and use pure python to create the pipeline rather than using Snakemake. Starting coding pipeline in python and created function to split files and process DLC but started connecting and ran into some issues. Working to see if pure python script can work for the pipeline. Also started working on getting to know process for continuing with UofA research after KEYS. Day 29 (7/18) : \u00b6 Continued work and research on pipeline with little success. DLC processing only process first video not all videos after they are cut up into 5 second chunks. Tried to continue work on pipeline to make it work along with practicing for KEYS Slam and Showcase. Pipeline will not be ready for end of KEYS most likely, and will be halted after KEYS due to focus on Dr. Hammer's research. Continued to practice presentations and delivery of research as well as format of elevator speech. Day 30 (7/19) : \u00b6 Continued some work on pipeline but mostly moved on to practicing for KEYS presentation and slam. Had meeting with KEYS group to practice and review speeches and presentations. Continued practice throughout the day with some work on pipeline to get it down. Using pure python for pipeline did not change much in terms of efficiency. After KEYS is done, will try using Nextflow instead of Snakemake in order to allow for easier pipelining. Day 31 (7/20) : \u00b6 Traveled to Tucson and recorded KEYS Slam presentation. Met all the KEYS staff and interns and had a great time overall. Started getting ready for poster session as well. Day 32 (7/21) : \u00b6 Finally presented poster and showcased my research to everyone. Recieved great feedback and compliments on my poster as well as my research. Had an amazing time with the interns and the experience of presenting research. Attended closing ceremony and recieved certificate as well as attended program debrief after. Concluded KEYS on a high note and had a great experience. Looking forward to continuing research with Dr. Merchant.","title":"Logbook"},{"location":"logbook/#logbook","text":"","title":"Logbook"},{"location":"logbook/#day-1-410","text":"Met with Dr. Merchant and discussed current standing and overall goals for the summer. Discussed possible interests and options and what can be started right now. Dr. Merchant talked about write-ups and possible publication, and recommended exploring Docker. Discussed previous projects as well and considered possible projects and pathways.","title":"Day 1 (4/10):"},{"location":"logbook/#day-2-67","text":"Second meeting with Dr. Merchant. Project was communicated over email along with lab agreement form. Possible project options were also discusses over email previously. Asked questions and clarified the project with Dr. Merchant as well as what could be done starting now. Learned about weekly meetings and standups as well as daily check-ins and check-outs and logs. Periodic demos were also mentioned. Dr. Merchant recommended learning UNIX and getting familiar with Linux Cl, Python, Jupyter Notebooks, Docker, and create a Git website. Prior research for video segmentation was also discussed for an understanding before the project.","title":"Day 2 (6/7):"},{"location":"logbook/#day-3-68","text":"Created and worked on Git website with some struggle on the Actions tab and getting it to auto-update and re-build the website. Also worked on UNIX and Git Bash commands. Will continue to go deeper into that tomorrow.","title":"Day 3 (6/8):"},{"location":"logbook/#day-4-69","text":"Finished up UNIX learning and testing as well as notes and documentation of learning on website. Learned version control with Git and took extensive notes on website, tried commands and created sample repository as well. Installed and started looking into Docker as well as Singularity and reading up on both softwares and learning the concept of containers. Concept of containerization and virtualization is still confusing and difference between VMs is unclear as well.","title":"Day 4 (6/9):"},{"location":"logbook/#day-5-612","text":"Solidified basic understanding of containers and containerization including use cases, advantages, differences from VMs, container clustering, and container images. Used a variety of sources and recorded notes and important information on website. Understood basic inner-workings and uses of Docker. Explored some commands and different aspects of Docker such as client, daemon, API, registry, and object. Took notes and recorded important and useful information on website. Recapped deep learning and deep learning concepts as well as benefits and use cases. Recorded important information on website and prepared for prior fact finding for actual project content (video segmentation).","title":"Day 5 (6/12):"},{"location":"logbook/#day-6-613","text":"Continued work on understanding video segmentation and the methods and models associated with it. Did not go into any specific research yet, but started gaining basic overview. Learned about VOS and VSS as well as the many different networks used in both methods. Finished learning and writing about VOS and main methods used in VOS. Started learning and working on VSS and got through one main method. Recorded learning on website with extensive notes as well as diagrams and images. Some technical concepts and technical words are still unclear, but are not needed for a basic understanding of methods. Can go into detail of the methods in the future if needed. Starting point and foundation has been established.","title":"Day 6 (6/13):"},{"location":"logbook/#day-7-614","text":"Continued work on understanding video segmentation and the methods and models associated with it. Finished learning about VSS methods and recorded findings and learnings on website. Also worked through challenges associated with video segmentation models. Started looking into video segmentation models/ packages such as DeepLabCut. Also started looking into literature search for such packages and software as well as reading through current papers on the topic. Got basic understanding of what DeepLabCut is, will continue understanding how it works and testing it as well.","title":"Day 7 (6/14):"},{"location":"logbook/#day-8-615","text":"Continued reading up on DeepLabCut and continued literature search. Started finding some papers and storing them for future study and possible use of their methods. Also researched what pipelines have been created in the past, if any, and what their use cases are. Also looked into point tracking and different methods for that. Started gathering questions for meeting with Dr. Merchant on Friday.","title":"Day 8 (6/15):"},{"location":"logbook/#day-9-616","text":"Continued literature search and reading up on current research and methods for video segmentation. Met with Dr. Merchant early in the day to discuss progress so far as well as upcoming timeline. Got many questions answered on actual tasks and use cases as well as clarified projection of project and tasks associated with it. Also created Trello board for timeline and planning of upcoming days and progress. Started getting research question down and moving forward with information on website as well.","title":"Day 9 (6/16):"},{"location":"logbook/#day-10-620","text":"Had to go back to DeepLabCut after conversation with Dr. Merchant. Read into it more and analyzed use cases as well as different research papers on the package. Understood how video segmentation (and software packages in general) work and how they can be used. Updated logbook with information and website with findings on DeepLabCut.","title":"Day 10 (6/20):"},{"location":"logbook/#day-11-621","text":"Finished reading both papers on DeepLabCut and gained working understanding of the package. Recorded everything on website along with visual models of how DeepLabCut works and what it does. Started searching for second and third method in order to research and learn more about. Found the package Track-Anything and started looking into it and seeing if it meets the needs of the project.","title":"Day 11 (6/21):"},{"location":"logbook/#day-12-622","text":"Read all about Track-Anything and learned how it works and what it is. Learned about segment anything models and how they differ from traditional video segmentation models like DeepLabCut. Also learned how models can be compounded and built off each other. For example, Track-Anything is based off SAM (Segment Anything Model) and XMem and therefore creates its own new output. Read the paper on Track-Anything and recorded findings on website. Also updated Trello board and started planning for next week.","title":"Day 12 (6/22):"},{"location":"logbook/#day-13-623","text":"Research the third package that could be used and evaluated against Track-Anything and DeepLabCut. Found a package called PaddleSeg and initially started reading through that, but realised it was only meant for image segmentation not video segmentation. Found another package called XMem (same one Track-Anything uses and is built off) and started researching and learning about it. Also asked Dr. Merchant some clarification questions and plans for next week after initial research has been done. Updated Trello board with plans for next week and shared link with Dr. Merchant. Finished reading and learning about XMem and its innovative memory allocation method and recorded all learnings on website.","title":"Day 13 (6/23):"},{"location":"logbook/#day-14-626","text":"Researched another package after starting to build the amatrix and realising that another method would be helpful. Found MaskFreeVIS which is unsupervised video segmentation. Researched MaskFreeVIS and read about how it works using TK-Loss and recorded learnings as well as findings on website. Also created criteria for matrix in order to find best method to use. Created table with multiple criteria and also found videos to use for testing all 4 methods. Found videos of single and multiple humans dancing and single and multiple dogs as well. Also started playing with DeepLabCut demo in Jupyter Notebook.","title":"Day 14 (6/26):"},{"location":"logbook/#day-15-627","text":"Started testing for all the criteria in the matrix. Tested DeepLabCut with all four videos and recorded all results in matrix as accurately as possible. Also repeated testing when unsure of something. Also tested Track-Anything and recorded all findings in matrix. Again, re-tested when in doubt of a recorded metric. So far, Track-Anything greatly outperformed DeepLabCut even though they have different ways of tracking (DeepLabCut has pose estimation, Track-Anything has traditional segmentation). Used DeepLabCut Model Zoo Jupyter Notebook for testing. Used HuggingFace demo of Track-Anything for testing. Also uploaded all 4 videos to website as well as segmented results of each video (segmented video from each method i.e. segmented dancing video that Track-Anything produced).","title":"Day 15 (6/27):"},{"location":"logbook/#day-16-628","text":"Started testing XMem package on Google Colab demo. Tested with the singular human dancing video first, but had to create a mask for XMem to work. Created mask using image segmentation software on Hugging Face and then through image editing in order to get the background black and the objects that needed to be detected in one solid color. Once the mask was created, plugged it into the Colab file along with the video but it did not work. Tried again and again by refining the mask and trying to make it in different ways and different colors and trying different image segmentation methods. XMem software still did not work by end of day. Decided to move on and come back later.","title":"Day 16 (6/28):"},{"location":"logbook/#day-17-629","text":"Tried to test MaskFreeVIS software today. Looked all through the GitHub repo but did not find any instructions on what to do and how to do it. Looked through all three different repos (PyTorch, Tensorflow, and Cuda) but to no avail. No instructions or references for testing were to be found. Looked through different websites and publications and the official MaskFreeVIS website as well. Also looked at each folder to try and see .txt files or if there are instructions in code files. Finally tried cloning the repo into Google Colab and trying something that way, but there were no instructions and ways to execute the code. Turned out to be a dead-end.","title":"Day 17 (6/29):"},{"location":"logbook/#day-18-630","text":"Re-tried both methods after countless failures the past two days. Starting to get very frustrated now. Retried XMem with different videos and different masks. Tried the single dog video and mask with different colors for the dog and different refined and unrefined masks, but still did not work. Even tried to remove an object from the example mask and try it, but still did not work. Tried a simple rectangle and oval on the mask and even no object on the mask (just a black image) but still did not get any results. Tried to find some sort of instruction or reference for MaskFreeVIS as well, looking through different publications as well as searching the internet, but found nothing. Had a meeting with Dr. Merchant and discussed progress so far as well as future steps. Next steps are to test the current methods with Brian Carter's data as well as some mice data and see how well they work and then report results. Dr. Merchant instructed to not waste time on XMem and MaskFreeVIS anymore since they did not work, instead add YOLO to the matrix next week. Also discussed opportunity after KEYS.","title":"Day 18 (6/30):"},{"location":"logbook/#day-19-73","text":"Read and researched information on YOLOv8 and recorded findings on website. Also looked at different sources to see the history of YOLO as well as the activeness and usefulness in the community. Conversed with Dr. Merchant and recieved some mice videos for further testing of methods as well as directions on how to apply YOLO to pose estimation. Looked at multiple videos and documentation sources for YOLOv8 in order to come up with a Colab file that can estimate poses in videos. Added to matrix and conducting testing on all 4 types of videos (single and multi human, single and multi animal) with good results for human pose estimation and worse results for animal pose estimation. Also used Brian Carter's video and conducted testing on the Track-Anything algorithm. Had to split the video into 4 parts for easy and efficient processing by the Track-Anything model. recorded all results on website and plan to continue testing Brian Carter's videos and mice videos with the 3 models (DLC, TA, YOLO).","title":"Day 19 (7/3):"},{"location":"logbook/#day-20-75","text":"Continued testing on Brian Carter videos with each method. Read up on new method in the field provided by Dr. Merchant (SAM w/ Point Tracking). No repo or source code for this new method yet, will continue to monitor, maybe it can be added into the matrix and tested as well. Brian Carter's Children of the Row Solo video yielded good segmentation results with Track-Anything and DeepLabCut but not with YOLO. Videos were run through all three models and saved locally for now and then added to a Google Drive folder for sharing, will be added to the website later with private link attachments to ensure the data is secured and only accesible to certain individuals. Started working on next video as well (American Mo). Had to segment video into seperate clips first, as did with Children of the Row Solo as well. Started plugging into Track Anything but started running into processing issues and long processing times. Will resume tomorrow after clearing cache and history to try and improve performance and possibly make video clips smaller as well.","title":"Day 20 (7/5):"},{"location":"logbook/#day-21-76","text":"Continued testing and segmenting American Mo videos using all three methods. Track-Anything still proved to have processing issues, divided videos up unti 10 second clips again and tried to process them. Processing was still slow but worked when given enough time. Processed videos through DLC and YOLO as well. Track-Anything yielded good results, DLC yielded moderate, and YOLOv8 did not yield good results for American Mo. Overall it was a difficult video to segment and took large processing times as well. All videos were tested and downloaded locally first, and then uploaded to Google Drive and shared with Dr. Merchant.","title":"Day 21 (7/6):"},{"location":"logbook/#day-22-77","text":"All videos that had been segmented in parts were combined into original length videos as intended by the Dr. Carter. Finished processing some leftover videos for Track-Anything from yesterday for American Mo since processing was taking too long. Finished processing and combining American Mo and uploaded finished products to Google Drive. Started learning about workflow managers such as SnakeMake and Nextflow. Also started looking into UA HPC and what the options in it are. Started learning about parallel computing on different cores and how to run processes faster using parallel computing.","title":"Day 22 (7/7):"},{"location":"logbook/#day-23-710","text":"Started working on data pipeline aftter discussion with Dr. Merchant. Used early part of the day to converse and clarify exact goals and specifications of the pipeline as well as what tools we would use and for what purpose. Also started research all three workflow managers/ engines (Snakemake, Nextflow, Makeflow). Started working on installation and looking through guides for each. Installed Snakemake using WSL (Windoes Subsystem Linux) but ran into lots of issues and found the process as well as the setup of linux on a windows machine to be too much of a hassle. Tried Nextflow installation using WSL as well, but failed due to some Java issues that I could not find solutions to. Tried to install Makeflow as well using conda and installer, but ran into issues with conda not being able to find the package needed (ndcctools). Tried to do seperate research to find a different and easier to use workflow manager and found Luigi as well as Parsl, but am not sure if they meet the requirements. Also worked on poster throughout the day. Ended day by posting questions to Dr. Merchant in slack to see what are some other options.","title":"Day 23 (7/10):"},{"location":"logbook/#day-24-711","text":"Continued work on data pipeline. Spent a majority of the day trying to figure out how Snakemake works and how workflows are created. After talking with Dr. Merchant, started using CyVerse cloud shell at first, but then that started maintennance, so switched to UA HPC Interactive Desktop (which has native linux) in order to work with Snakemake easily. Read up on how to use UA HPC along with how to allocate resources and work with Linux. Created first Snakemake pipeline for splitting video into 10 second chunks. Did lots of research on existing solutions, but had to create new solution using ffmpeg command in shell ran through Snakemake workflow. Started working on DLC integration with pipeline, trying to see if Snakemake works with Google Colab.","title":"Day 24 (7/11):"},{"location":"logbook/#day-25-712","text":"Kept working on data pipeline. Created a python script to run DLC modelzoo demo locally instead of on Google Colab. Started using script for analysis and processing of videos. Created a way to run the python script using the shell and by inputing a video file of the user's choice. Created a snakemake rule for DLC processing and started trying to connect that to the Snakemake split rule. Goal is to use the output from the Snakemake split rule as input for the DLC processing rule and produce segmented clips. Kept working and researchign for ways to do this, tried to use wildcards etc but was not able to find any way to make it work. Also had to switch back to CyVerse Desktop because UA HPC was not working with DLC. Continued research and work on poster.","title":"Day 25 (7/12):"},{"location":"logbook/#day-26-713","text":"Continued research and work on combining DLC and splitting script. Did significant work on poster in order to make sure it is ready for presentation time. Added information from website and segmented testing videos to display research effectively. Scheduled meeting with Zi Deng to get help on Snakemake script and try and get it to work before the poster is due. Also attended meeting with Dr. Merchant, Dr. Mike Hammer, and Collin K. Discussed what Dr. Hammer needs for mouse tracking and seizure detection and started discussing what solutions we can provide to them. Need an automated way to detect and log seizures in mice. Possible using DLC and pose estimation (based on the crescent shape). Recieved videos for testing and training from Collin, ready to start work on this new issue once KEYS is sorted.","title":"Day 26 (7/13):"},{"location":"logbook/#day-27-714","text":"Finished poster by adding final touches and submitted for approval for KEYS Crew. Recieved edits to make from Alexis and Robyn and made those edits on the poster and then resubmitted for final approval. After confirmation from KEYS Crew, submitted poster and slideshow for approval to Dr. Merchant and Jordan. Recieved feedback from Dr. Merchant and made changes accordingly and also recieved feedback from Jordan and made changes accordingly. Submitted for final approval over the weekend after getting requested changes as well. Continued work on the pipeline with not much success, will continue spending some more time moving forward, but not much since it is not a priority. Will try and spend time rehearsing for KEYS as well as spend time analyzing Dr. Hammer's videos if time permits.","title":"Day 27 (7/14):"},{"location":"logbook/#day-28-717","text":"Made final edits to poster and recieved approval from Jordan. Did final reviews over poster and then submitted to KEYS for printing. Submitted presentation as well after adding final citations and small updates. Continued work on Snakemake pipeline with no success. Tried to look at documentation and tried various different methods to no avail. Tried to create a new analysis in CyVerse and use pure python to create the pipeline rather than using Snakemake. Starting coding pipeline in python and created function to split files and process DLC but started connecting and ran into some issues. Working to see if pure python script can work for the pipeline. Also started working on getting to know process for continuing with UofA research after KEYS.","title":"Day 28 (7/17):"},{"location":"logbook/#day-29-718","text":"Continued work and research on pipeline with little success. DLC processing only process first video not all videos after they are cut up into 5 second chunks. Tried to continue work on pipeline to make it work along with practicing for KEYS Slam and Showcase. Pipeline will not be ready for end of KEYS most likely, and will be halted after KEYS due to focus on Dr. Hammer's research. Continued to practice presentations and delivery of research as well as format of elevator speech.","title":"Day 29 (7/18):"},{"location":"logbook/#day-30-719","text":"Continued some work on pipeline but mostly moved on to practicing for KEYS presentation and slam. Had meeting with KEYS group to practice and review speeches and presentations. Continued practice throughout the day with some work on pipeline to get it down. Using pure python for pipeline did not change much in terms of efficiency. After KEYS is done, will try using Nextflow instead of Snakemake in order to allow for easier pipelining.","title":"Day 30 (7/19):"},{"location":"logbook/#day-31-720","text":"Traveled to Tucson and recorded KEYS Slam presentation. Met all the KEYS staff and interns and had a great time overall. Started getting ready for poster session as well.","title":"Day 31 (7/20):"},{"location":"logbook/#day-32-721","text":"Finally presented poster and showcased my research to everyone. Recieved great feedback and compliments on my poster as well as my research. Had an amazing time with the interns and the experience of presenting research. Attended closing ceremony and recieved certificate as well as attended program debrief after. Concluded KEYS on a high note and had a great experience. Looking forward to continuing research with Dr. Merchant.","title":"Day 32 (7/21):"},{"location":"myproject/","text":"My Project \u00b6 Brian Carter Video Tests: \u00b6 Children of the Row Solo Original Video : DeepLabCut : Track-Anything : YOLOv8 : American MO Pt 2 Original Video : DeepLabCut : Track-Anything : YOLOv8 : Mice Video Tests: \u00b6","title":"My Project"},{"location":"myproject/#my-project","text":"","title":"My Project"},{"location":"myproject/#brian-carter-video-tests","text":"Children of the Row Solo Original Video : DeepLabCut : Track-Anything : YOLOv8 : American MO Pt 2 Original Video : DeepLabCut : Track-Anything : YOLOv8 :","title":"Brian Carter Video Tests:"},{"location":"myproject/#mice-video-tests","text":"","title":"Mice Video Tests:"},{"location":"poster/","text":"","title":"Poster"},{"location":"priorresearch/","text":"Prior Research/ Initial Fact-finding \u00b6 Project Description: \u00b6 Concepts Overview: \u00b6 Links: \u00b6 What is Deep Learning What is Video Segmentation Deep Learning: \u00b6 Deep learning is a method in artificial intelligence (AI) that teaches computers to process data in a way that is inspired by the human brain. Deep learning models can recognize complex patterns in pictures, text, sounds, and other data to produce accurate insights and predictions. Deep learning methods can also be used to automate tasks that typically require human intelligence, such as describing images or transcribing a sound file into text. Deep learning is used heavily in many different machine learning use cases and concepts like computer vision, speech recognition, natural language processing, and recommendation engines. Computer vision is the computer's ability to extract information and insights from images and videos. Computers can use deep learning techniques to comprehend images in the same way that humans do. Deep learning models can analyze human speech despite varying speech patterns, pitch, tone, language, and accent. Computers use deep learning algorithms to gather insights and meaning from text data and documents. Applications can use deep learning methods to track user activity and develop personalized recommendations. They can analyze the behavior of various users and help them discover new products or services. Deep learning models use neural networks which contain thousands of artificial nodes and neurons in order to process data in a way similar to humans. These networks contains many layers that process data from the input layer and release the processed data to the output layer. The input layer of a artificial neural network (ANN) has several nodes that input data into the network. The data is then passed into the hidden layer which processes and passes the data to layers further in the neural network. These hidden layers process information at different levels, adapting their behavior as they receive new information. Deep learning networks have hundreds of hidden layers that they can use to analyze a problem from several different angles. The processed data is then passed into the output layer which consists of the nodes that output the data. Deep learning models that output \"yes\" or \"no\" answers have only two nodes in the output layer. On the other hand, those that output a wider range of answers have more nodes. Deep learning is a subset of machine learning. Deep learning algorithms emerged in an attempt to make traditional machine learning techniques more efficient. Traditional machine learning methods utilize what is known as supervised learning in which the humans have to assist in training the machine. Deep learning generally relies on unsupervised learning and finds patterns in the data on its own, therefore making it much more efficient. Deep learning is better than machine learning because of a couple reasons. Deep learning models can comprehend unstructured data and make general observations without manual feature extraction. A deep learning application can analyze large amounts of data more deeply and reveal new insights for which it might not have been trained. In this way deep learning has an advantage for finding hidden relationships and discovering patterns. Deep learning models can learn and improve over time based on user behavior. They do not require large variations of labeled datasets. Once again, this is where unsupervised learning benefits deep learning models. However deep learning models have some drawbacks as well. They work a lot better when they are trained on large, high-quality datasets, and outliers or mistakes in the input dataset can significantly affect the deep learning process. Because of this, deep learning requires lots of data pre-processing and data storage capacity as well. Deep learning algorithms are also compute-intensive and require infrastructure with sufficient compute capacity to properly function. Otherwise, they take a long time to process results. Video Segmentation: \u00b6 Video segmentation is the process by which videos are partitioned into seperate regions by a variety of characteristics. These characteristics include object boundaries, motion, color, texture, or other visual features. The goal of video segmentation is to seperate different objects from the background in a video and to provide a more detailed representation of the content. Video segmentation is a very useful technology especially in the field of computer vision since it allows for the identification and characterization of individual objects and events in the video as well as the organization and classification of video content. Video segmentation divdes the content into individual segments or shots (or frames) which can then be characterized and analyzed based on pre-defined attributes. It can also be performed at various levels of granularity, from a single object to whole backgrounds. Video segmentation uses two broad techniques, Video Object Segmentation (VOS) and Video Semantic Segmentation (VSS). VOS focuses on tracking objects within a video and is used in applications such as surveillance and autonomous vehicles. VSS focuses on understanding the overall scene and its contents and is used in applications such as augmented reality and video summarization. Video Object Segmentation (VOS) Methods and Models: \u00b6 VOS is the task of segmenting and tracking specific objects within a video. This is typically done by object initialization\u2014identifying the object in the first frame of the video\u2014and then tracking its movement throughout the rest of the video. The goal is to segment the object from the background and the follow the changes in its movement throughout the video. There are various methods for object initialization, such as manual annotation (most accurate but most time-consuming), automatic annotation (least accurate but fastest), semi-automatic annotation (balances accuracy and speed). After initialization the object must be tracked throughout the video. Methods for object tracking include traditional object tracking algorithms, such as the Kalman filter and the particle filter, and more recent deep learning-based methods. These deep learning-based methods typically use a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) to segment and track objects. Evaluation of video object segmentation methods is typically done using metrics such as the Intersection over Union (IoU) and the Multiple Object Tracking Accuracy (MOTA). IoU measures the overlap between the predicted object mask and the ground truth mask, while MOTA measures the overall accuracy of the object tracking algorithm. Unsupervised VOS: \u00b6 As the name suggests, aims to segment objects in a video without using any labeled data. This task requires the model to learn the appearance and motion of objects in the video and to separate them from the background. A popular approach to unsupervised VOS is based on optical flow. Optical flow is a technique that estimates the motion of pixels between frames. Optical flow can be used to track the motion of objects in the video and to segment them from the background. An example of such a method is the Focus on Foreground Network (F2Net). F2Net exploits center point information in order to focus on the foreground object. It also establishes a \"Center Prediction Branch\" to estimate the center location of the primary object. Then, the predicted center point is encoded into a gauss map as the spatial guidance prior to enhancing the intra-frame and inter-frame feature matching in our Center Guiding Appearance Diffusion Module, leading the model to focus on the foreground object. After the appearance matching process, F2Net gets three kinds of information flows: inter-frame features, intra-frame features, and original semantic features of the current frame. Instead of concatenating these features, F2Net uses an attention-based Dynamic Information Fusion Module to automatically select the most discriminative features. This allows F2Net to produce better segmentation. Semi-Supervised VOS: \u00b6 Semi-Supervised VOS uses small amounts of labeled data in order to guide the segmentation process, and then uses unsupervised methods to refine the results. In this way, Semi-Supervised VOS can leverage both supervised and unsupervised methods to achieve higher efficiency and accuracy. The key advantage of this method is that it requires much less labeled data than a supervised approach. Additionally, the unsupervised methods used in semi-supervised VOS can help to improve the robustness and generalization of the segmentation results, as they can take into account additional context and information that may not be present in the labeled data. The Sparse Spatiotemporal Transformers (SST) model proposed in 2021 uses semi-supervised learning for the VOS task. SST processes videos in a single pass of an efficient attention-based network. At every layer of this net, each spatiotemporal feature vector simultaneously interacts with all other feature vectors in the video. SST being feedforward also helps it avoid the compounding issue present with recurrent methods. SST addresses computational complexity using sparse attention operator variants, making it possible to apply self-attention to high-resolution videos. Interactive VOS: \u00b6 Interactive VOS is used to track and segment object in real-time. The user\u2019s ability to provide input to the algorithm is what makes this method interactive. This user input can then guide the algorithm in its segmentation and tracking of the object throughout the rest of the video. The main feature of interactive VOS is the ability to improve object segmentation and tracking accuracy and reliability. This technique can also help train more accurate object detection models by providing annotated and labeled data. One of the problems or challenges associated with interactive VOS is choosing the frame through which the user should provide input. This is known as a Markov Decision Process (MDP). Language-guided VOS: \u00b6 Language-guided VOS uses natural language input to guide segmentation and tracking of objects in a video. Similar to interactive VOS, Language-guided VOS relies on user input, but the input is natural language rather than outlines. This is typically done by using a combination of machine learning algorithms, such as Convolutional Neural Networks (CNNs) and Recurrent Neural networks (RNNs), and Natural Language Processing (NLP) techniques to understand the user's input. Natural language input allows for more flexible and intuitive interaction with the algorithm. Instead of defining markers or initial locations, the user can provide a simple verbal description of the object they want tracked. This can be especially useful when there are multiple similar objects or they are difficult to locate. The algorithm first uses NLP techniques to process the user's input and extract relevant information about the object to be segmented and tracked. This information is then used to guide the segmentation and tracking process. One such framework is the Multimodal Tracking Transformer (MTTR) where the objective is to segment text-referred object instances in the frames of a given video. The MTTR model extracts linguistic features from the text query using a standard Transformer-based text encoder and visual features from the video frames using a spatiotemporal encoder. The features are then passed into a multimodal Transformer, which outputs several sequences of object predictions. To determine which predicted sequence best matches the user description, MTTR computes a text-reference score for each sequence for which a temporal segment voting scheme is developed. This allows the model to focus on more relevant parts of the video when making the decision. Video Semantic Segmentation (VSS) Methods and Models: \u00b6 VSS is the task of segmenting and understanding the semantic content of a video. Which means not only segmenting objects but also understanding their meaning and context. For example, a video semantic segmentation model might be able to identify that a person is walking on a sidewalk, a car is driving on the road, and a building is a skyscraper. The goal is to understand the scene and its contents rather than just tracking specific objects. The process of VSS begins with extracting features from the video frames using CNNs. CNNs can learn hierarchical representations of the image data, allowing them to understand the contents of the image at multiple levels of abstraction. CNNs can learn hierarchical representations of the image data, allowing them to understand the contents of the image at multiple levels of abstraction. Those features are then used to classify each pixel in a video. This is typically done using a fully convolutional network (FCN), a type of CNN designed for dense prediction tasks. FCNs can take an input image and produce a dense output, where each pixel in the output corresponds to a class label (\u201cobject\u201d or \u201cbackground,\u201d for example). VSS methods are evaluated using metrics such as the mean Intersection over Union (mIoU) and the Pixel Accuracies (PA). mIoU measures the average overlap between the predicted object mask and the ground truth mask, while PA measures the overall accuracy of the object segmentation algorithm. Instance-Agnostic VSS: \u00b6 Instance-Agnostic VSS is a method to identify and segment objects in a video sequence without considering the individual instances of the objects. This approach is in contrast to instance-aware semantic segmentation, which tracks and segments individual instances of objects within a video, making it less computationally demanding. The Temporally Distributed Network (TDNet) is an example of a video instance segmentation architecture inspired by Group Convolutions, which shows that extracting features with separated filter groups not only allows for model parallelization but also helps learn better representations. Given a deep image segmentation network, TDNet divides the features extracted by the deep model into N (e.g., N=2 or 4) groups and uses N distinct shallow sub-networks to approximate each group of feature channels. By forcing each sub-network to cover a separate feature subspace, a strong feature representation can be produced by reassembling the output of these sub-networks. For balanced and efficient computation over time, the N sub-networks share the same shallow architecture, which is set to be (1/N) of the original deep model\u2019s size to preserve a similar total model capacity. The architecture is coupled with a grouped Knowledge Distillation loss to accelerate the semantic segmentation models for videos. Video Instance Segmentation: \u00b6 Video Instance Segmentation identifies and segments individual instances of objects within a video. This approach is in contrast to the instance-agnostic VSS, which only identifies and segments objects within a video without considering individual instances. Video instance segmentation Transformer (VisTR) is a framework built for instance segmentation that views the instance segmentation task as a parallel sequence decoding/prediction problem. Given a video clip that has multiple image frames as input, the VisTR algorithm outputs the sequence of masks for each instance in the video directly. Given a sequence of video frames, a CNN module extracts features of individual image frames. THe multiple image features are then concatenated into the frame order to form the clip-level feature sequence. Next, the Transformer takes the clip-level feature sequence as input and outputs a sequence of object predictions in order. The sequence of predictions follows the order of input images, and the predictions of each image follow the same instance order. Thus, instance tracking is achieved seamlessly and naturally in the same framework of instance segmentation. Video Panoptic Segmentation: \u00b6 Video panoptic segmentation (VPS) identifies and segments both objects and their parts in a video sequence in a single step. This method combines the strengths of both instance-agnostic VSS and video instance segmentation. One of the biggest advantages of VPS is that it can differentiate between object, object parts, and backgrounds in a video, all of which provides for a more detailed understanding of the scene. It also allows us to distinguish and segment multiple instances of the same object in a video, even when they overlap, which comes at the cost of high computational demand. An example of this is the ViP-DeepLab which performs Depth-aware Video Panoptic Segmentation (DVPS) in order to solve the inverse projection problem (which refers to the ambiguous mapping from the retinal images to the sources of retinal stimulation). It has been found that VPS can be modeled as concatenated image panoptic segmentation. As a result of this, the Panoptic-DeepLab model has been used to perform center regression for two consecutive frames with respect to only the object centers appearing in the first frame. During inference, this offset prediction allows ViP-DeepLab to group all the pixels in the two frames to the same object that appears in the first frame. New instances emerge if they are not grouped with the previously detected instances. Challenges of Video Segmentation: \u00b6 Variability in Video Content and Quality - This can be as simple as variations in lighting, resolution, frame rate, and other factors that can affect the appearance and characteristics of the video. There are some methods in order to fight large variations in object appearance, such as multi-scale features, deep-learning methods, and domain adaption techniques. To comabt variations in lighting or viewpoints, methods such as color histograms and texture features can be used. Lack of Temporal Consistency - Videos are a sequence of frames, and the contents of the frames can change significantly from frame to frame. This makes it difficult to maintain consistency in the segmentation across frames. Methods for dealing with temporal consistency include using recurrent neural networks (RNNs), optical flow, or motion features. Occlusions - Occlusions occur when one object blocks the view of another object, making it difficult or impossible to track. There are various methods for dealing with occlusions, including using multiple cameras or sensors, depth sensors, and object re-detection. Complexity of Visual Scenes - Video segmentation can be challenging due to the complexity of the visual scenes depicted in the video. This can include the presence of multiple objects and events, as well as occlusions, reflections, and other visual distractions that can make it challenging to identify and segment the content of the video. Lack of Training Data - Supervised approaches for video segmentation require the availability of labeled training data, which can be challenging to obtain for many video datasets. This can limit the effectiveness and generalizability of these approaches. Computational Complexity - Video segmentation can be computationally intensive, especially for large or high-resolution video datasets. This poses challenges in performing real-time or online video segmentation or scaling the segmentation process to extensive video collections. Evaluation and Benchmarking - Evaluating the performance of video segmentation approaches can be difficult due to the lack of standardized benchmarks and evaluation metrics. This can make it challenging to compare and evaluate different approaches or to determine the best approach for a given video dataset.","title":"Prior Research/ Fact Finding"},{"location":"priorresearch/#prior-research-initial-fact-finding","text":"","title":"Prior Research/ Initial Fact-finding"},{"location":"priorresearch/#project-description","text":"","title":"Project Description:"},{"location":"priorresearch/#concepts-overview","text":"","title":"Concepts Overview:"},{"location":"priorresearch/#links","text":"What is Deep Learning What is Video Segmentation","title":"Links:"},{"location":"priorresearch/#deep-learning","text":"Deep learning is a method in artificial intelligence (AI) that teaches computers to process data in a way that is inspired by the human brain. Deep learning models can recognize complex patterns in pictures, text, sounds, and other data to produce accurate insights and predictions. Deep learning methods can also be used to automate tasks that typically require human intelligence, such as describing images or transcribing a sound file into text. Deep learning is used heavily in many different machine learning use cases and concepts like computer vision, speech recognition, natural language processing, and recommendation engines. Computer vision is the computer's ability to extract information and insights from images and videos. Computers can use deep learning techniques to comprehend images in the same way that humans do. Deep learning models can analyze human speech despite varying speech patterns, pitch, tone, language, and accent. Computers use deep learning algorithms to gather insights and meaning from text data and documents. Applications can use deep learning methods to track user activity and develop personalized recommendations. They can analyze the behavior of various users and help them discover new products or services. Deep learning models use neural networks which contain thousands of artificial nodes and neurons in order to process data in a way similar to humans. These networks contains many layers that process data from the input layer and release the processed data to the output layer. The input layer of a artificial neural network (ANN) has several nodes that input data into the network. The data is then passed into the hidden layer which processes and passes the data to layers further in the neural network. These hidden layers process information at different levels, adapting their behavior as they receive new information. Deep learning networks have hundreds of hidden layers that they can use to analyze a problem from several different angles. The processed data is then passed into the output layer which consists of the nodes that output the data. Deep learning models that output \"yes\" or \"no\" answers have only two nodes in the output layer. On the other hand, those that output a wider range of answers have more nodes. Deep learning is a subset of machine learning. Deep learning algorithms emerged in an attempt to make traditional machine learning techniques more efficient. Traditional machine learning methods utilize what is known as supervised learning in which the humans have to assist in training the machine. Deep learning generally relies on unsupervised learning and finds patterns in the data on its own, therefore making it much more efficient. Deep learning is better than machine learning because of a couple reasons. Deep learning models can comprehend unstructured data and make general observations without manual feature extraction. A deep learning application can analyze large amounts of data more deeply and reveal new insights for which it might not have been trained. In this way deep learning has an advantage for finding hidden relationships and discovering patterns. Deep learning models can learn and improve over time based on user behavior. They do not require large variations of labeled datasets. Once again, this is where unsupervised learning benefits deep learning models. However deep learning models have some drawbacks as well. They work a lot better when they are trained on large, high-quality datasets, and outliers or mistakes in the input dataset can significantly affect the deep learning process. Because of this, deep learning requires lots of data pre-processing and data storage capacity as well. Deep learning algorithms are also compute-intensive and require infrastructure with sufficient compute capacity to properly function. Otherwise, they take a long time to process results.","title":"Deep Learning:"},{"location":"priorresearch/#video-segmentation","text":"Video segmentation is the process by which videos are partitioned into seperate regions by a variety of characteristics. These characteristics include object boundaries, motion, color, texture, or other visual features. The goal of video segmentation is to seperate different objects from the background in a video and to provide a more detailed representation of the content. Video segmentation is a very useful technology especially in the field of computer vision since it allows for the identification and characterization of individual objects and events in the video as well as the organization and classification of video content. Video segmentation divdes the content into individual segments or shots (or frames) which can then be characterized and analyzed based on pre-defined attributes. It can also be performed at various levels of granularity, from a single object to whole backgrounds. Video segmentation uses two broad techniques, Video Object Segmentation (VOS) and Video Semantic Segmentation (VSS). VOS focuses on tracking objects within a video and is used in applications such as surveillance and autonomous vehicles. VSS focuses on understanding the overall scene and its contents and is used in applications such as augmented reality and video summarization.","title":"Video Segmentation:"},{"location":"priorresearch/#video-object-segmentation-vos-methods-and-models","text":"VOS is the task of segmenting and tracking specific objects within a video. This is typically done by object initialization\u2014identifying the object in the first frame of the video\u2014and then tracking its movement throughout the rest of the video. The goal is to segment the object from the background and the follow the changes in its movement throughout the video. There are various methods for object initialization, such as manual annotation (most accurate but most time-consuming), automatic annotation (least accurate but fastest), semi-automatic annotation (balances accuracy and speed). After initialization the object must be tracked throughout the video. Methods for object tracking include traditional object tracking algorithms, such as the Kalman filter and the particle filter, and more recent deep learning-based methods. These deep learning-based methods typically use a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) to segment and track objects. Evaluation of video object segmentation methods is typically done using metrics such as the Intersection over Union (IoU) and the Multiple Object Tracking Accuracy (MOTA). IoU measures the overlap between the predicted object mask and the ground truth mask, while MOTA measures the overall accuracy of the object tracking algorithm.","title":"Video Object Segmentation (VOS) Methods and Models:"},{"location":"priorresearch/#unsupervised-vos","text":"As the name suggests, aims to segment objects in a video without using any labeled data. This task requires the model to learn the appearance and motion of objects in the video and to separate them from the background. A popular approach to unsupervised VOS is based on optical flow. Optical flow is a technique that estimates the motion of pixels between frames. Optical flow can be used to track the motion of objects in the video and to segment them from the background. An example of such a method is the Focus on Foreground Network (F2Net). F2Net exploits center point information in order to focus on the foreground object. It also establishes a \"Center Prediction Branch\" to estimate the center location of the primary object. Then, the predicted center point is encoded into a gauss map as the spatial guidance prior to enhancing the intra-frame and inter-frame feature matching in our Center Guiding Appearance Diffusion Module, leading the model to focus on the foreground object. After the appearance matching process, F2Net gets three kinds of information flows: inter-frame features, intra-frame features, and original semantic features of the current frame. Instead of concatenating these features, F2Net uses an attention-based Dynamic Information Fusion Module to automatically select the most discriminative features. This allows F2Net to produce better segmentation.","title":"Unsupervised VOS:"},{"location":"priorresearch/#semi-supervised-vos","text":"Semi-Supervised VOS uses small amounts of labeled data in order to guide the segmentation process, and then uses unsupervised methods to refine the results. In this way, Semi-Supervised VOS can leverage both supervised and unsupervised methods to achieve higher efficiency and accuracy. The key advantage of this method is that it requires much less labeled data than a supervised approach. Additionally, the unsupervised methods used in semi-supervised VOS can help to improve the robustness and generalization of the segmentation results, as they can take into account additional context and information that may not be present in the labeled data. The Sparse Spatiotemporal Transformers (SST) model proposed in 2021 uses semi-supervised learning for the VOS task. SST processes videos in a single pass of an efficient attention-based network. At every layer of this net, each spatiotemporal feature vector simultaneously interacts with all other feature vectors in the video. SST being feedforward also helps it avoid the compounding issue present with recurrent methods. SST addresses computational complexity using sparse attention operator variants, making it possible to apply self-attention to high-resolution videos.","title":"Semi-Supervised VOS:"},{"location":"priorresearch/#interactive-vos","text":"Interactive VOS is used to track and segment object in real-time. The user\u2019s ability to provide input to the algorithm is what makes this method interactive. This user input can then guide the algorithm in its segmentation and tracking of the object throughout the rest of the video. The main feature of interactive VOS is the ability to improve object segmentation and tracking accuracy and reliability. This technique can also help train more accurate object detection models by providing annotated and labeled data. One of the problems or challenges associated with interactive VOS is choosing the frame through which the user should provide input. This is known as a Markov Decision Process (MDP).","title":"Interactive VOS:"},{"location":"priorresearch/#language-guided-vos","text":"Language-guided VOS uses natural language input to guide segmentation and tracking of objects in a video. Similar to interactive VOS, Language-guided VOS relies on user input, but the input is natural language rather than outlines. This is typically done by using a combination of machine learning algorithms, such as Convolutional Neural Networks (CNNs) and Recurrent Neural networks (RNNs), and Natural Language Processing (NLP) techniques to understand the user's input. Natural language input allows for more flexible and intuitive interaction with the algorithm. Instead of defining markers or initial locations, the user can provide a simple verbal description of the object they want tracked. This can be especially useful when there are multiple similar objects or they are difficult to locate. The algorithm first uses NLP techniques to process the user's input and extract relevant information about the object to be segmented and tracked. This information is then used to guide the segmentation and tracking process. One such framework is the Multimodal Tracking Transformer (MTTR) where the objective is to segment text-referred object instances in the frames of a given video. The MTTR model extracts linguistic features from the text query using a standard Transformer-based text encoder and visual features from the video frames using a spatiotemporal encoder. The features are then passed into a multimodal Transformer, which outputs several sequences of object predictions. To determine which predicted sequence best matches the user description, MTTR computes a text-reference score for each sequence for which a temporal segment voting scheme is developed. This allows the model to focus on more relevant parts of the video when making the decision.","title":"Language-guided VOS:"},{"location":"priorresearch/#video-semantic-segmentation-vss-methods-and-models","text":"VSS is the task of segmenting and understanding the semantic content of a video. Which means not only segmenting objects but also understanding their meaning and context. For example, a video semantic segmentation model might be able to identify that a person is walking on a sidewalk, a car is driving on the road, and a building is a skyscraper. The goal is to understand the scene and its contents rather than just tracking specific objects. The process of VSS begins with extracting features from the video frames using CNNs. CNNs can learn hierarchical representations of the image data, allowing them to understand the contents of the image at multiple levels of abstraction. CNNs can learn hierarchical representations of the image data, allowing them to understand the contents of the image at multiple levels of abstraction. Those features are then used to classify each pixel in a video. This is typically done using a fully convolutional network (FCN), a type of CNN designed for dense prediction tasks. FCNs can take an input image and produce a dense output, where each pixel in the output corresponds to a class label (\u201cobject\u201d or \u201cbackground,\u201d for example). VSS methods are evaluated using metrics such as the mean Intersection over Union (mIoU) and the Pixel Accuracies (PA). mIoU measures the average overlap between the predicted object mask and the ground truth mask, while PA measures the overall accuracy of the object segmentation algorithm.","title":"Video Semantic Segmentation (VSS) Methods and Models:"},{"location":"priorresearch/#instance-agnostic-vss","text":"Instance-Agnostic VSS is a method to identify and segment objects in a video sequence without considering the individual instances of the objects. This approach is in contrast to instance-aware semantic segmentation, which tracks and segments individual instances of objects within a video, making it less computationally demanding. The Temporally Distributed Network (TDNet) is an example of a video instance segmentation architecture inspired by Group Convolutions, which shows that extracting features with separated filter groups not only allows for model parallelization but also helps learn better representations. Given a deep image segmentation network, TDNet divides the features extracted by the deep model into N (e.g., N=2 or 4) groups and uses N distinct shallow sub-networks to approximate each group of feature channels. By forcing each sub-network to cover a separate feature subspace, a strong feature representation can be produced by reassembling the output of these sub-networks. For balanced and efficient computation over time, the N sub-networks share the same shallow architecture, which is set to be (1/N) of the original deep model\u2019s size to preserve a similar total model capacity. The architecture is coupled with a grouped Knowledge Distillation loss to accelerate the semantic segmentation models for videos.","title":"Instance-Agnostic VSS:"},{"location":"priorresearch/#video-instance-segmentation","text":"Video Instance Segmentation identifies and segments individual instances of objects within a video. This approach is in contrast to the instance-agnostic VSS, which only identifies and segments objects within a video without considering individual instances. Video instance segmentation Transformer (VisTR) is a framework built for instance segmentation that views the instance segmentation task as a parallel sequence decoding/prediction problem. Given a video clip that has multiple image frames as input, the VisTR algorithm outputs the sequence of masks for each instance in the video directly. Given a sequence of video frames, a CNN module extracts features of individual image frames. THe multiple image features are then concatenated into the frame order to form the clip-level feature sequence. Next, the Transformer takes the clip-level feature sequence as input and outputs a sequence of object predictions in order. The sequence of predictions follows the order of input images, and the predictions of each image follow the same instance order. Thus, instance tracking is achieved seamlessly and naturally in the same framework of instance segmentation.","title":"Video Instance Segmentation:"},{"location":"priorresearch/#video-panoptic-segmentation","text":"Video panoptic segmentation (VPS) identifies and segments both objects and their parts in a video sequence in a single step. This method combines the strengths of both instance-agnostic VSS and video instance segmentation. One of the biggest advantages of VPS is that it can differentiate between object, object parts, and backgrounds in a video, all of which provides for a more detailed understanding of the scene. It also allows us to distinguish and segment multiple instances of the same object in a video, even when they overlap, which comes at the cost of high computational demand. An example of this is the ViP-DeepLab which performs Depth-aware Video Panoptic Segmentation (DVPS) in order to solve the inverse projection problem (which refers to the ambiguous mapping from the retinal images to the sources of retinal stimulation). It has been found that VPS can be modeled as concatenated image panoptic segmentation. As a result of this, the Panoptic-DeepLab model has been used to perform center regression for two consecutive frames with respect to only the object centers appearing in the first frame. During inference, this offset prediction allows ViP-DeepLab to group all the pixels in the two frames to the same object that appears in the first frame. New instances emerge if they are not grouped with the previously detected instances.","title":"Video Panoptic Segmentation:"},{"location":"priorresearch/#challenges-of-video-segmentation","text":"Variability in Video Content and Quality - This can be as simple as variations in lighting, resolution, frame rate, and other factors that can affect the appearance and characteristics of the video. There are some methods in order to fight large variations in object appearance, such as multi-scale features, deep-learning methods, and domain adaption techniques. To comabt variations in lighting or viewpoints, methods such as color histograms and texture features can be used. Lack of Temporal Consistency - Videos are a sequence of frames, and the contents of the frames can change significantly from frame to frame. This makes it difficult to maintain consistency in the segmentation across frames. Methods for dealing with temporal consistency include using recurrent neural networks (RNNs), optical flow, or motion features. Occlusions - Occlusions occur when one object blocks the view of another object, making it difficult or impossible to track. There are various methods for dealing with occlusions, including using multiple cameras or sensors, depth sensors, and object re-detection. Complexity of Visual Scenes - Video segmentation can be challenging due to the complexity of the visual scenes depicted in the video. This can include the presence of multiple objects and events, as well as occlusions, reflections, and other visual distractions that can make it challenging to identify and segment the content of the video. Lack of Training Data - Supervised approaches for video segmentation require the availability of labeled training data, which can be challenging to obtain for many video datasets. This can limit the effectiveness and generalizability of these approaches. Computational Complexity - Video segmentation can be computationally intensive, especially for large or high-resolution video datasets. This poses challenges in performing real-time or online video segmentation or scaling the segmentation process to extensive video collections. Evaluation and Benchmarking - Evaluating the performance of video segmentation approaches can be difficult due to the lack of standardized benchmarks and evaluation metrics. This can make it challenging to compare and evaluate different approaches or to determine the best approach for a given video dataset.","title":"Challenges of Video Segmentation:"},{"location":"references/","text":"References \u00b6 \u201cAccelerated, Containerized Application Development.\u201d Docker, 5 July 2023, www.docker.com/ . Cheng, Ho Kei, and Alexander G. Schwing. \u201cXMEM: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model.\u201d arXiv.Org, 18 July 2022, arxiv.org/abs/2207.07115. \u201cCommanddictionary_table.PDF.\u201d Google Drive, drive.google.com/file/d/1LcvPbvkkYcssL65ZuwfsRVrzk6GqjB5m/view?usp=sharing. Accessed 16 July 2023. \u201cContainer Images: Architecture and Best Practices.\u201d Aqua, 7 Dec. 2022, www.aquasec.com/cloud-native-academy/container-security/container-images/#What-is-Docker-Hub ? Cosi, Michele. \u201cThe Unix Shell, Git and Github: An Introduction\u00b6.\u201d 0. The Shell and Git - CyVerse Foundational Open Science Skills 2023, foss.cyverse.org/00_basics/. Accessed 16 July 2023. \u201cDeeplabcut.\u201d GitHub, github.com/DeepLabCut. Accessed 16 July 2023. \u201cDocker Overview.\u201d Docker Documentation, 13 July 2023, docs.docker.com/get-started/overview/. \u201cGit Cheat Sheet Light (Final) (2).JPG.\u201d Google Drive, drive.google.com/file/d/1K3F4_GCemJsxVjGLadyMtOtTTt3jpuVG/view. Accessed 16 July 2023. Grand, Rachel. \u201cWhat Are Containers and How Do They Work?\u201d Ridge Cloud, 15 May 2023, www.ridge.co/blog/what-are-containers/#what-exactly-is-a-container . Hkchengrex. \u201cHKCHENGREX/XMEM: [ECCV 2022] XMEM: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model.\u201d GitHub, github.com/hkchengrex/XMem. Accessed 16 July 2023. Ke, Lei, et al. \u201cMask-Free Video Instance Segmentation.\u201d arXiv.Org, 28 Mar. 2023, arxiv.org/abs/2303.15904. Kundu, Rohit. \u201cVideo Segmentation: Intro, Methods, Tutorial.\u201d V7, 12 Mar. 2023, www.v7labs.com/blog/video-segmentation-guide . Mathis, Alexander, et al. \u201cDeeplabcut: Markerless Pose Estimation of User-Defined Body Parts with Deep Learning.\u201d Nature News, 20 Aug. 2018, www.nature.com/articles/s41593-018-0209-y/ . Mathis. \u201cDeeplabcut.\u201d The Mathis Lab of Adaptive Motor Control, www.mackenziemathislab.org/deeplabcut-home . Accessed 16 July 2023. Nath, Tanmay, et al. \u201cUsing DeepLabCut for 3D Markerless Pose Estimation across Species and Behaviors.\u201d bioRxiv, 1 Jan. 2018, doi.org/10.1101/476531. \u201cThe Open Science Workspace for Collaborative Data-Driven Discovery.\u201d CyVerse News, www.cyverse.org/ . Accessed 16 July 2023. \u201cOverview.\u201d Docker Documentation, 13 July 2023, docs.docker.com/get-started/. Snakemake, snakemake.readthedocs.io/en/stable/. Accessed 16 July 2023. \u201cSummary and Setup.\u201d The Unix Shell: Summary and Setup, 11 July 2023, swcarpentry.github.io/shell-novice/index.html. \u201cSummary and Setup.\u201d Version Control with Git: Summary and Setup, 11 July 2023, swcarpentry.github.io/git-novice/. \u201cSYSCV/Maskfreevis: Mask-Free Video Instance Segmentation [CVPR 2023].\u201d GitHub, github.com/SysCV/MaskFreeVIS. Accessed 16 July 2023. \u201cTrack-Anything.\u201d GitHub, github.com/gaomingqi/Track-Anything/blob/master/README.md. Accessed 16 July 2023. \u201cUltralytics.\u201d Ultralytics YOLOv8 Docs, docs.ultralytics.com/. Accessed 16 July 2023. \u201cWhat Are Containers? | Google Cloud.\u201d Google, cloud.google.com/learn/what-are-containers. Accessed 16 July 2023. \u201cWhat Is Deep Learning?\u201d Amazon, aws.amazon.com/what-is/deep-learning/#:~:text=Deep%20learning%20is%20a%20method,produce%20accurate%20insights%20and%20predictions. Accessed 16 July 2023. Yang, Jinyu, et al. \u201cTrack Anything: Segment Anything Meets Videos.\u201d arXiv.Org, 28 Apr. 2023, arxiv.org/abs/2304.11968. Yu, Fisher. \u201cMask-Free Video Instance Segmentation.\u201d ETH VIS Group, 17 Apr. 2023, www.vis.xyz/pub/maskfreevis/ .","title":"References"},{"location":"references/#references","text":"\u201cAccelerated, Containerized Application Development.\u201d Docker, 5 July 2023, www.docker.com/ . Cheng, Ho Kei, and Alexander G. Schwing. \u201cXMEM: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model.\u201d arXiv.Org, 18 July 2022, arxiv.org/abs/2207.07115. \u201cCommanddictionary_table.PDF.\u201d Google Drive, drive.google.com/file/d/1LcvPbvkkYcssL65ZuwfsRVrzk6GqjB5m/view?usp=sharing. Accessed 16 July 2023. \u201cContainer Images: Architecture and Best Practices.\u201d Aqua, 7 Dec. 2022, www.aquasec.com/cloud-native-academy/container-security/container-images/#What-is-Docker-Hub ? Cosi, Michele. \u201cThe Unix Shell, Git and Github: An Introduction\u00b6.\u201d 0. The Shell and Git - CyVerse Foundational Open Science Skills 2023, foss.cyverse.org/00_basics/. Accessed 16 July 2023. \u201cDeeplabcut.\u201d GitHub, github.com/DeepLabCut. Accessed 16 July 2023. \u201cDocker Overview.\u201d Docker Documentation, 13 July 2023, docs.docker.com/get-started/overview/. \u201cGit Cheat Sheet Light (Final) (2).JPG.\u201d Google Drive, drive.google.com/file/d/1K3F4_GCemJsxVjGLadyMtOtTTt3jpuVG/view. Accessed 16 July 2023. Grand, Rachel. \u201cWhat Are Containers and How Do They Work?\u201d Ridge Cloud, 15 May 2023, www.ridge.co/blog/what-are-containers/#what-exactly-is-a-container . Hkchengrex. \u201cHKCHENGREX/XMEM: [ECCV 2022] XMEM: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model.\u201d GitHub, github.com/hkchengrex/XMem. Accessed 16 July 2023. Ke, Lei, et al. \u201cMask-Free Video Instance Segmentation.\u201d arXiv.Org, 28 Mar. 2023, arxiv.org/abs/2303.15904. Kundu, Rohit. \u201cVideo Segmentation: Intro, Methods, Tutorial.\u201d V7, 12 Mar. 2023, www.v7labs.com/blog/video-segmentation-guide . Mathis, Alexander, et al. \u201cDeeplabcut: Markerless Pose Estimation of User-Defined Body Parts with Deep Learning.\u201d Nature News, 20 Aug. 2018, www.nature.com/articles/s41593-018-0209-y/ . Mathis. \u201cDeeplabcut.\u201d The Mathis Lab of Adaptive Motor Control, www.mackenziemathislab.org/deeplabcut-home . Accessed 16 July 2023. Nath, Tanmay, et al. \u201cUsing DeepLabCut for 3D Markerless Pose Estimation across Species and Behaviors.\u201d bioRxiv, 1 Jan. 2018, doi.org/10.1101/476531. \u201cThe Open Science Workspace for Collaborative Data-Driven Discovery.\u201d CyVerse News, www.cyverse.org/ . Accessed 16 July 2023. \u201cOverview.\u201d Docker Documentation, 13 July 2023, docs.docker.com/get-started/. Snakemake, snakemake.readthedocs.io/en/stable/. Accessed 16 July 2023. \u201cSummary and Setup.\u201d The Unix Shell: Summary and Setup, 11 July 2023, swcarpentry.github.io/shell-novice/index.html. \u201cSummary and Setup.\u201d Version Control with Git: Summary and Setup, 11 July 2023, swcarpentry.github.io/git-novice/. \u201cSYSCV/Maskfreevis: Mask-Free Video Instance Segmentation [CVPR 2023].\u201d GitHub, github.com/SysCV/MaskFreeVIS. Accessed 16 July 2023. \u201cTrack-Anything.\u201d GitHub, github.com/gaomingqi/Track-Anything/blob/master/README.md. Accessed 16 July 2023. \u201cUltralytics.\u201d Ultralytics YOLOv8 Docs, docs.ultralytics.com/. Accessed 16 July 2023. \u201cWhat Are Containers? | Google Cloud.\u201d Google, cloud.google.com/learn/what-are-containers. Accessed 16 July 2023. \u201cWhat Is Deep Learning?\u201d Amazon, aws.amazon.com/what-is/deep-learning/#:~:text=Deep%20learning%20is%20a%20method,produce%20accurate%20insights%20and%20predictions. Accessed 16 July 2023. Yang, Jinyu, et al. \u201cTrack Anything: Segment Anything Meets Videos.\u201d arXiv.Org, 28 Apr. 2023, arxiv.org/abs/2304.11968. Yu, Fisher. \u201cMask-Free Video Instance Segmentation.\u201d ETH VIS Group, 17 Apr. 2023, www.vis.xyz/pub/maskfreevis/ .","title":"References"},{"location":"unixshell/","text":"UNIX Shell \u00b6 Links \u00b6 Software Carpentry UNIX Shell Tutorial Shell & Git Basics CyVerse UNIX Shell Commands Notes \u00b6 The Unix shell is both a command-line interface (CLI) and a scripting language, allowing such repetitive tasks to be done automatically and fast. The shell is a program where users can type commands. With the shell, it\u2019s possible to invoke complicated programs like climate modeling software or simple commands that create an empty directory with only one line of code. The most popular Unix shell is Bash. General Syntax of a Shell Command Shell commands are used to navigate, visualize, modify (files/folders) and automate (processes), and can only be executed through the shell's terminal window. Once something is deleted from the Shell, it is gone forever. There is not recycle bin. The most effective way to use the Shell is to combine commands such as capturing output, filtering output, and passing output in what are known as pipes. cut -d, -f 2 animals.csv | sort | uniq -c | wc -l We can use for loops in order to repeat tasks over many files or instances. Loops can also be nested and can be written in one line or multiple. This is the basic syntax of a for loop. # The word \"for\" indicated the start of a \"For-loop\" command for thing in list_of_things #The word \"do\" indicates the start of job execution list do # Indentation within the loop is not required, but aids legibility operation_using/command $thing # The word \"done\" indicates the end of a loop done You can also use the echo command to see what your Shell command would do without actually running it. You can save numerours commands in a file, called a Shell Script, and re-run all those operations by typing a single command. This helps with time management, error prevention, and reproducibility. By adding commands to a file, and then calling that file using the bash command, you can execute the script. You can add as many commands as you want in the file. You can also use the $ command followed by a number to add arguments to the command when calling the script. For example, this script: head -n 15 \"$1\" | tail -n 5 Can be called with this command (note the parameter for the $1 ): $ bash middle.sh octane.pdb A very useful command for filtering and finding is the grep command. It can be used to find all items in a file that include the key word (passed as a parameter). Things like word boundaries and case can also be limited for different results.","title":"UNIX Shell"},{"location":"unixshell/#unix-shell","text":"","title":"UNIX Shell"},{"location":"unixshell/#links","text":"Software Carpentry UNIX Shell Tutorial Shell & Git Basics CyVerse UNIX Shell Commands","title":"Links"},{"location":"unixshell/#notes","text":"The Unix shell is both a command-line interface (CLI) and a scripting language, allowing such repetitive tasks to be done automatically and fast. The shell is a program where users can type commands. With the shell, it\u2019s possible to invoke complicated programs like climate modeling software or simple commands that create an empty directory with only one line of code. The most popular Unix shell is Bash. General Syntax of a Shell Command Shell commands are used to navigate, visualize, modify (files/folders) and automate (processes), and can only be executed through the shell's terminal window. Once something is deleted from the Shell, it is gone forever. There is not recycle bin. The most effective way to use the Shell is to combine commands such as capturing output, filtering output, and passing output in what are known as pipes. cut -d, -f 2 animals.csv | sort | uniq -c | wc -l We can use for loops in order to repeat tasks over many files or instances. Loops can also be nested and can be written in one line or multiple. This is the basic syntax of a for loop. # The word \"for\" indicated the start of a \"For-loop\" command for thing in list_of_things #The word \"do\" indicates the start of job execution list do # Indentation within the loop is not required, but aids legibility operation_using/command $thing # The word \"done\" indicates the end of a loop done You can also use the echo command to see what your Shell command would do without actually running it. You can save numerours commands in a file, called a Shell Script, and re-run all those operations by typing a single command. This helps with time management, error prevention, and reproducibility. By adding commands to a file, and then calling that file using the bash command, you can execute the script. You can add as many commands as you want in the file. You can also use the $ command followed by a number to add arguments to the command when calling the script. For example, this script: head -n 15 \"$1\" | tail -n 5 Can be called with this command (note the parameter for the $1 ): $ bash middle.sh octane.pdb A very useful command for filtering and finding is the grep command. It can be used to find all items in a file that include the key word (passed as a parameter). Things like word boundaries and case can also be limited for different results.","title":"Notes"},{"location":"vcwgit/","text":"Version Control with Git \u00b6 Links \u00b6 Software Carpentry Version Control with Git Tutorial Shell & Git Basics CyVerse Git Commands Sheet Notes \u00b6 Nothing that is committed to version control is ever lost. All old versions of files are saved, so it\u2019s always possible to go back in time to see exactly who wrote what on a particular day, or what version of a program was used to generate a particular set of results. Because of that we know who to ask if we have questions later on, and, if needed, revert to a previous version, much like the \u201cundo\u201d feature in an editor. The version control system automatically notifies users whenever there\u2019s a conflict between one person\u2019s work and another\u2019s (editing and overwriting). Version control is the lab notebook of the digital world: it\u2019s what professionals use to keep track of what they\u2019ve done and to collaborate with other people. Every large software development project relies on it, and most programmers use it for their small jobs as well. Version Control with Git is used in the UNIX Shell and run in the same way, just with different commands. Version control systems start with a base version of the document and then record changes you make each step of the way. A version control system is a tool that keeps track of changes for us, effectively creating different versions of our files. It allows us to decide which changes will be made to the next version (each record of these changes is called a commit), and keeps useful metadata about them. The complete history of commits for a particular project and their metadata make up a repository. Repositories can be kept in sync across different computers, facilitating collaboration among different people. On a command line, Git commands are written as git verb options , where verb is what we actually want to do and options is additional optional information which may be needed for the verb . Here is an example of how to first configure Git. $ git config --global user.name \"Tony Stark\" $ git config --global user.email \"iam@ironman.snap\" This is the same username and email as your GitHub account and will be used to publish your changes directly to Github. You must also set the default text editor you want to use. Here is an example of that command for the Nano editor. $ git config --global core.editor \"nano -w\" The git init command will make the directory into a repository that can include subdirectories and their files. Creating a directory and initializing it with Git are two seperate things. We can use the git status command at any time to see the status of our repository. Git doesn't automatically keep track of files. Use the git add command, followed by the file name, in order to have Git keep track of a file and save changes. You can then use the git commit command in order to save everything you have saved through git add permanently. A commit message should also be written in order to make describe changes made to the file(s). When updating a file, you can use git diff to see the changes made between the last version and the current version. In order to see differences between older commits, simply type $ git diff HEAD~1 mars.txt , the number after the tilde ( ~ ) represents how many commits back you are seeing. Then use git add to save the file, and git commit to save the version. You can also use git log to see a history of changes. Files can be stored in a project\u2019s working directory (which users see), the staging area (where the next commit is being built up) and the local repository (where commits are permanently recorded). git checkout can restore old versions of the file. By typing $ git checkout HEAD mars.txt you can restore the last commit, to go back further you wwould need the commit identifier in place of HEAD . Remember that we must use the commit number that identifies the state of the repository before the change we\u2019re trying to undo. A common mistake is to use the number of the commit in which we made the change we\u2019re trying to discard. In the example below, we want to retrieve the state from before the most recent commit (HEAD~1), which is commit f22b25e. Here is an overall working of Git and Version Control. You can also have Git ignore files to save disk space and memory and keep your repository clean. We do this by creating a file in the root directory of our project called .gitignore . Then you can add the names of the files, or types of files, as well as the directory they are in, that you want Git to ignore. You must still add and commit the .gitignore file however. In order to connect our local files to the cloud, we need a Github repository. The repository should be empty with a README and license. We can connect our local and GitHub repositories by making the GitHub repository a remote for the local repository. Make the GitHub repository SSH, copy its URL, and run this command $ git remote add origin git@github.com:vlad/planets.git to connect them. In order to authenticate with GitHub, we can use an SSH. SSH uses a key pair, a public key and a private key that both must be used. Check what SSH pairs already exist on your machine by using this command ls -al ~/.ssh . To create a key pair, use this command, $ ssh-keygen -t ed25519 -C \"iam@ironman.snap\" . Use this command, cat ~/.ssh/id_ed25519.pub to copy your public key, and then go to GitHub.com, click on your profile icon in the top right corner to get the drop-down menu. Click \u201cSettings,\u201d then on the settings page, click \u201cSSH and GPG keys,\u201d on the left side \u201cAccount settings\u201d menu. Click the \u201cNew SSH key\u201d button on the right side. Now, you can add the title, paste your SSH key into the field, and click the \u201cAdd SSH key\u201d to complete the setup. Then run the command $ ssh -T git@github.com to check the SSH authentication. Once a connection is established, you can use the command $ git push origin main to push all your changes to the GitHub repository. You can also pull changes from the GitHub repository by using the command $ git pull origin main . You can also add collaboraters through GitHub and clone repositories that are already on GitHub by using the command $ git clone git@github.com:stark/suits.git ~/Desktop/stark-suits . Git will reject changes made to the repository if you do not have the most updated version of the repository. To avoid this you must pull changed from GitHub. Be careful however, after pulling both changes will be simultaneously applied and cause a conflict in the file. The file must then be edited to resolve the conflict.","title":"Version Control with Git"},{"location":"vcwgit/#version-control-with-git","text":"","title":"Version Control with Git"},{"location":"vcwgit/#links","text":"Software Carpentry Version Control with Git Tutorial Shell & Git Basics CyVerse Git Commands Sheet","title":"Links"},{"location":"vcwgit/#notes","text":"Nothing that is committed to version control is ever lost. All old versions of files are saved, so it\u2019s always possible to go back in time to see exactly who wrote what on a particular day, or what version of a program was used to generate a particular set of results. Because of that we know who to ask if we have questions later on, and, if needed, revert to a previous version, much like the \u201cundo\u201d feature in an editor. The version control system automatically notifies users whenever there\u2019s a conflict between one person\u2019s work and another\u2019s (editing and overwriting). Version control is the lab notebook of the digital world: it\u2019s what professionals use to keep track of what they\u2019ve done and to collaborate with other people. Every large software development project relies on it, and most programmers use it for their small jobs as well. Version Control with Git is used in the UNIX Shell and run in the same way, just with different commands. Version control systems start with a base version of the document and then record changes you make each step of the way. A version control system is a tool that keeps track of changes for us, effectively creating different versions of our files. It allows us to decide which changes will be made to the next version (each record of these changes is called a commit), and keeps useful metadata about them. The complete history of commits for a particular project and their metadata make up a repository. Repositories can be kept in sync across different computers, facilitating collaboration among different people. On a command line, Git commands are written as git verb options , where verb is what we actually want to do and options is additional optional information which may be needed for the verb . Here is an example of how to first configure Git. $ git config --global user.name \"Tony Stark\" $ git config --global user.email \"iam@ironman.snap\" This is the same username and email as your GitHub account and will be used to publish your changes directly to Github. You must also set the default text editor you want to use. Here is an example of that command for the Nano editor. $ git config --global core.editor \"nano -w\" The git init command will make the directory into a repository that can include subdirectories and their files. Creating a directory and initializing it with Git are two seperate things. We can use the git status command at any time to see the status of our repository. Git doesn't automatically keep track of files. Use the git add command, followed by the file name, in order to have Git keep track of a file and save changes. You can then use the git commit command in order to save everything you have saved through git add permanently. A commit message should also be written in order to make describe changes made to the file(s). When updating a file, you can use git diff to see the changes made between the last version and the current version. In order to see differences between older commits, simply type $ git diff HEAD~1 mars.txt , the number after the tilde ( ~ ) represents how many commits back you are seeing. Then use git add to save the file, and git commit to save the version. You can also use git log to see a history of changes. Files can be stored in a project\u2019s working directory (which users see), the staging area (where the next commit is being built up) and the local repository (where commits are permanently recorded). git checkout can restore old versions of the file. By typing $ git checkout HEAD mars.txt you can restore the last commit, to go back further you wwould need the commit identifier in place of HEAD . Remember that we must use the commit number that identifies the state of the repository before the change we\u2019re trying to undo. A common mistake is to use the number of the commit in which we made the change we\u2019re trying to discard. In the example below, we want to retrieve the state from before the most recent commit (HEAD~1), which is commit f22b25e. Here is an overall working of Git and Version Control. You can also have Git ignore files to save disk space and memory and keep your repository clean. We do this by creating a file in the root directory of our project called .gitignore . Then you can add the names of the files, or types of files, as well as the directory they are in, that you want Git to ignore. You must still add and commit the .gitignore file however. In order to connect our local files to the cloud, we need a Github repository. The repository should be empty with a README and license. We can connect our local and GitHub repositories by making the GitHub repository a remote for the local repository. Make the GitHub repository SSH, copy its URL, and run this command $ git remote add origin git@github.com:vlad/planets.git to connect them. In order to authenticate with GitHub, we can use an SSH. SSH uses a key pair, a public key and a private key that both must be used. Check what SSH pairs already exist on your machine by using this command ls -al ~/.ssh . To create a key pair, use this command, $ ssh-keygen -t ed25519 -C \"iam@ironman.snap\" . Use this command, cat ~/.ssh/id_ed25519.pub to copy your public key, and then go to GitHub.com, click on your profile icon in the top right corner to get the drop-down menu. Click \u201cSettings,\u201d then on the settings page, click \u201cSSH and GPG keys,\u201d on the left side \u201cAccount settings\u201d menu. Click the \u201cNew SSH key\u201d button on the right side. Now, you can add the title, paste your SSH key into the field, and click the \u201cAdd SSH key\u201d to complete the setup. Then run the command $ ssh -T git@github.com to check the SSH authentication. Once a connection is established, you can use the command $ git push origin main to push all your changes to the GitHub repository. You can also pull changes from the GitHub repository by using the command $ git pull origin main . You can also add collaboraters through GitHub and clone repositories that are already on GitHub by using the command $ git clone git@github.com:stark/suits.git ~/Desktop/stark-suits . Git will reject changes made to the repository if you do not have the most updated version of the repository. To avoid this you must pull changed from GitHub. Be careful however, after pulling both changes will be simultaneously applied and cause a conflict in the file. The file must then be edited to resolve the conflict.","title":"Notes"}]}