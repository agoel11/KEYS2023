
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="KEYS internship 2023">
      
      
        <link rel="canonical" href="https://agoel11.github.io/KEYS2023/priorresearch/">
      
      
        <meta name="author" content="Atharva Goel">
      
      <link rel="shortcut icon" href="../assets/logo.jpg">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-6.1.0">
    
    
      
        <title>Prior Research/ Fact Finding - KEYS Internship 2023</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.bc7e593a.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.ab28b872.min.css">
        
          
          
          <meta name="theme-color" content="#000000">
        
      
    
    
    
      
        
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans:300,400,400i,700%7CRegular&display=fallback">
        <style>body,input{font-family:"Google Sans",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Regular",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="red">
      
  
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#prior-research-initial-fact-finding" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href="https://agoel11.github.io/KEYS2023/" title="KEYS Internship 2023" class="md-header-nav__button md-logo" aria-label="KEYS Internship 2023">
      
  <img src="../assets/logo.jpg" alt="logo">

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            KEYS Internship 2023
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              Prior Research/ Fact Finding
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/agoel11/KEYS2023/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    KEYS2023
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="https://agoel11.github.io/KEYS2023/" title="KEYS Internship 2023" class="md-nav__button md-logo" aria-label="KEYS Internship 2023">
      
  <img src="../assets/logo.jpg" alt="logo">

    </a>
    KEYS Internship 2023
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/agoel11/KEYS2023/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    KEYS2023
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." class="md-nav__link">
      Introduction
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../logbook/" class="md-nav__link">
      Logbook
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../keysassignments/" class="md-nav__link">
      Assignments
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../jupyter/" class="md-nav__link">
      Jupyter Notebooks
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      GitHub Actions
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="GitHub Actions" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        <span class="md-nav__icon md-icon"></span>
        GitHub Actions
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../githubed/" class="md-nav__link">
      GitHub Education Access
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-6" type="checkbox" id="nav-6">
    
    <label class="md-nav__link" for="nav-6">
      Project Notes
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Project Notes" data-md-level="1">
      <label class="md-nav__title" for="nav-6">
        <span class="md-nav__icon md-icon"></span>
        Project Notes
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../unixshell/" class="md-nav__link">
      UNIX Shell
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../vcwgit/" class="md-nav__link">
      Version Control with Git
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../containers/" class="md-nav__link">
      Containers
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-7" type="checkbox" id="nav-7" checked>
    
    <label class="md-nav__link" for="nav-7">
      KEYS 2023 Main Project
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="KEYS 2023 Main Project" data-md-level="1">
      <label class="md-nav__title" for="nav-7">
        <span class="md-nav__icon md-icon"></span>
        KEYS 2023 Main Project
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../myproject/" class="md-nav__link">
      My Project
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Prior Research/ Fact Finding
        <span class="md-nav__icon md-icon"></span>
      </label>
    
    <a href="./" class="md-nav__link md-nav__link--active">
      Prior Research/ Fact Finding
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#project-description" class="md-nav__link">
    Project Description:
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#concepts-overview" class="md-nav__link">
    Concepts Overview:
  </a>
  
    <nav class="md-nav" aria-label="Concepts Overview:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#links" class="md-nav__link">
    Links:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deep-learning" class="md-nav__link">
    Deep Learning:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#video-segmentation" class="md-nav__link">
    Video Segmentation:
  </a>
  
    <nav class="md-nav" aria-label="Video Segmentation:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#video-object-segmentation-vos-methods-and-models" class="md-nav__link">
    Video Object Segmentation (VOS) Methods and Models:
  </a>
  
    <nav class="md-nav" aria-label="Video Object Segmentation (VOS) Methods and Models:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#unsupervised-vos" class="md-nav__link">
    Unsupervised VOS:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#semi-supervised-vos" class="md-nav__link">
    Semi-Supervised VOS:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interactive-vos" class="md-nav__link">
    Interactive VOS:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#language-guided-vos" class="md-nav__link">
    Language-guided VOS:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#video-semantic-segmentation-vss-methods-and-models" class="md-nav__link">
    Video Semantic Segmentation (VSS) Methods and Models:
  </a>
  
    <nav class="md-nav" aria-label="Video Semantic Segmentation (VSS) Methods and Models:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#instance-agnostic-vss" class="md-nav__link">
    Instance-Agnostic VSS:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#video-instance-segmentation" class="md-nav__link">
    Video Instance Segmentation:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#video-panoptic-segmentation" class="md-nav__link">
    Video Panoptic Segmentation:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#challenges-of-video-segmentation" class="md-nav__link">
    Challenges of Video Segmentation:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../litsearch/" class="md-nav__link">
      Literature Search
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-8" type="checkbox" id="nav-8">
    
    <label class="md-nav__link" for="nav-8">
      Results
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Results" data-md-level="1">
      <label class="md-nav__title" for="nav-8">
        <span class="md-nav__icon md-icon"></span>
        Results
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../poster/" class="md-nav__link">
      Poster
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../references/" class="md-nav__link">
      References
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#project-description" class="md-nav__link">
    Project Description:
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#concepts-overview" class="md-nav__link">
    Concepts Overview:
  </a>
  
    <nav class="md-nav" aria-label="Concepts Overview:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#links" class="md-nav__link">
    Links:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deep-learning" class="md-nav__link">
    Deep Learning:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#video-segmentation" class="md-nav__link">
    Video Segmentation:
  </a>
  
    <nav class="md-nav" aria-label="Video Segmentation:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#video-object-segmentation-vos-methods-and-models" class="md-nav__link">
    Video Object Segmentation (VOS) Methods and Models:
  </a>
  
    <nav class="md-nav" aria-label="Video Object Segmentation (VOS) Methods and Models:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#unsupervised-vos" class="md-nav__link">
    Unsupervised VOS:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#semi-supervised-vos" class="md-nav__link">
    Semi-Supervised VOS:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interactive-vos" class="md-nav__link">
    Interactive VOS:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#language-guided-vos" class="md-nav__link">
    Language-guided VOS:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#video-semantic-segmentation-vss-methods-and-models" class="md-nav__link">
    Video Semantic Segmentation (VSS) Methods and Models:
  </a>
  
    <nav class="md-nav" aria-label="Video Semantic Segmentation (VSS) Methods and Models:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#instance-agnostic-vss" class="md-nav__link">
    Instance-Agnostic VSS:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#video-instance-segmentation" class="md-nav__link">
    Video Instance Segmentation:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#video-panoptic-segmentation" class="md-nav__link">
    Video Panoptic Segmentation:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#challenges-of-video-segmentation" class="md-nav__link">
    Challenges of Video Segmentation:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/agoel11/KEYS2023/edit/main/docs/priorresearch.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="prior-research-initial-fact-finding">Prior Research/ Initial Fact-finding<a class="headerlink" href="#prior-research-initial-fact-finding" title="Permanent link">&para;</a></h1>
<h2 id="project-description">Project Description:<a class="headerlink" href="#project-description" title="Permanent link">&para;</a></h2>
<p><img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/77e2dfbd-7cde-4ee9-9cde-72bf51fa559a" /></p>
<h2 id="concepts-overview">Concepts Overview:<a class="headerlink" href="#concepts-overview" title="Permanent link">&para;</a></h2>
<h3 id="links">Links:<a class="headerlink" href="#links" title="Permanent link">&para;</a></h3>
<p><a href="https://aws.amazon.com/what-is/deep-learning/#:~:text=Deep%20learning%20is%20a%20method,produce%20accurate%20insights%20and%20predictions.">What is Deep Learning</a>                                           <br />
<a href="https://www.v7labs.com/blog/video-segmentation-guide">What is Video Segmentation</a></p>
<h3 id="deep-learning">Deep Learning:<a class="headerlink" href="#deep-learning" title="Permanent link">&para;</a></h3>
<ol>
<li>Deep learning is a method in artificial intelligence (AI) that teaches computers to process data in a way that is inspired by the human brain. Deep learning models can recognize complex patterns in pictures, text, sounds, and other data to produce accurate insights and predictions. Deep learning methods can also be used to automate tasks that typically require human intelligence, such as describing images or transcribing a sound file into text.</li>
<li>Deep learning is used heavily in many different machine learning use cases and concepts like computer vision, speech recognition, natural language processing, and recommendation engines. Computer vision is the computer's ability to extract information and insights from images and videos. Computers can use deep learning techniques to comprehend images in the same way that humans do. Deep learning models can analyze human speech despite varying speech patterns, pitch, tone, language, and accent. Computers use deep learning algorithms to gather insights and meaning from text data and documents. Applications can use deep learning methods to track user activity and develop personalized recommendations. They can analyze the behavior of various users and help them discover new products or services.</li>
<li>Deep learning models use neural networks which contain thousands of artificial nodes and neurons in order to process data in a way similar to humans. These networks contains many layers that process data from the input layer and release the processed data to the output layer.</li>
<li>The input layer of a artificial neural network (ANN) has several nodes that input data into the network. The data is then passed into the hidden layer which processes and passes the data to layers further in the neural network. These hidden layers process information at different levels, adapting their behavior as they receive new information. Deep learning networks have hundreds of hidden layers that they can use to analyze a problem from several different angles. The processed data is then passed into the output layer which consists of the nodes that output the data. Deep learning models that output "yes" or "no" answers have only two nodes in the output layer. On the other hand, those that output a wider range of answers have more nodes. <img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/0eb15ca0-d355-4476-9a2c-0d0b3f9db981" />  </li>
<li>Deep learning is a subset of machine learning. Deep learning algorithms emerged in an attempt to make traditional machine learning techniques more efficient. Traditional machine learning methods utilize what is known  as supervised learning in which the humans have to assist in training the machine. Deep learning generally relies on unsupervised learning and finds patterns in the data on its own, therefore making it much more          efficient.</li>
<li>Deep learning is better than machine learning because of a couple reasons. Deep learning models can comprehend unstructured data and make general observations without manual feature extraction. A deep learning          application can analyze large amounts of data more deeply and reveal new insights for which it might not have been trained. In this way deep learning has an advantage for finding hidden relationships and discovering      patterns. Deep learning models can learn and improve over time based on user behavior. They do not require large variations of labeled datasets. Once again, this is where unsupervised learning benefits deep learning      models.</li>
<li>However deep learning models have some drawbacks as well. They work a lot better when they are trained on large, high-quality datasets, and outliers or mistakes in the input dataset can significantly affect the deep    learning process. Because of this, deep learning requires lots of data pre-processing and data storage capacity as well. Deep learning algorithms are also compute-intensive and require infrastructure with sufficient      compute capacity to properly function. Otherwise, they take a long time to process results.</li>
</ol>
<h3 id="video-segmentation">Video Segmentation:<a class="headerlink" href="#video-segmentation" title="Permanent link">&para;</a></h3>
<ol>
<li>Video segmentation is the process by which videos are partitioned into seperate regions by a variety of characteristics. These characteristics include object boundaries, motion, color, texture, or other visual features. The goal of video segmentation is to seperate different objects from the background in a video and to provide a more detailed representation of the content.</li>
<li>Video segmentation is a very useful technology especially in the field of computer vision since it allows for the identification and characterization of individual objects and events in the video as well as the organization and classification of video content.</li>
<li>Video segmentation divdes the content into individual segments or shots (or frames) which can then be characterized and analyzed based on pre-defined attributes. It can also be performed at various levels of granularity, from a single object to whole backgrounds. Video segmentation uses two broad techniques, Video Object Segmentation (VOS) and Video Semantic Segmentation (VSS). VOS focuses on tracking objects within a video and is used in applications such as surveillance and autonomous vehicles. VSS focuses on understanding the overall scene and its contents and is used in applications such as augmented reality and video summarization.
<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/56aa86e5-1246-47c6-9f19-80e836763085" /></li>
<li>
<h4 id="video-object-segmentation-vos-methods-and-models">Video Object Segmentation (VOS) Methods and Models:<a class="headerlink" href="#video-object-segmentation-vos-methods-and-models" title="Permanent link">&para;</a></h4>
<ol>
<li>VOS is the task of segmenting and tracking specific objects within a video. This is typically done by object initialization—identifying the object in the first frame of the video—and then tracking its movement           throughout the rest of the video. The goal is to segment the object from the background and the follow the changes in its movement throughout the video. There are various methods for object initialization, such as       manual annotation (most accurate but most time-consuming), automatic annotation (least accurate but fastest), semi-automatic annotation (balances accuracy and speed).</li>
<li>After initialization the object must be tracked throughout the video. Methods for object tracking include traditional object tracking algorithms, such as the Kalman filter and the particle filter, and more recent       deep learning-based methods. These deep learning-based methods typically use a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) to segment and track objects.
<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/6ab7da13-711a-44bd-9205-4af9539c7e22" /></li>
<li>Evaluation of video object segmentation methods is typically done using metrics such as the Intersection over Union (IoU) and the Multiple Object Tracking Accuracy (MOTA). IoU measures the overlap between the           predicted object mask and the ground truth mask, while MOTA measures the overall accuracy of the object tracking algorithm.</li>
<li>
<h5 id="unsupervised-vos">Unsupervised VOS:<a class="headerlink" href="#unsupervised-vos" title="Permanent link">&para;</a></h5>
<ol>
<li>As the name suggests, aims to segment objects in a video without using any labeled data. This task requires the model to learn the appearance and motion of objects in the video and to separate them from the           background. A popular approach to unsupervised VOS is based on optical flow. Optical flow is a technique that estimates the motion of pixels between frames. Optical flow can be used to track the motion of objects         in the video and to segment them from the background.</li>
<li>An example of such a method is the Focus on Foreground Network (F2Net). F2Net exploits center point information in order to focus on the foreground object. It also establishes a "Center Prediction Branch" to           estimate the center location of the primary object. Then, the predicted center point is encoded into a gauss map as the spatial guidance prior to enhancing the intra-frame and inter-frame feature matching in our         Center Guiding Appearance Diffusion Module, leading the model to focus on the foreground object.
<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/8d5c4715-fd77-4e53-9d3c-49fa5edc14ee" /></li>
<li>After the appearance matching process, F2Net gets three kinds of information flows: inter-frame features, intra-frame features, and original semantic features of the current frame. Instead of concatenating             these features, F2Net uses an attention-based Dynamic Information Fusion Module to automatically select the most discriminative features. This allows F2Net to produce better segmentation. 
<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/3186b024-99f1-4a1e-98e7-a55a9c2265be" /></li>
</ol>
</li>
<li>
<h5 id="semi-supervised-vos">Semi-Supervised VOS:<a class="headerlink" href="#semi-supervised-vos" title="Permanent link">&para;</a></h5>
<ol>
<li>Semi-Supervised VOS uses small amounts of labeled data in order to guide the segmentation process, and then uses unsupervised methods to refine the results. In this way, Semi-Supervised VOS can leverage both           supervised and unsupervised methods to achieve higher efficiency and accuracy.</li>
<li>The key advantage of this method is that it requires much less labeled data than a supervised approach. Additionally, the unsupervised methods used in semi-supervised VOS can help to improve the robustness and         generalization of the segmentation results, as they can take into account additional context and information that may not be present in the labeled data.</li>
<li>The Sparse Spatiotemporal Transformers (SST) model proposed in 2021 uses semi-supervised learning for the VOS task. SST processes videos in a single pass of an efficient attention-based network. At every layer         of this net, each spatiotemporal feature vector simultaneously interacts with all other feature vectors in the video.
<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/a4819d60-2d9b-488a-9471-f1392219b03e" /></li>
<li>SST being feedforward also helps it avoid the compounding issue present with recurrent methods. SST addresses computational complexity using sparse attention operator variants, making it possible to apply             self-attention to high-resolution videos.
<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/eb292229-90bc-434d-9c15-6018c5f2f8d3" /></li>
</ol>
</li>
<li>
<h5 id="interactive-vos">Interactive VOS:<a class="headerlink" href="#interactive-vos" title="Permanent link">&para;</a></h5>
<ol>
<li>Interactive VOS is used to track and segment object in real-time. The user’s ability to provide input to the algorithm is what makes this method interactive. This user input can then guide the algorithm in its         segmentation and tracking of the object throughout the rest of the video.</li>
<li>The main feature of interactive VOS is the ability to improve object segmentation and tracking accuracy and reliability. This technique can also help train more accurate object detection models by providing           annotated and labeled data.</li>
<li>One of the problems or challenges associated with interactive VOS is choosing the frame through which the user should provide input. This is known as a Markov Decision Process (MDP).
<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/45973fda-0288-4a42-8019-aced5747f07d" /></li>
</ol>
</li>
<li>
<h5 id="language-guided-vos">Language-guided VOS:<a class="headerlink" href="#language-guided-vos" title="Permanent link">&para;</a></h5>
<ol>
<li>Language-guided VOS uses natural language input to guide segmentation and tracking of objects in a video. Similar to interactive VOS, Language-guided VOS relies on user input, but the input is natural language         rather than outlines. This is typically done by using a combination of machine learning algorithms, such as Convolutional Neural Networks (CNNs) and Recurrent Neural networks (RNNs), and Natural Language                 Processing (NLP) techniques to understand the user's input.</li>
<li>Natural language input allows for more flexible and intuitive interaction with the algorithm. Instead of defining markers or initial locations, the user can provide a simple verbal description of the object           they want tracked. This can be especially useful when there are multiple similar objects or they are difficult to locate.
<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/d8ad0908-4a6c-49c7-ba6d-21653117c4b7" /></li>
<li>The algorithm first uses NLP techniques to process the user's input and extract relevant information about the object to be segmented and tracked. This information is then used to guide the segmentation and           tracking process.</li>
<li>One such framework is the Multimodal Tracking Transformer (MTTR) where the objective is to segment text-referred object instances in the frames of a given video. The MTTR model extracts linguistic features             from the text query using a standard Transformer-based text encoder and visual features from the video frames using a spatiotemporal encoder. The features are then passed into a multimodal Transformer, which             outputs several sequences of object predictions.
<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/d90381ad-857c-4351-8902-c4cb7003a385" /></li>
<li>To determine which predicted sequence best matches the user description, MTTR computes a text-reference score for each sequence for which a temporal segment voting scheme is developed. This allows the model to         focus on more relevant parts of the video when making the decision.
<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/1ac681d6-9e57-42bf-8cb1-766a820e193f" /></li>
</ol>
</li>
</ol>
</li>
<li>
<h4 id="video-semantic-segmentation-vss-methods-and-models">Video Semantic Segmentation (VSS) Methods and Models:<a class="headerlink" href="#video-semantic-segmentation-vss-methods-and-models" title="Permanent link">&para;</a></h4>
<ol>
<li>VSS is the task of segmenting and understanding the semantic content of a video. Which means not only segmenting objects but also understanding their meaning and context. For example, a video semantic segmentation     model might be able to identify that a person is walking on a sidewalk, a car is driving on the road, and a building is a skyscraper. The goal is to understand the scene and its contents rather than just tracking         specific objects.</li>
<li>The process of VSS begins with extracting features from the video frames using CNNs. CNNs can learn hierarchical representations of the image data, allowing them to understand the contents of the image at multiple     levels of abstraction. CNNs can learn hierarchical representations of the image data, allowing them to understand the contents of the image at multiple levels of abstraction. Those features are then used to classify     each pixel in a video. This is typically done using a fully convolutional network (FCN), a type of CNN designed for dense prediction tasks. FCNs can take an input image and produce a dense output, where each pixel in     the output corresponds to a class label (“object” or “background,” for example).</li>
<li>VSS methods are evaluated using metrics such as the mean Intersection over Union (mIoU) and the Pixel Accuracies (PA). mIoU measures the average overlap between the predicted object mask and the ground truth mask,     while PA measures the overall accuracy of the object segmentation algorithm.</li>
<li>
<h5 id="instance-agnostic-vss">Instance-Agnostic VSS:<a class="headerlink" href="#instance-agnostic-vss" title="Permanent link">&para;</a></h5>
<ol>
<li>Instance-Agnostic VSS is a method to identify and segment objects in a video sequence without considering the individual instances of the objects. This approach is in contrast to instance-aware semantic               segmentation, which tracks and segments individual instances of objects within a video, making it less computationally demanding.</li>
<li>The Temporally Distributed Network (TDNet) is an example of a video instance segmentation architecture inspired by Group Convolutions, which shows that extracting features with separated filter groups not only         allows for model parallelization but also helps learn better representations.</li>
<li>Given a deep image segmentation network, TDNet divides the features extracted by the deep model into N (e.g., N=2 or 4) groups and uses N distinct shallow sub-networks to approximate each group of feature             channels. By forcing each sub-network to cover a separate feature subspace, a strong feature representation can be produced by reassembling the output of these sub-networks. For balanced and efficient computation         over time, the N sub-networks share the same shallow architecture, which is set to be (1/N) of the original deep model’s size to preserve a similar total model capacity.
<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/abd102aa-484f-4bf3-88a8-41b9d7841fe7" /></li>
<li>The architecture is coupled with a grouped Knowledge Distillation loss to accelerate the semantic segmentation models for videos.
<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/e94aff12-73cc-4069-801e-83a64c2589c8" /></li>
</ol>
</li>
<li>
<h5 id="video-instance-segmentation">Video Instance Segmentation:<a class="headerlink" href="#video-instance-segmentation" title="Permanent link">&para;</a></h5>
<ol>
<li>Video Instance Segmentation identifies and segments individual instances of objects within a video. This approach is in contrast to the instance-agnostic VSS, which only identifies and segments objects within         a video without considering individual instances.
<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/27729eea-01d2-4f47-a275-e527d727ba62" /></li>
<li>Video instance segmentation Transformer (VisTR) is a framework built for instance segmentation that views the instance segmentation task as a parallel sequence decoding/prediction problem. Given a video clip           that has multiple image frames as input, the VisTR algorithm outputs the sequence of masks for each instance in the video directly.</li>
<li>Given a sequence of video frames, a CNN module extracts features of individual image frames. THe multiple image features are then concatenated into the frame order to form the clip-level feature sequence.             Next, the Transformer takes the clip-level feature sequence as input and outputs a sequence of object predictions in order.
<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/af274a35-2230-408b-ac47-8b1da71ce091" /></li>
<li>The sequence of predictions follows the order of input images, and the predictions of each image follow the same instance order. Thus, instance tracking is achieved seamlessly and naturally in the same                 framework of instance segmentation.
<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/dc835abb-ef3e-4dec-b4b4-1b57bab7a4ff" /></li>
</ol>
</li>
<li>
<h5 id="video-panoptic-segmentation">Video Panoptic Segmentation:<a class="headerlink" href="#video-panoptic-segmentation" title="Permanent link">&para;</a></h5>
<ol>
<li>Video panoptic segmentation (VPS) identifies and segments both objects and their parts in a video sequence in a single step. This method combines the strengths of both instance-agnostic VSS and video instance         segmentation.</li>
<li>One of the biggest advantages of VPS is that it can differentiate between object, object parts, and backgrounds in a video, all of which provides for a more detailed understanding of the scene. It also allows         us to distinguish and segment multiple instances of the same object in a video, even when they overlap, which comes at the cost of high computational demand.</li>
<li>An example of this is the ViP-DeepLab which performs Depth-aware Video Panoptic Segmentation (DVPS) in order to solve the inverse projection problem (which refers to the ambiguous mapping from the                     retinal images to the sources of retinal stimulation).</li>
<li>It has been found that VPS can be modeled as concatenated image panoptic segmentation. As a result of this, the Panoptic-DeepLab model has been used to perform center regression for two consecutive frames with         respect to only the object centers appearing in the first frame. During inference, this offset prediction allows ViP-DeepLab to group all the pixels in the two frames to the same object that appears in the               first frame. New instances emerge if they are not grouped with the previously detected instances.</li>
</ol>
<p><img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/9fa1ce34-66e0-43bc-af76-d1953af3f7c1" /></p>
<p><img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/3d7dd2d0-9124-4909-baa9-d4805f9af1a1" /></p>
<p><img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/7bfc04ce-e9c2-4273-8c9b-272ac653f30e" /></p>
</li>
</ol>
</li>
<li>
<h4 id="challenges-of-video-segmentation">Challenges of Video Segmentation:<a class="headerlink" href="#challenges-of-video-segmentation" title="Permanent link">&para;</a></h4>
<ol>
<li>Variability in Video Content and Quality - This can be as simple as variations in lighting, resolution, frame rate, and other factors that can affect the appearance and characteristics of the video. There are some     methods in order to fight large variations in object appearance, such as multi-scale features, deep-learning methods, and domain adaption techniques. To comabt variations in lighting or viewpoints, methods such as       color histograms and texture features can be used.</li>
<li>Lack of Temporal Consistency - Videos are a sequence of frames, and the contents of the frames can change significantly from frame to frame. This makes it difficult to maintain consistency in the segmentation         across frames. Methods for dealing with temporal consistency include using recurrent neural networks (RNNs), optical flow, or motion features.</li>
<li>Occlusions - Occlusions occur when one object blocks the view of another object, making it difficult or impossible to track. There are various methods for dealing with occlusions, including using multiple cameras     or sensors, depth sensors, and object re-detection.</li>
<li>Complexity of Visual Scenes - Video segmentation can be challenging due to the complexity of the visual scenes depicted in the video. This can include the presence of multiple objects and events, as well as           occlusions, reflections, and other visual distractions that can make it challenging to identify and segment the content of the video.</li>
<li>Lack of Training Data - Supervised approaches for video segmentation require the availability of labeled training data, which can be challenging to obtain for many video datasets. This can limit the effectiveness     and generalizability of these approaches.</li>
<li>Computational Complexity - Video segmentation can be computationally intensive, especially for large or high-resolution video datasets. This poses challenges in performing real-time or online video segmentation or     scaling the segmentation process to extensive video collections.</li>
<li>Evaluation and Benchmarking - Evaluating the performance of video segmentation approaches can be difficult due to the lack of standardized benchmarks and evaluation metrics. This can make it challenging to compare     and evaluate different approaches or to determine the best approach for a given video dataset.</li>
</ol>
</li>
</ol>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="Footer">
        
          <a href="../myproject/" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                My Project
              </div>
            </div>
          </a>
        
        
          <a href="../litsearch/" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Literature Search
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2023 - 2024 Atharva Goel
          </div>
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
  <div class="md-footer-social">
    
      
      
        
        
      
      <a href="https://github.com/agoel11/KEYS2023.git" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/vendor.6a3d08fc.min.js"></script>
      <script src="../assets/javascripts/bundle.71201edf.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}</script>
      
      <script>
        app = initialize({
          base: "..",
          features: [],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.4ac00218.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
    
  </body>
</html>