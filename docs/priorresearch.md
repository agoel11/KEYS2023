#Prior Research/ Initial Fact-finding

##Project Description:                                                                                                                                                              
![image](https://github.com/agoel11/KEYS2023/assets/81878922/77e2dfbd-7cde-4ee9-9cde-72bf51fa559a)

##Concepts Overview

###Links                                                                                                                                                                                                              
[What is Deep Learning](https://aws.amazon.com/what-is/deep-learning/#:~:text=Deep%20learning%20is%20a%20method,produce%20accurate%20insights%20and%20predictions.)                                             
[What is Video Segmentation](https://www.v7labs.com/blog/video-segmentation-guide)

###Deep Learning
1. Deep learning is a method in artificial intelligence (AI) that teaches computers to process data in a way that is inspired by the human brain. Deep learning models can recognize complex patterns in pictures, text, sounds, and other data to produce accurate insights and predictions. Deep learning methods can also be used to automate tasks that typically require human intelligence, such as describing images or transcribing a sound file into text.
2. Deep learning is used heavily in many different machine learning use cases and concepts like computer vision, speech recognition, natural language processing, and recommendation engines. Computer vision is the computer's ability to extract information and insights from images and videos. Computers can use deep learning techniques to comprehend images in the same way that humans do. Deep learning models can analyze human speech despite varying speech patterns, pitch, tone, language, and accent. Computers use deep learning algorithms to gather insights and meaning from text data and documents. Applications can use deep learning methods to track user activity and develop personalized recommendations. They can analyze the behavior of various users and help them discover new products or services.
3. Deep learning models use neural networks which contain thousands of artificial nodes and neurons in order to process data in a way similar to humans. These networks contains many layers that process data from the input layer and release the processed data to the output layer.
4. The input layer of a artificial neural network (ANN) has several nodes that input data into the network. The data is then passed into the hidden layer which processes and passes the data to layers further in the neural network. These hidden layers process information at different levels, adapting their behavior as they receive new information. Deep learning networks have hundreds of hidden layers that they can use to analyze a problem from several different angles. The processed data is then passed into the output layer which consists of the nodes that output the data. Deep learning models that output "yes" or "no" answers have only two nodes in the output layer. On the other hand, those that output a wider range of answers have more nodes.

![image](https://github.com/agoel11/KEYS2023/assets/81878922/0eb15ca0-d355-4476-9a2c-0d0b3f9db981)                                                                                                      
5. Deep learning is a subset of machine learning. Deep learning algorithms emerged in an attempt to make traditional machine learning techniques more efficient. Traditional machine learning methods utilize what is known as supervised learning in which the humans have to assist in training the machine. Deep learning generally relies on unsupervised learning and finds patterns in the data on its own, therefore making it much more efficient.                                                                                                                                                                                                              
6. Deep learning is better than machine learning because of a couple reasons. Deep learning models can comprehend unstructured data and make general observations without manual feature extraction. A deep learning application can analyze large amounts of data more deeply and reveal new insights for which it might not have been trained. In this way deep learning has an advantage for finding hidden relationships and discovering patterns. Deep learning models can learn and improve over time based on user behavior. They do not require large variations of labeled datasets. Once again, this is where unsupervised learning benefits deep learning models.                                                                                                                                                                                                                    
7. However deep learning models have some drawbacks as well. They work a lot better when they are trained on large, high-quality datasets, and outliers or mistakes in the input dataset can significantly affect the deep learning process. Because of this, deep learning requires lots of data pre-processing and data storage capacity as well. Deep learning algorithms are also compute-intensive and require infrastructure with sufficient compute capacity to properly function. Otherwise, they take a long time to process results.

###Video Segmentation
1. Video segmentation is the process by which videos are partitioned into seperate regions by a variety of characteristics. These characteristics include object boundaries, motion, color, texture, or other visual features. The goal of video segmentation is to seperate different objects from the background in a video and to provide a more detailed representation of the content.
2. Video segmentation is a very useful technology especially in the field of computer vision since it allows for the identification and characterization of individual objects and events in the video as well as the organization and classification of video content.
3. Video segmentation divdes the content into individual segments or shots (or frames) which can then be characterized and analyzed based on pre-defined attributes. It can also be performed at various levels of granularity, from a single object to whole backgrounds. Video segmentation uses two broad techniques, Video Object Segmentation (VOS) and Video Semantic Segmentation (VSS). VOS focuses on tracking objects within a video and is used in applications such as surveillance and autonomous vehicles. VSS focuses on understanding the overall scene and its contents and is used in applications such as augmented reality and video summarization.
![image](https://github.com/agoel11/KEYS2023/assets/81878922/56aa86e5-1246-47c6-9f19-80e836763085)
4. ####Video Object Segmentation (VOS) Methods and Models:
    1. VOS is the task of segmenting and tracking specific objects within a video. This is typically done by object initialization—identifying the object in the first frame of the video—and then tracking its movement           throughout the rest of the video. The goal is to segment the object from the background and the follow the changes in its movement throughout the video. There are various methods for object initialization, such as       manual annotation (most accurate but most time-consuming), automatic annotation (least accurate but fastest), semi-automatic annotation (balances accuracy and speed).
    2. After initialization the object must be tracked throughout the video. Methods for object tracking include traditional object tracking algorithms, such as the Kalman filter and the particle filter, and more recent       deep learning-based methods. These deep learning-based methods typically use a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) to segment and track objects.
    ![image](https://github.com/agoel11/KEYS2023/assets/81878922/6ab7da13-711a-44bd-9205-4af9539c7e22)
    3. Evaluation of video object segmentation methods is typically done using metrics such as the Intersection over Union (IoU) and the Multiple Object Tracking Accuracy (MOTA). IoU measures the overlap between the           predicted object mask and the ground truth mask, while MOTA measures the overall accuracy of the object tracking algorithm.
    4. #####Unsupervised VOS:
        1. As the name suggests, aims to segment objects in a video without using any labeled data. This task requires the model to learn the appearance and motion of objects in the video and to separate them from the           background. A popular approach to unsupervised VOS is based on optical flow. Optical flow is a technique that estimates the motion of pixels between frames. Optical flow can be used to track the motion of objects         in the video and to segment them from the background.
        2. An example of such a method is the Focus on Foreground Network (F2Net). F2Net exploits center point information in order to focus on the foreground object. It also establishes a "Center Prediction Branch" to           estimate the center location of the primary object. Then, the predicted center point is encoded into a gauss map as the spatial guidance prior to enhancing the intra-frame and inter-frame feature matching in our         Center Guiding Appearance Diffusion Module, leading the model to focus on the foreground object.
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/8d5c4715-fd77-4e53-9d3c-49fa5edc14ee)
        3. After the appearance matching process, F2Net gets three kinds of information flows: inter-frame features, intra-frame features, and original semantic features of the current frame. Instead of concatenating             these features, F2Net uses an attention-based Dynamic Information Fusion Module to automatically select the most discriminative features. This allows F2Net to produce better segmentation. 
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/3186b024-99f1-4a1e-98e7-a55a9c2265be)
    5. #####Semi-Supervised VOS:
        1. Semi-Supervised VOS uses small amounts of labeled data in order to guide the segmentation process, and then uses unsupervised methods to refine the results. In this way, Semi-Supervised VOS can leverage both           supervised and unsupervised methods to achieve higher efficiency and accuracy.
        2. The key advantage of this method is that it requires much less labeled data than a supervised approach. Additionally, the unsupervised methods used in semi-supervised VOS can help to improve the robustness and         generalization of the segmentation results, as they can take into account additional context and information that may not be present in the labeled data.
        3. The Sparse Spatiotemporal Transformers (SST) model proposed in 2021 uses semi-supervised learning for the VOS task. SST processes videos in a single pass of an efficient attention-based network. At every layer         of this net, each spatiotemporal feature vector simultaneously interacts with all other feature vectors in the video.
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/a4819d60-2d9b-488a-9471-f1392219b03e)
        4. SST being feedforward also helps it avoid the compounding issue present with recurrent methods. SST addresses computational complexity using sparse attention operator variants, making it possible to apply             self-attention to high-resolution videos.
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/eb292229-90bc-434d-9c15-6018c5f2f8d3)
    6. #####Interactive VOS:
        1. Interactive VOS is used to track and segment object in real-time. The user’s ability to provide input to the algorithm is what makes this method interactive. This user input can then guide the algorithm in its         segmentation and tracking of the object throughout the rest of the video.
        2. The main feature of interactive VOS is the ability to improve object segmentation and tracking accuracy and reliability. This technique can also help train more accurate object detection models by providing           annotated and labeled data.
        3. One of the problems or challenges associated with interactive VOS is choosing the frame through which the user should provide input. This is known as a Markov Decision Process (MDP).
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/45973fda-0288-4a42-8019-aced5747f07d)
    7. #####Language-guided VOS:
        1. Language-guided VOS uses natural language input to guide segmentation and tracking of objects in a video. Similar to interactive VOS, Language-guided VOS relies on user input, but the input is natural language         rather than outlines. This is typically done by using a combination of machine learning algorithms, such as Convolutional Neural Networks (CNNs) and Recurrent Neural networks (RNNs), and Natural Language                 Processing (NLP) techniques to understand the user's input.
        2. Natural language input allows for more flexible and intuitive interaction with the algorithm. Instead of defining markers or initial locations, the user can provide a simple verbal description of the object           they want tracked. This can be especially useful when there are multiple similar objects or they are difficult to locate.
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/d8ad0908-4a6c-49c7-ba6d-21653117c4b7)
        3. The algorithm first uses NLP techniques to process the user's input and extract relevant information about the object to be segmented and tracked. This information is then used to guide the segmentation and           tracking process.
        4. One such framework is the Multimodal Tracking Transformer (MTTR) where the objective is to segment text-referred object instances in the frames of a given video. The MTTR model extracts linguistic features             from the text query using a standard Transformer-based text encoder and visual features from the video frames using a spatiotemporal encoder. The features are then passed into a multimodal Transformer, which             outputs several sequences of object predictions.
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/d90381ad-857c-4351-8902-c4cb7003a385)
        5. To determine which predicted sequence best matches the user description, MTTR computes a text-reference score for each sequence for which a temporal segment voting scheme is developed. This allows the model to         focus on more relevant parts of the video when making the decision.
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/1ac681d6-9e57-42bf-8cb1-766a820e193f)
5. ####Video Semantic Segmentation (VSS) Methods and Models:
    1. VSS is the task of segmenting and understanding the semantic content of a video. Which means not only segmenting objects but also understanding their meaning and context. For example, a video semantic segmentation     model might be able to identify that a person is walking on a sidewalk, a car is driving on the road, and a building is a skyscraper. The goal is to understand the scene and its contents rather than just tracking         specific objects.
    2. The process of VSS begins with extracting features from the video frames using CNNs. CNNs can learn hierarchical representations of the image data, allowing them to understand the contents of the image at multiple     levels of abstraction. CNNs can learn hierarchical representations of the image data, allowing them to understand the contents of the image at multiple levels of abstraction. Those features are then used to classify     each pixel in a video. This is typically done using a fully convolutional network (FCN), a type of CNN designed for dense prediction tasks. FCNs can take an input image and produce a dense output, where each pixel in     the output corresponds to a class label (“object” or “background,” for example).
    3. VSS methods are evaluated using metrics such as the mean Intersection over Union (mIoU) and the Pixel Accuracies (PA). mIoU measures the average overlap between the predicted object mask and the ground truth mask,     while PA measures the overall accuracy of the object segmentation algorithm.
    
##Research Papers

###Links

###Notes

##Current Methods

###Links

###Notes
