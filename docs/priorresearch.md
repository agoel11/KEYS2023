#Prior Research/ Initial Fact-finding

##Project Description:                                                                                                                                                              
![image](https://github.com/agoel11/KEYS2023/assets/81878922/77e2dfbd-7cde-4ee9-9cde-72bf51fa559a)

##Concepts Overview:

###Links:                                                                                                                                                                                                              
[What is Deep Learning](https://aws.amazon.com/what-is/deep-learning/#:~:text=Deep%20learning%20is%20a%20method,produce%20accurate%20insights%20and%20predictions.)                                             
[What is Video Segmentation](https://www.v7labs.com/blog/video-segmentation-guide)

###Deep Learning:
1. Deep learning is a method in artificial intelligence (AI) that teaches computers to process data in a way that is inspired by the human brain. Deep learning models can recognize complex patterns in pictures, text, sounds, and other data to produce accurate insights and predictions. Deep learning methods can also be used to automate tasks that typically require human intelligence, such as describing images or transcribing a sound file into text.
2. Deep learning is used heavily in many different machine learning use cases and concepts like computer vision, speech recognition, natural language processing, and recommendation engines. Computer vision is the computer's ability to extract information and insights from images and videos. Computers can use deep learning techniques to comprehend images in the same way that humans do. Deep learning models can analyze human speech despite varying speech patterns, pitch, tone, language, and accent. Computers use deep learning algorithms to gather insights and meaning from text data and documents. Applications can use deep learning methods to track user activity and develop personalized recommendations. They can analyze the behavior of various users and help them discover new products or services.
3. Deep learning models use neural networks which contain thousands of artificial nodes and neurons in order to process data in a way similar to humans. These networks contains many layers that process data from the input layer and release the processed data to the output layer.
4. The input layer of a artificial neural network (ANN) has several nodes that input data into the network. The data is then passed into the hidden layer which processes and passes the data to layers further in the neural network. These hidden layers process information at different levels, adapting their behavior as they receive new information. Deep learning networks have hundreds of hidden layers that they can use to analyze a problem from several different angles. The processed data is then passed into the output layer which consists of the nodes that output the data. Deep learning models that output "yes" or "no" answers have only two nodes in the output layer. On the other hand, those that output a wider range of answers have more nodes. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/0eb15ca0-d355-4476-9a2c-0d0b3f9db981)  
5. Deep learning is a subset of machine learning. Deep learning algorithms emerged in an attempt to make traditional machine learning techniques more efficient. Traditional machine learning methods utilize what is known  as supervised learning in which the humans have to assist in training the machine. Deep learning generally relies on unsupervised learning and finds patterns in the data on its own, therefore making it much more          efficient.
7. Deep learning is better than machine learning because of a couple reasons. Deep learning models can comprehend unstructured data and make general observations without manual feature extraction. A deep learning          application can analyze large amounts of data more deeply and reveal new insights for which it might not have been trained. In this way deep learning has an advantage for finding hidden relationships and discovering      patterns. Deep learning models can learn and improve over time based on user behavior. They do not require large variations of labeled datasets. Once again, this is where unsupervised learning benefits deep learning      models.
8. However deep learning models have some drawbacks as well. They work a lot better when they are trained on large, high-quality datasets, and outliers or mistakes in the input dataset can significantly affect the deep    learning process. Because of this, deep learning requires lots of data pre-processing and data storage capacity as well. Deep learning algorithms are also compute-intensive and require infrastructure with sufficient      compute capacity to properly function. Otherwise, they take a long time to process results.

###Video Segmentation:
1. Video segmentation is the process by which videos are partitioned into seperate regions by a variety of characteristics. These characteristics include object boundaries, motion, color, texture, or other visual features. The goal of video segmentation is to seperate different objects from the background in a video and to provide a more detailed representation of the content.
2. Video segmentation is a very useful technology especially in the field of computer vision since it allows for the identification and characterization of individual objects and events in the video as well as the organization and classification of video content.
3. Video segmentation divdes the content into individual segments or shots (or frames) which can then be characterized and analyzed based on pre-defined attributes. It can also be performed at various levels of granularity, from a single object to whole backgrounds. Video segmentation uses two broad techniques, Video Object Segmentation (VOS) and Video Semantic Segmentation (VSS). VOS focuses on tracking objects within a video and is used in applications such as surveillance and autonomous vehicles. VSS focuses on understanding the overall scene and its contents and is used in applications such as augmented reality and video summarization.
![image](https://github.com/agoel11/KEYS2023/assets/81878922/56aa86e5-1246-47c6-9f19-80e836763085)
4. ####Video Object Segmentation (VOS) Methods and Models:
    1. VOS is the task of segmenting and tracking specific objects within a video. This is typically done by object initialization—identifying the object in the first frame of the video—and then tracking its movement           throughout the rest of the video. The goal is to segment the object from the background and the follow the changes in its movement throughout the video. There are various methods for object initialization, such as       manual annotation (most accurate but most time-consuming), automatic annotation (least accurate but fastest), semi-automatic annotation (balances accuracy and speed).
    2. After initialization the object must be tracked throughout the video. Methods for object tracking include traditional object tracking algorithms, such as the Kalman filter and the particle filter, and more recent       deep learning-based methods. These deep learning-based methods typically use a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) to segment and track objects.
    ![image](https://github.com/agoel11/KEYS2023/assets/81878922/6ab7da13-711a-44bd-9205-4af9539c7e22)
    3. Evaluation of video object segmentation methods is typically done using metrics such as the Intersection over Union (IoU) and the Multiple Object Tracking Accuracy (MOTA). IoU measures the overlap between the           predicted object mask and the ground truth mask, while MOTA measures the overall accuracy of the object tracking algorithm.
    4. #####Unsupervised VOS:
        1. As the name suggests, aims to segment objects in a video without using any labeled data. This task requires the model to learn the appearance and motion of objects in the video and to separate them from the           background. A popular approach to unsupervised VOS is based on optical flow. Optical flow is a technique that estimates the motion of pixels between frames. Optical flow can be used to track the motion of objects         in the video and to segment them from the background.
        2. An example of such a method is the Focus on Foreground Network (F2Net). F2Net exploits center point information in order to focus on the foreground object. It also establishes a "Center Prediction Branch" to           estimate the center location of the primary object. Then, the predicted center point is encoded into a gauss map as the spatial guidance prior to enhancing the intra-frame and inter-frame feature matching in our         Center Guiding Appearance Diffusion Module, leading the model to focus on the foreground object.
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/8d5c4715-fd77-4e53-9d3c-49fa5edc14ee)
        3. After the appearance matching process, F2Net gets three kinds of information flows: inter-frame features, intra-frame features, and original semantic features of the current frame. Instead of concatenating             these features, F2Net uses an attention-based Dynamic Information Fusion Module to automatically select the most discriminative features. This allows F2Net to produce better segmentation. 
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/3186b024-99f1-4a1e-98e7-a55a9c2265be)
    5. #####Semi-Supervised VOS:
        1. Semi-Supervised VOS uses small amounts of labeled data in order to guide the segmentation process, and then uses unsupervised methods to refine the results. In this way, Semi-Supervised VOS can leverage both           supervised and unsupervised methods to achieve higher efficiency and accuracy.
        2. The key advantage of this method is that it requires much less labeled data than a supervised approach. Additionally, the unsupervised methods used in semi-supervised VOS can help to improve the robustness and         generalization of the segmentation results, as they can take into account additional context and information that may not be present in the labeled data.
        3. The Sparse Spatiotemporal Transformers (SST) model proposed in 2021 uses semi-supervised learning for the VOS task. SST processes videos in a single pass of an efficient attention-based network. At every layer         of this net, each spatiotemporal feature vector simultaneously interacts with all other feature vectors in the video.
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/a4819d60-2d9b-488a-9471-f1392219b03e)
        4. SST being feedforward also helps it avoid the compounding issue present with recurrent methods. SST addresses computational complexity using sparse attention operator variants, making it possible to apply             self-attention to high-resolution videos.
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/eb292229-90bc-434d-9c15-6018c5f2f8d3)
    6. #####Interactive VOS:
        1. Interactive VOS is used to track and segment object in real-time. The user’s ability to provide input to the algorithm is what makes this method interactive. This user input can then guide the algorithm in its         segmentation and tracking of the object throughout the rest of the video.
        2. The main feature of interactive VOS is the ability to improve object segmentation and tracking accuracy and reliability. This technique can also help train more accurate object detection models by providing           annotated and labeled data.
        3. One of the problems or challenges associated with interactive VOS is choosing the frame through which the user should provide input. This is known as a Markov Decision Process (MDP).
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/45973fda-0288-4a42-8019-aced5747f07d)
    7. #####Language-guided VOS:
        1. Language-guided VOS uses natural language input to guide segmentation and tracking of objects in a video. Similar to interactive VOS, Language-guided VOS relies on user input, but the input is natural language         rather than outlines. This is typically done by using a combination of machine learning algorithms, such as Convolutional Neural Networks (CNNs) and Recurrent Neural networks (RNNs), and Natural Language                 Processing (NLP) techniques to understand the user's input.
        2. Natural language input allows for more flexible and intuitive interaction with the algorithm. Instead of defining markers or initial locations, the user can provide a simple verbal description of the object           they want tracked. This can be especially useful when there are multiple similar objects or they are difficult to locate.
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/d8ad0908-4a6c-49c7-ba6d-21653117c4b7)
        3. The algorithm first uses NLP techniques to process the user's input and extract relevant information about the object to be segmented and tracked. This information is then used to guide the segmentation and           tracking process.
        4. One such framework is the Multimodal Tracking Transformer (MTTR) where the objective is to segment text-referred object instances in the frames of a given video. The MTTR model extracts linguistic features             from the text query using a standard Transformer-based text encoder and visual features from the video frames using a spatiotemporal encoder. The features are then passed into a multimodal Transformer, which             outputs several sequences of object predictions.
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/d90381ad-857c-4351-8902-c4cb7003a385)
        5. To determine which predicted sequence best matches the user description, MTTR computes a text-reference score for each sequence for which a temporal segment voting scheme is developed. This allows the model to         focus on more relevant parts of the video when making the decision.
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/1ac681d6-9e57-42bf-8cb1-766a820e193f)
5. ####Video Semantic Segmentation (VSS) Methods and Models:
    1. VSS is the task of segmenting and understanding the semantic content of a video. Which means not only segmenting objects but also understanding their meaning and context. For example, a video semantic segmentation     model might be able to identify that a person is walking on a sidewalk, a car is driving on the road, and a building is a skyscraper. The goal is to understand the scene and its contents rather than just tracking         specific objects.
    2. The process of VSS begins with extracting features from the video frames using CNNs. CNNs can learn hierarchical representations of the image data, allowing them to understand the contents of the image at multiple     levels of abstraction. CNNs can learn hierarchical representations of the image data, allowing them to understand the contents of the image at multiple levels of abstraction. Those features are then used to classify     each pixel in a video. This is typically done using a fully convolutional network (FCN), a type of CNN designed for dense prediction tasks. FCNs can take an input image and produce a dense output, where each pixel in     the output corresponds to a class label (“object” or “background,” for example).
    3. VSS methods are evaluated using metrics such as the mean Intersection over Union (mIoU) and the Pixel Accuracies (PA). mIoU measures the average overlap between the predicted object mask and the ground truth mask,     while PA measures the overall accuracy of the object segmentation algorithm.
    4. #####Instance-Agnostic VSS:
        1. Instance-Agnostic VSS is a method to identify and segment objects in a video sequence without considering the individual instances of the objects. This approach is in contrast to instance-aware semantic               segmentation, which tracks and segments individual instances of objects within a video, making it less computationally demanding.
        2. The Temporally Distributed Network (TDNet) is an example of a video instance segmentation architecture inspired by Group Convolutions, which shows that extracting features with separated filter groups not only         allows for model parallelization but also helps learn better representations.
        3. Given a deep image segmentation network, TDNet divides the features extracted by the deep model into N (e.g., N=2 or 4) groups and uses N distinct shallow sub-networks to approximate each group of feature             channels. By forcing each sub-network to cover a separate feature subspace, a strong feature representation can be produced by reassembling the output of these sub-networks. For balanced and efficient computation         over time, the N sub-networks share the same shallow architecture, which is set to be (1/N) of the original deep model’s size to preserve a similar total model capacity.
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/abd102aa-484f-4bf3-88a8-41b9d7841fe7)
        4. The architecture is coupled with a grouped Knowledge Distillation loss to accelerate the semantic segmentation models for videos.
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/e94aff12-73cc-4069-801e-83a64c2589c8)
    5. #####Video Instance Segmentation:
        1. Video Instance Segmentation identifies and segments individual instances of objects within a video. This approach is in contrast to the instance-agnostic VSS, which only identifies and segments objects within         a video without considering individual instances.
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/27729eea-01d2-4f47-a275-e527d727ba62)
        2. Video instance segmentation Transformer (VisTR) is a framework built for instance segmentation that views the instance segmentation task as a parallel sequence decoding/prediction problem. Given a video clip           that has multiple image frames as input, the VisTR algorithm outputs the sequence of masks for each instance in the video directly.
        3. Given a sequence of video frames, a CNN module extracts features of individual image frames. THe multiple image features are then concatenated into the frame order to form the clip-level feature sequence.             Next, the Transformer takes the clip-level feature sequence as input and outputs a sequence of object predictions in order.
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/af274a35-2230-408b-ac47-8b1da71ce091)
        4. The sequence of predictions follows the order of input images, and the predictions of each image follow the same instance order. Thus, instance tracking is achieved seamlessly and naturally in the same                 framework of instance segmentation.
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/dc835abb-ef3e-4dec-b4b4-1b57bab7a4ff)
    6. #####Video Panoptic Segmentation:
        1. Video panoptic segmentation (VPS) identifies and segments both objects and their parts in a video sequence in a single step. This method combines the strengths of both instance-agnostic VSS and video instance         segmentation.
        2. One of the biggest advantages of VPS is that it can differentiate between object, object parts, and backgrounds in a video, all of which provides for a more detailed understanding of the scene. It also allows         us to distinguish and segment multiple instances of the same object in a video, even when they overlap, which comes at the cost of high computational demand.
        3. An example of this is the ViP-DeepLab which performs Depth-aware Video Panoptic Segmentation (DVPS) in order to solve the inverse projection problem (which refers to the ambiguous mapping from the                     retinal images to the sources of retinal stimulation).
        4. It has been found that VPS can be modeled as concatenated image panoptic segmentation. As a result of this, the Panoptic-DeepLab model has been used to perform center regression for two consecutive frames with         respect to only the object centers appearing in the first frame. During inference, this offset prediction allows ViP-DeepLab to group all the pixels in the two frames to the same object that appears in the               first frame. New instances emerge if they are not grouped with the previously detected instances.
        
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/9fa1ce34-66e0-43bc-af76-d1953af3f7c1)
        
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/3d7dd2d0-9124-4909-baa9-d4805f9af1a1)
        
        ![image](https://github.com/agoel11/KEYS2023/assets/81878922/7bfc04ce-e9c2-4273-8c9b-272ac653f30e)
        
 6. ####Challenges of Video Segmentation:
    1. Variability in Video Content and Quality - This can be as simple as variations in lighting, resolution, frame rate, and other factors that can affect the appearance and characteristics of the video. There are some     methods in order to fight large variations in object appearance, such as multi-scale features, deep-learning methods, and domain adaption techniques. To comabt variations in lighting or viewpoints, methods such as       color histograms and texture features can be used.
    2. Lack of Temporal Consistency - Videos are a sequence of frames, and the contents of the frames can change significantly from frame to frame. This makes it difficult to maintain consistency in the segmentation         across frames. Methods for dealing with temporal consistency include using recurrent neural networks (RNNs), optical flow, or motion features.
    3. Occlusions - Occlusions occur when one object blocks the view of another object, making it difficult or impossible to track. There are various methods for dealing with occlusions, including using multiple cameras     or sensors, depth sensors, and object re-detection.
    4. Complexity of Visual Scenes - Video segmentation can be challenging due to the complexity of the visual scenes depicted in the video. This can include the presence of multiple objects and events, as well as           occlusions, reflections, and other visual distractions that can make it challenging to identify and segment the content of the video.
    5. Lack of Training Data - Supervised approaches for video segmentation require the availability of labeled training data, which can be challenging to obtain for many video datasets. This can limit the effectiveness     and generalizability of these approaches.
    6. Computational Complexity - Video segmentation can be computationally intensive, especially for large or high-resolution video datasets. This poses challenges in performing real-time or online video segmentation or     scaling the segmentation process to extensive video collections.
    7. Evaluation and Benchmarking - Evaluating the performance of video segmentation approaches can be difficult due to the lack of standardized benchmarks and evaluation metrics. This can make it challenging to compare     and evaluate different approaches or to determine the best approach for a given video dataset.

##Method Search & Software:

###Links:

[DeepLabCut Official](http://www.mackenziemathislab.org/deeplabcut-home)  
[DeepLabCut GitHub](https://github.com/DeepLabCut)

###DeepLabCut:
1. DeepLabCut is a software package designed for 2D and 3D markerless pose estimation based on transfer learning with deep neural networks. DeepLabCut is very accurate and efficient and requires minimal training data as well. The versatility of this framework is demonstrated by tracking various body parts in multiple species across a broad collection of behaviors. The package is open source, fast, robust, and can be used to compute 3D pose estimates or for multi-animals. This package is collaboratively developed by the Mathis Group & Mathis Lab at EPFL (releases prior to 2.1.9 were developed at Harvard University).
2. To use DeepLabCut you can use their own GUI, their Jupyter Notebook, their Google Colab, or your own terminal. They also provide lots of data that helps you demo the package and test installation.![image](https://github.com/agoel11/KEYS2023/assets/81878922/e87628ff-14ee-47bd-8a0d-3c8b2295feef)
3. DeepLabCut has been used for trail tracking, reaching in mice, and various Drosophila behaviours during egg-laying. But this toolbox has a variety of applications and it has already been applied to rats, humans, various fish species, bacteria, leeches, various robots, cheetahs, mouse whiskers and race horses. DeepLabCut utilizes the feature detectors (ResNets + readout layers) of one of the state-of-the-art algorithms for human pose estimation by Insafutdinov et al., called DeeperCut. They have improved the inference speed and provided both additional and novel augmentation methods, added real-time, and multi-animal support and currently provide state-of-the-art performance for animal pose estimation.
4. Because of transfer learning, the package requires little training data for multiple challenging behaviors. The feature detectors are robust to video compression. It allows 3D pose estimation with a single network and camera. It allows 3D pose estimation with a single network trained on data from multiple cameras together with standard triangulation methods. DeepLabCut is embedding in a larger open-source eco-system, providing behavioral tracking for neuroscience, ecology, medical, and technical applications. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/cd61a3ad-5a8f-415c-ab18-ec0fe46b77af)
5. ####DeepLabCut: Markerless Pose Estimation of User-Defined Body Parts With Deep Learning
    1. New methods for markerless tracking using deep learning and neural networks. Excellent performance, comparable to human accuracy. Minimal training data also yields excellent results across various species and behaviors.
    2. 
