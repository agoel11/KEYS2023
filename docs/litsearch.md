##Method Search & Software:

###Links:

[DeepLabCut Official](http://www.mackenziemathislab.org/deeplabcut-home)  
[DeepLabCut GitHub](https://github.com/DeepLabCut)  
[Track-Anything GitHub](https://github.com/gaomingqi/Track-Anything/blob/master/README.md)  
[XMem GitHub](https://github.com/hkchengrex/XMem)

###DeepLabCut:
1. DeepLabCut is a software package designed for 2D and 3D markerless pose estimation based on transfer learning with deep neural networks. DeepLabCut is very accurate and efficient and requires minimal training data as well. The versatility of this framework is demonstrated by tracking various body parts in multiple species across a broad collection of behaviors. The package is open source, fast, robust, and can be used to compute 3D pose estimates or for multi-animals. This package is collaboratively developed by the Mathis Group & Mathis Lab at EPFL (releases prior to 2.1.9 were developed at Harvard University).
2. To use DeepLabCut you can use their own GUI, their Jupyter Notebook, their Google Colab, or your own terminal. They also provide lots of data that helps you demo the package and test installation.![image](https://github.com/agoel11/KEYS2023/assets/81878922/e87628ff-14ee-47bd-8a0d-3c8b2295feef)
3. DeepLabCut has been used for trail tracking, reaching in mice, and various Drosophila behaviours during egg-laying. But this toolbox has a variety of applications and it has already been applied to rats, humans, various fish species, bacteria, leeches, various robots, cheetahs, mouse whiskers and race horses. DeepLabCut utilizes the feature detectors (ResNets + readout layers) of one of the state-of-the-art algorithms for human pose estimation by Insafutdinov et al., called DeeperCut. They have improved the inference speed and provided both additional and novel augmentation methods, added real-time, and multi-animal support and currently provide state-of-the-art performance for animal pose estimation.
4. Because of transfer learning, the package requires little training data for multiple challenging behaviors. The feature detectors are robust to video compression. It allows 3D pose estimation with a single network and camera. It allows 3D pose estimation with a single network trained on data from multiple cameras together with standard triangulation methods. DeepLabCut is embedding in a larger open-source eco-system, providing behavioral tracking for neuroscience, ecology, medical, and technical applications. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/cd61a3ad-5a8f-415c-ab18-ec0fe46b77af)
5. ####[DeepLabCut: Markerless Pose Estimation of User-Defined Body Parts With Deep Learning](https://doi.org/10.1038/s41593-018-0209-y)
    1. New methods for markerless tracking using deep learning and neural networks. Excellent performance, comparable to human accuracy. Minimal training data also yields excellent results across various species and behaviors.
    2. Quantification of behavior essential for understanding brain. Computer vision much easier and more efficient than manual analysis. Markers, used traditionally, invade space and aren't cost effective and are distracting to subjects. Deep learning architecture greatly improve accuracy of pose estimation. Large datasets can be tackled by using transfer learning.
    3. Used feature detection architecture from DeeperCut (best pose estimation algorithms, can acheieve human-level labeling accuracy with minimal data). Result of transfer learning, feature detectors are based on extremely deep neural networks, which were pretrained on ImageNet, a massive dataset for object recognition.
    4. DeeperCut achieves outstanding performance on multi-human pose detection benchmarks, trained on thousands of labeled images. DeepLabCut is DeeperCut but focuses on feature detectors, which are variations of deep residual neural networks (ResNet) with readout layers that predict the location of a body part.
    5. DeepLabCut is a deep convolutional network combining two key ingredients from object recognition and semantic segmentation: pretrained ResNets and deconvolutional layers. The network consists of a variant of ResNets, whose weights were trained on a popular, large-scale object recognition benchmark called ImageNet. Deconvolutional layers are used to up-sample the visual information and produce spatial probability densities. For each body part, its probability density represents the ‘evidence’ that a body part is in a particular location. To fine-tune the network for a particular task, its weights are trained on labeled data, which consist of frames and the accompanying annotated body part locations. The weights are adjusted in an iterative fashion such that for a given frame the network assigns high probabilities to labeled body part locations and low probabilities elsewhere. The network is rewired and ‘learns’ feature detectors for the labeled body parts. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/028d8ebb-13f3-4bd4-8882-37e622e7711c)
    6. Average variability to ground truth was found to be very low and small. To quantidy accuracy, datasets were split and a certain percentage was used to train while the rest was used to test. When trained with 80% of
the data the algorithm achieved human-level accuracy. After varying training and testing percentages, it was found that even 100 frames were enough to achieve excellent generalization. Data augmentation (such as rotations or translations) also resulted in minimal differences, demonstrating the data-efficiency of DeepLabCut. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/6bf3d011-6145-458d-9e50-d91172aac2c4)
    7. The feature detectors were able to translate pretty well to novel mouse behaviors as well as videos with multiple mice. Although it wasn't error free, this can be improved by simply training on that data or on data for multiple mice. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/b9e8c1d2-aebe-410d-be88-d239c2f84903)
    8. End-to-end training allows the model to facilitate the localization of one body part based on other labeled body parts. The network that was trained with all body part labels simultaneously outperforms the specialized networks nearly twofold. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/88060d7d-89ba-44d2-8b36-90f7c40824f5)
    9. Temporal information could indeed be beneficial in certain contexts, challenges remain to using end-to-end-trained deep architectures for video data to extract postures. Because of the curse of dimensionality, deep architectures on videos must rely on input images with lower spatial resolution, and thus the best-performing action recognition algorithms still rely on frame-by-frame analysis with deep networks pretrained on ImageNet as a result of hardware limitations. Therefore currently, in situations where occlusions are very common, such as in social behaviors, pairwise interactions could also be added to improve performance.
6. ####[Using DeepLabCut for 3D Markerless Pose Estimation Across Species and Behaviors](https://doi.org/10.1101/476531)
    1. Transfer learning, the ability to take a network, which was trained on a task with a large supervised data set, and utilize it for another task with a small supervised data set, is beginning to allow users to broadly apply deep learning methods. DeepLabCut provides tools to create annotated training sets, train robust feature detectors, and utilize them to analyze novel behavioral videos.
    2. The major motivation for developing the DeepLabCut toolbox was to provide a reliable and efficient tool for high-throughput video analysis, where powerful feature detectors of user-defined body parts need to be learned for a specific situation. The toolbox is aimed to solve the problem of detecting body parts in dynamic visual environments where varying background, reflective walls or motion blur hinder the performance of common techniques, such as thresholding or regression based on visual features. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/4c483668-9292-4897-972a-1c9feebcf69f)
    3. The user starts by creating a new project based on a project and username as well as some (initial) videos, which are required to create the training dataset (additional videos can also be added after the creation of the project). Next, DeepLabCut extracts frames, which reflect the diversity of the behavior with respect to postures, animal identities, etc. Then the user can label the points of interest in the extracted frames. These
annotated frames can be visually checked for accuracy, and corrected if necessary. Eventually, a training dataset is created by merging all the extracted labeled frames and splitting them into subsets of test and train frames. Then, a pre-trained network (ResNet) is trained end-to-end to adapt its weights in order to predict the desired features. The performance of the trained network can then be evaluated on the training and test frames. The trained network can be used to analyze videos yielding extracted pose files.  In case the trained network does not generalize well to unseen data in the evaluation and analysis step, then additional frames with poor results can be extracted and the predicted labels can be manually shifted to their ideal location. This refinement step, if needed, creates an additional set of annotated images that can then be merged with the original training dataset to create a new training dataset. This larger training set can then be used to re-train the feature detectors for better results. This active learning loop can be done iteratively to robustly and accurately analyze videos with potentially large variability- i.e. experiments that include many individuals, and run over long time periods. Furthermore, the user can add additional body parts/labels at later stages during a project as well as correct user-defined labels. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/d3a53dad-bbb0-47d8-b8ff-b2d939e9f728)
    4. However, if a user aims to track (adult) human poses, many excellent options exist, including DeeperCut, ArtTrack, DeepPose, OpenPose, and OpenPose-Plus (better for humans).
    5. DeepLabCut does not support occlusions, requires HPC and GPUs, and also workers faster with smaller images.

###Track-Anything:
1. Track-Anything is a flexible and interactive tool for video object tracking and segmentation. It is developed upon Segment Anything, can specify anything to track and segment via user clicks only. During tracking, users can flexibly change the objects they wanna track or correct the region of interest if there are any ambiguities.
2. Track-Anything is suitable for video object tracking and segmentation with shot changes, visualized development and data annotation for video object tracking and segmentation, object-centric downstream video tasks (such as video inpainting and editing).
3. ####[Track Anything: Segment Anything Meets Videos](https://arxiv.org/pdf/2304.11968.pdf)
    4. Segment Anything Model (SAM) displays impressive performance on images, but it performs poorly on video segmentation. The proposed Track Anything Model (TAM), achieves high-performance interactive tracking and segmentation in videos.
    5. Current state-of-the-art video trackers/segmenters are trained on large-scale manually-annotated datasets and initialized by a bounding box or a segmentation mask. Moreover, current initialization settings, especially the semi-supervised VOS, need specific object mask groundtruth for model initialization. Large amouns of human labor is also required for huge amounts of annotated and labeled data.
    6. Recently, Segment-Anything Model (SAM) has been proposed, which is a large foundation model for image segmentation. It supports flexible prompts and computes masks in real-time, thus allowing interactive use. Trained on 11 million images and 1.1 billion masks, SAM can produce high-quality masks and do zero-shot segmentation in generic scenarios. With input user-friendly prompts of points, boxes, or language, SAM can give satisfactory segmentation masks on specific image areas. However, using SAM in videos directly does not give an impressive performance due to its deficiency in temporal correspondence. But tracking or segmenting in videos faces challenges from scale variation, target deformation, motion blur, camera motion, similar objects, and so on.
    7. In this paper, the Track-Anything toolkit is proposed for high-performance object tracking and segmentation in videos. With a user-friendly interface, the Track Anything Model (TAM) can track and segment any objects in a given video with only one-pass inference. TAM combines SAM, a large segmentation model, and XMem, an advanced VOS model. Firstly, users can interactively initialize the SAM, i.e., clicking on the object, to define a target object; then, XMem is used to give a mask prediction of the object in the next frame according to both temporal and spatial correspondence; next, SAM is utilized to give a more precise mask description; during the tracking process, users can pause and correct as soon as they notice tracking failures.
    8. TAM is able to promote the SAM applications to the video level to achieve interactive video object tracking and segmentation. Rather than separately using SAM per frame, it integrates SAM into the process of temporal correspondence construction. The Track Anything task aims for flexible object tracking in arbitrary videos. The target objects can be flexibly selected, added, or removed in any way according to the users’ interests. Also, the video length and types can be arbitrary rather than limited to trimmed or natural videos. With such settings, diverse downstream tasks can be achieved, including single/multiple object tracking, short-/long-term object tracking, unsupervised VOS, semi-supervised VOS, referring VOS, interactive VOS, long-term VOS, and more. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/88c0ad4d-b6e1-4afd-b6f3-cfd964cdd4d7)
    9. As a foundation model for image segmentation (and the genesis for TAM), SAM is based on ViT and trained on the large-scale dataset SA-1B. Obviously, SAM shows promising segmentation ability on images, especially on zero-shot segmentation tasks. Unfortunately, SAM only shows superior performance on image segmentation, while it cannot deal with complex video segmentation.
    10. Given the mask description of the target object at the first frame, XMem can track the object and generate corresponding masks in the subsequent frames. Inspired by the Atkinson-Shiffrin memory model, it aims to solve the difficulties in long-term videos with unified feature memory stores. The drawbacks of XMem are also obvious: as a semi-supervised VOS model, it requires a precise mask to initialize; for long videos, it is difficult for XMem to recover from tracking or segmentation failure. In this paper, we solve both difficulties by importing interactive tracking with SAM.
    11. The Track-Anything process is divided into the following processes:
        1. Initialization with SAM - As SAM provides an opportunity to segment a region of interest with weak prompts, e.g., points, and bounding boxes, we use it to give an initial mask of the target object. Following SAM, users can get a mask description of the interested object by a click or modify the object mask with several clicks to get a satisfactory initialization.
        2. Tracking with XMem - Given the initialized mask, XMem performs semi-supervised VOS on the following frames. Since XMem is an advanced VOS method that can output satisfactory results on simple scenarios, we output the predicted masks of XMem on most occasions. When the mask quality is not as good, we save the XMem predictions and corresponding intermediate parameters, i.e., probes and affinities, and skip to the next step.
        3. Refinement with SAM - During the inference of VOS models, keep predicting consistent and precise masks are challenging. In fact, most state-of-the-art VOS models tend to segment more and more coarsely over time during inference. Therefore, we utilize SAM to refine the masks predicted by XMem when its quality assessment is not satisfactory. Specifically, we project the probes and affinities to be point prompts for SAM, and the predicted mask from Step 2 is used as a mask prompt for SAM. Then, with these prompts, SAM is able to produce a refined segmentation mask. Such refined masks will also be added to the temporal correspondence of XMem to refine all subsequent object discrimination.
        4. Correction with human participation - After the above three steps, the TAM can now successfully solve some common challenges and predict segmentation masks. However, it is still difficult to accurately distinguish the objects in some extremely challenging scenarios, especially when processing long videos. Therefore, we propose to add human correction during inference, which can bring a qualitative leap in performance with only very small human efforts. In detail, users can compulsively stop the TAM process and correct the mask of the current frame with positive and negative clicks. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/8d5b3478-e24a-4293-95fb-9c6b37b50c68)
        5. TAM can handle multi-object separation, target deformation, scale change, and camera motion well, which demonstrates its superior tracking and segmentation abilities within only click initialization and one-round inference. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/4c07b326-bb83-488f-9a4d-f81e9c8bb95a)
        6. Some failed cases: ![image](https://github.com/agoel11/KEYS2023/assets/81878922/1f2f0236-5846-472a-bdbc-e4a914d97059)
        7. The failed cases typically appear on the following two occasions. Current VOS models are mostly designed for short videos, which focus more on maintaining short-term memory rather than long-term memory. This leads to mask shrinkage or lacking refinement in long-term videos, as shown in seq (a). Essentially, this is solved in step 3 by the refinement ability of SAM, while its effectiveness is lower than
expected in realistic applications. It indicates that the ability of SAM refinement based on multiple prompts can be further improved in the future. On the other hand, human participation/interaction in TAM can be an approach to solving such difficulties, while too much interaction will also result in low efficiency. Thus, the mechanism of long-term memory preserving and transient memory updating is still important.
        8. When the object structure is complex, e.g., the bicycle wheels in seq (b) contain many cavities in groundtruth masks. It is very difficult to get a fine-grained initialized mask by propagating the clicks. Thus, the coarse initialized masks may have side effects on the subsequent frames and lead to poor predictions. This suggests that SAM is still struggling with complex and precision structures.

###XMem:
1. 
