##Method Search & Software:

###Links:

[DeepLabCut Official](http://www.mackenziemathislab.org/deeplabcut-home)  
[DeepLabCut GitHub](https://github.com/DeepLabCut)  
[Track-Anything GitHub](https://github.com/gaomingqi/Track-Anything/blob/master/README.md)  
[XMem GitHub](https://github.com/hkchengrex/XMem)  
[MaskFreeVIS Official](https://www.vis.xyz/pub/maskfreevis/)  
[MaskFreeVIS GitHub](https://github.com/SysCV/MaskFreeVIS)  

###DeepLabCut:
1. DeepLabCut is a software package designed for 2D and 3D markerless pose estimation based on transfer learning with deep neural networks. DeepLabCut is very accurate and efficient and requires minimal training data as well. The versatility of this framework is demonstrated by tracking various body parts in multiple species across a broad collection of behaviors. The package is open source, fast, robust, and can be used to compute 3D pose estimates or for multi-animals. This package is collaboratively developed by the Mathis Group & Mathis Lab at EPFL (releases prior to 2.1.9 were developed at Harvard University).
2. To use DeepLabCut you can use their own GUI, their Jupyter Notebook, their Google Colab, or your own terminal. They also provide lots of data that helps you demo the package and test installation.![image](https://github.com/agoel11/KEYS2023/assets/81878922/e87628ff-14ee-47bd-8a0d-3c8b2295feef)
3. DeepLabCut has been used for trail tracking, reaching in mice, and various Drosophila behaviours during egg-laying. But this toolbox has a variety of applications and it has already been applied to rats, humans, various fish species, bacteria, leeches, various robots, cheetahs, mouse whiskers and race horses. DeepLabCut utilizes the feature detectors (ResNets + readout layers) of one of the state-of-the-art algorithms for human pose estimation by Insafutdinov et al., called DeeperCut. They have improved the inference speed and provided both additional and novel augmentation methods, added real-time, and multi-animal support and currently provide state-of-the-art performance for animal pose estimation.
4. Because of transfer learning, the package requires little training data for multiple challenging behaviors. The feature detectors are robust to video compression. It allows 3D pose estimation with a single network and camera. It allows 3D pose estimation with a single network trained on data from multiple cameras together with standard triangulation methods. DeepLabCut is embedding in a larger open-source eco-system, providing behavioral tracking for neuroscience, ecology, medical, and technical applications. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/cd61a3ad-5a8f-415c-ab18-ec0fe46b77af)
5. ####[DeepLabCut: Markerless Pose Estimation of User-Defined Body Parts With Deep Learning](https://doi.org/10.1038/s41593-018-0209-y):
    1. New methods for markerless tracking using deep learning and neural networks. Excellent performance, comparable to human accuracy. Minimal training data also yields excellent results across various species and behaviors.
    2. Quantification of behavior essential for understanding brain. Computer vision much easier and more efficient than manual analysis. Markers, used traditionally, invade space and aren't cost effective and are distracting to subjects. Deep learning architecture greatly improve accuracy of pose estimation. Large datasets can be tackled by using transfer learning.
    3. Used feature detection architecture from DeeperCut (best pose estimation algorithms, can acheieve human-level labeling accuracy with minimal data). Result of transfer learning, feature detectors are based on extremely deep neural networks, which were pretrained on ImageNet, a massive dataset for object recognition.
    4. DeeperCut achieves outstanding performance on multi-human pose detection benchmarks, trained on thousands of labeled images. DeepLabCut is DeeperCut but focuses on feature detectors, which are variations of deep residual neural networks (ResNet) with readout layers that predict the location of a body part.
    5. DeepLabCut is a deep convolutional network combining two key ingredients from object recognition and semantic segmentation: pretrained ResNets and deconvolutional layers. The network consists of a variant of ResNets, whose weights were trained on a popular, large-scale object recognition benchmark called ImageNet. Deconvolutional layers are used to up-sample the visual information and produce spatial probability densities. For each body part, its probability density represents the ‘evidence’ that a body part is in a particular location. To fine-tune the network for a particular task, its weights are trained on labeled data, which consist of frames and the accompanying annotated body part locations. The weights are adjusted in an iterative fashion such that for a given frame the network assigns high probabilities to labeled body part locations and low probabilities elsewhere. The network is rewired and ‘learns’ feature detectors for the labeled body parts. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/028d8ebb-13f3-4bd4-8882-37e622e7711c)
    6. Average variability to ground truth was found to be very low and small. To quantidy accuracy, datasets were split and a certain percentage was used to train while the rest was used to test. When trained with 80% of
the data the algorithm achieved human-level accuracy. After varying training and testing percentages, it was found that even 100 frames were enough to achieve excellent generalization. Data augmentation (such as rotations or translations) also resulted in minimal differences, demonstrating the data-efficiency of DeepLabCut. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/6bf3d011-6145-458d-9e50-d91172aac2c4)
    7. The feature detectors were able to translate pretty well to novel mouse behaviors as well as videos with multiple mice. Although it wasn't error free, this can be improved by simply training on that data or on data for multiple mice. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/b9e8c1d2-aebe-410d-be88-d239c2f84903)
    8. End-to-end training allows the model to facilitate the localization of one body part based on other labeled body parts. The network that was trained with all body part labels simultaneously outperforms the specialized networks nearly twofold. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/88060d7d-89ba-44d2-8b36-90f7c40824f5)
    9. Temporal information could indeed be beneficial in certain contexts, challenges remain to using end-to-end-trained deep architectures for video data to extract postures. Because of the curse of dimensionality, deep architectures on videos must rely on input images with lower spatial resolution, and thus the best-performing action recognition algorithms still rely on frame-by-frame analysis with deep networks pretrained on ImageNet as a result of hardware limitations. Therefore currently, in situations where occlusions are very common, such as in social behaviors, pairwise interactions could also be added to improve performance.
6. ####[Using DeepLabCut for 3D Markerless Pose Estimation Across Species and Behaviors](https://doi.org/10.1101/476531):
    1. Transfer learning, the ability to take a network, which was trained on a task with a large supervised data set, and utilize it for another task with a small supervised data set, is beginning to allow users to broadly apply deep learning methods. DeepLabCut provides tools to create annotated training sets, train robust feature detectors, and utilize them to analyze novel behavioral videos.
    2. The major motivation for developing the DeepLabCut toolbox was to provide a reliable and efficient tool for high-throughput video analysis, where powerful feature detectors of user-defined body parts need to be learned for a specific situation. The toolbox is aimed to solve the problem of detecting body parts in dynamic visual environments where varying background, reflective walls or motion blur hinder the performance of common techniques, such as thresholding or regression based on visual features. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/4c483668-9292-4897-972a-1c9feebcf69f)
    3. The user starts by creating a new project based on a project and username as well as some (initial) videos, which are required to create the training dataset (additional videos can also be added after the creation of the project). Next, DeepLabCut extracts frames, which reflect the diversity of the behavior with respect to postures, animal identities, etc. Then the user can label the points of interest in the extracted frames. These
annotated frames can be visually checked for accuracy, and corrected if necessary. Eventually, a training dataset is created by merging all the extracted labeled frames and splitting them into subsets of test and train frames. Then, a pre-trained network (ResNet) is trained end-to-end to adapt its weights in order to predict the desired features. The performance of the trained network can then be evaluated on the training and test frames. The trained network can be used to analyze videos yielding extracted pose files.  In case the trained network does not generalize well to unseen data in the evaluation and analysis step, then additional frames with poor results can be extracted and the predicted labels can be manually shifted to their ideal location. This refinement step, if needed, creates an additional set of annotated images that can then be merged with the original training dataset to create a new training dataset. This larger training set can then be used to re-train the feature detectors for better results. This active learning loop can be done iteratively to robustly and accurately analyze videos with potentially large variability- i.e. experiments that include many individuals, and run over long time periods. Furthermore, the user can add additional body parts/labels at later stages during a project as well as correct user-defined labels. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/d3a53dad-bbb0-47d8-b8ff-b2d939e9f728)
    4. However, if a user aims to track (adult) human poses, many excellent options exist, including DeeperCut, ArtTrack, DeepPose, OpenPose, and OpenPose-Plus (better for humans).
    5. DeepLabCut does not support occlusions, requires HPC and GPUs, and also workers faster with smaller images.

###Track-Anything:
1. Track-Anything is a flexible and interactive tool for video object tracking and segmentation. It is developed upon Segment Anything, can specify anything to track and segment via user clicks only. During tracking, users can flexibly change the objects they wanna track or correct the region of interest if there are any ambiguities.
2. Track-Anything is suitable for video object tracking and segmentation with shot changes, visualized development and data annotation for video object tracking and segmentation, object-centric downstream video tasks (such as video inpainting and editing).
3. ####[Track Anything: Segment Anything Meets Videos](https://arxiv.org/pdf/2304.11968.pdf):
    4. Segment Anything Model (SAM) displays impressive performance on images, but it performs poorly on video segmentation. The proposed Track Anything Model (TAM), achieves high-performance interactive tracking and segmentation in videos.
    5. Current state-of-the-art video trackers/segmenters are trained on large-scale manually-annotated datasets and initialized by a bounding box or a segmentation mask. Moreover, current initialization settings, especially the semi-supervised VOS, need specific object mask groundtruth for model initialization. Large amouns of human labor is also required for huge amounts of annotated and labeled data.
    6. Recently, Segment-Anything Model (SAM) has been proposed, which is a large foundation model for image segmentation. It supports flexible prompts and computes masks in real-time, thus allowing interactive use. Trained on 11 million images and 1.1 billion masks, SAM can produce high-quality masks and do zero-shot segmentation in generic scenarios. With input user-friendly prompts of points, boxes, or language, SAM can give satisfactory segmentation masks on specific image areas. However, using SAM in videos directly does not give an impressive performance due to its deficiency in temporal correspondence. But tracking or segmenting in videos faces challenges from scale variation, target deformation, motion blur, camera motion, similar objects, and so on.
    7. In this paper, the Track-Anything toolkit is proposed for high-performance object tracking and segmentation in videos. With a user-friendly interface, the Track Anything Model (TAM) can track and segment any objects in a given video with only one-pass inference. TAM combines SAM, a large segmentation model, and XMem, an advanced VOS model. Firstly, users can interactively initialize the SAM, i.e., clicking on the object, to define a target object; then, XMem is used to give a mask prediction of the object in the next frame according to both temporal and spatial correspondence; next, SAM is utilized to give a more precise mask description; during the tracking process, users can pause and correct as soon as they notice tracking failures.
    8. TAM is able to promote the SAM applications to the video level to achieve interactive video object tracking and segmentation. Rather than separately using SAM per frame, it integrates SAM into the process of temporal correspondence construction. The Track Anything task aims for flexible object tracking in arbitrary videos. The target objects can be flexibly selected, added, or removed in any way according to the users’ interests. Also, the video length and types can be arbitrary rather than limited to trimmed or natural videos. With such settings, diverse downstream tasks can be achieved, including single/multiple object tracking, short-/long-term object tracking, unsupervised VOS, semi-supervised VOS, referring VOS, interactive VOS, long-term VOS, and more. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/88c0ad4d-b6e1-4afd-b6f3-cfd964cdd4d7)
    9. As a foundation model for image segmentation (and the genesis for TAM), SAM is based on ViT and trained on the large-scale dataset SA-1B. Obviously, SAM shows promising segmentation ability on images, especially on zero-shot segmentation tasks. Unfortunately, SAM only shows superior performance on image segmentation, while it cannot deal with complex video segmentation.
    10. Given the mask description of the target object at the first frame, XMem can track the object and generate corresponding masks in the subsequent frames. Inspired by the Atkinson-Shiffrin memory model, it aims to solve the difficulties in long-term videos with unified feature memory stores. The drawbacks of XMem are also obvious: as a semi-supervised VOS model, it requires a precise mask to initialize; for long videos, it is difficult for XMem to recover from tracking or segmentation failure. In this paper, we solve both difficulties by importing interactive tracking with SAM.
    11. The Track-Anything process is divided into the following processes:
        1. Initialization with SAM - As SAM provides an opportunity to segment a region of interest with weak prompts, e.g., points, and bounding boxes, we use it to give an initial mask of the target object. Following SAM, users can get a mask description of the interested object by a click or modify the object mask with several clicks to get a satisfactory initialization.
        2. Tracking with XMem - Given the initialized mask, XMem performs semi-supervised VOS on the following frames. Since XMem is an advanced VOS method that can output satisfactory results on simple scenarios, we output the predicted masks of XMem on most occasions. When the mask quality is not as good, we save the XMem predictions and corresponding intermediate parameters, i.e., probes and affinities, and skip to the next step.
        3. Refinement with SAM - During the inference of VOS models, keep predicting consistent and precise masks are challenging. In fact, most state-of-the-art VOS models tend to segment more and more coarsely over time during inference. Therefore, we utilize SAM to refine the masks predicted by XMem when its quality assessment is not satisfactory. Specifically, we project the probes and affinities to be point prompts for SAM, and the predicted mask from Step 2 is used as a mask prompt for SAM. Then, with these prompts, SAM is able to produce a refined segmentation mask. Such refined masks will also be added to the temporal correspondence of XMem to refine all subsequent object discrimination.
        4. Correction with human participation - After the above three steps, the TAM can now successfully solve some common challenges and predict segmentation masks. However, it is still difficult to accurately distinguish the objects in some extremely challenging scenarios, especially when processing long videos. Therefore, we propose to add human correction during inference, which can bring a qualitative leap in performance with only very small human efforts. In detail, users can compulsively stop the TAM process and correct the mask of the current frame with positive and negative clicks. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/8d5b3478-e24a-4293-95fb-9c6b37b50c68)
        5. TAM can handle multi-object separation, target deformation, scale change, and camera motion well, which demonstrates its superior tracking and segmentation abilities within only click initialization and one-round inference. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/4c07b326-bb83-488f-9a4d-f81e9c8bb95a)
        6. Some failed cases: ![image](https://github.com/agoel11/KEYS2023/assets/81878922/1f2f0236-5846-472a-bdbc-e4a914d97059)
        7. The failed cases typically appear on the following two occasions. Current VOS models are mostly designed for short videos, which focus more on maintaining short-term memory rather than long-term memory. This leads to mask shrinkage or lacking refinement in long-term videos, as shown in seq (a). Essentially, this is solved in step 3 by the refinement ability of SAM, while its effectiveness is lower than
expected in realistic applications. It indicates that the ability of SAM refinement based on multiple prompts can be further improved in the future. On the other hand, human participation/interaction in TAM can be an approach to solving such difficulties, while too much interaction will also result in low efficiency. Thus, the mechanism of long-term memory preserving and transient memory updating is still important.
        8. When the object structure is complex, e.g., the bicycle wheels in seq (b) contain many cavities in groundtruth masks. It is very difficult to get a fine-grained initialized mask by propagating the clicks. Thus, the coarse initialized masks may have side effects on the subsequent frames and lead to poor predictions. This suggests that SAM is still struggling with complex and precision structures.

###XMem:
1. XMem frames Video Object Segmentation (VOS) as a memory problem. Prior works mostly use a single type of feature memory. This can be in the form of network weights (i.e., online learning), last frame segmentation (e.g., MaskTrack), spatial hidden representation (e.g., Conv-RNN-based methods), spatial-attentional features (e.g., STM, STCN, AOT), or some sort of long-term compact features (e.g., AFB-URR).
2. Methods with a short memory span are not robust to changes, while those with a large memory bank are subject to a catastrophic increase in computation and GPU memory usage. Attempts at long-term attentional VOS like AFB-URR compress features eagerly as soon as they are generated, leading to a loss of feature resolution.
3. XMem is inspired by the Atkinson-Shiffrin human memory model, which has a sensory memory, a working memory, and a long-term memory. These memory stores have different temporal scales and complement each other in our memory reading mechanism. It performs well in both short-term and long-term video datasets, handling videos with more than 10,000 frames with ease.
4. ####[XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model](https://arxiv.org/pdf/2207.07115.pdf):
    1. XMem is a video object segmentation architecture for long videos with unified feature memory stores inspired by the Atkinson-Shiffrin memory model. Prior work on video object segmentation typically only uses one type of feature memory. For videos longer than a minute, a single feature memory model tightly links memory consumption and accuracy.
    2. XMem uses an architecture that incorporates multiple independent yet deeply-connected feature memory stores: a rapidly updated sensory memory, a high-resolution working memory, and a compact thus sustained long-term memory. A memory potentiation algorithm that routinely consolidates actively used working memory elements into the long-term memory, which avoids memory explosion and minimizes performance decay for long-term prediction is developed. Combined with a new memory reading mechanism, XMem greatly exceeds state-of-the-art performance on long-video datasets while being on par with state-of-theart methods (that do not work on long videos) on short-video datasets.
    3. XMem relies on semi-supervised VOS by using a first-frame annotation, provided by the user, and segments objects in all other frames as accurately as possible while preferably running in real-time, online, and while having a small memory footprint even when processing long videos.
    4. VOS methods employ a feature memory to store relevant deep-net representations of an object to propogate annotations to other frames. Online learning methods use the weights of a network as their feature memory and recurrent methods propagate information often from the most recent frames, either via a mask or via a hidden representation. But these methods are prone to drifting and struggle with occlusions. Recent VOS methods store representations of past frames in the feature memory with features extracted from the newly observed query frame which needs to be segmented. Although these methods are high-performance, they require a large amount of GPU memory and struggle to handle videos longer  than a minute. Methods designed for longer videos, however, often sacrifice on segmentation quality.
    5. It is proposed that this connection of performance and GPU memory consumption is a direct consequence of using a single feature memory type. Instead a unified memory architecture, dubbed XMem, is proposed. XMem maintains three independent yet deeply-connected feature memory stores: a rapidly updated sensory memory, a high-resolution working memory, and a compact thus sustained long-term memory. The sensory memory corresponds to the hidden representation of a GRU which is updated every frame. It provides temporal smoothness but fails for long-term prediction due to representation drift. To complement, the working memory is agglomerated from a subset of historical frames and considers them equally without drifting over time. To control the size of the working memory, XMem routinely consolidates its representations into the long-term memory, inspired by the consolidation mechanism in the human memory. XMem stores long-term memory as a set of highly compact prototypes. For this, they develop a memory potentiation algorithm that aggregates richer information into these prototypes to prevent aliasing due to sub-sampling. To read from the working and long-term memory, they devise a space-time memory reading operation. The three feature memory stores combined permit handling long videos with high accuracy while keeping GPU memory usage low.![image](https://github.com/agoel11/KEYS2023/assets/81878922/1ec9b7e8-8200-4a2a-852a-bd6a558cae0d)
    6. Given the image and target object mask at the first frame, XMem tracks the object and generates corresponding masks for subsequent query frames. For this, it first initializes the different feature memory stores using the inputs. For each subsequent query frame, it performs memory reading from long-term memory, working memory, and sensory memory respectively. The readout features are used to generate a segmentation mask. Then, it updates each of the feature memory stores at different frequencies. It updates the sensory memory every frame and inserts features into the working memory at every r-th frame. When the working memory reaches a pre-defined maximum of Tmax frames, it consolidates features from the working memory into the long-term memory in a highly compact form. When the long-term memory is also full (which only happens after processing thousands of frames), it discards obsolete features to bound the maximum GPU memory usage. These feature memory stores work in conjunction to provide high-quality features with low GPU memory usage even for very long videos. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/d7b6f519-ce0f-4bcb-9c8e-7beb2dcf6ef3)
    7. XMem consists of three end-to-end trainable convolutional networks as shown in Figure 3: a query encoder that extracts query-specific image features, a decoder that takes the output of the memory reading step to generate an object mask, and a value encoder that combines the image with the object mask to extract new memory features. See Section 3.6 for details of these networks. In the following, we will first describe the memory reading operation before discussing each feature memory store in detail.
    8. The method sometimes fails when the target object moves too quickly or has severe motion blur as even the fastest updating sensory memory cannot catch up.  
    9. Comparisons:  
![image](https://github.com/agoel11/KEYS2023/assets/81878922/107a01c1-cd91-4cac-a5f7-d6eb4b783d43)  
![image](https://github.com/agoel11/KEYS2023/assets/81878922/f1fc5dbd-09c0-478c-8596-7ee4f139e47c)  
![image](https://github.com/agoel11/KEYS2023/assets/81878922/c5794ae8-48da-4eed-8f5a-c06fa5f97603)
    10. Failure Cases: ![image](https://github.com/agoel11/KEYS2023/assets/81878922/58af7de8-c761-409e-976d-5a3705c95823)

###MaskFreeVIS:
1. The recent advancement in Video Instance Segmentation (VIS) has largely been driven by the use of deeper and increasingly data-hungry transformer-based models. However, video masks are tedious and expensive to annotate, limiting the scale and diversity of existing VIS datasets. The solution proposed is MaskFreeVIS, which aims to remove the mask-annotation requirement, achieving highly competitive VIS performance, while only using bounding box annotations for the object state. It leverages the rich temporal mask consistency constraints in videos by introducing the Temporal KNN-patch Loss (TK-Loss), providing strong mask supervision without any labels. TK-Loss finds one-to-many matches across frames, through an efficient patch-matching step followed by a K-nearest neighbor selection. A consistency loss is then enforced on the found matches. The mask-free objective is simple to implement, has no trainable parameters, is computationally efficient, yet outperforms baselines employing, e.g., state-of-the-art optical flow to enforce temporal mask consistency. MaskFreeVIS is validated on the YouTube-VIS 2019/2021, OVIS and BDD100K MOTS benchmarks. The results clearly demonstrate the efficacy of the method by drastically narrowing the gap between fully and weakly-supervised VIS performance.
2. MaskFreeVIS has high-performing video instance segmentation without using any video masks or even image mask labels. Using SwinL and built on Mask2Former, MaskFreeVIS achieved 56.0 AP on YTVIS without using any video masks labels. Using ResNet-101, MaskFreeVIS achieves 49.1 AP without using video masks, and 47.3 AP only using COCO mask initialized model.
3. A new parameter-free Temporal KNN-patch Loss (TK-Loss), which leverages temporal masks consistency using unsupervised one-to-k patch correspondence is what MaskFreeVIS uses. TK-Loss is flexible to intergrated with state-of-the-art transformer-based VIS models, with no trainable parameters.
4. ####[Mask-Free Video Instance Segmentation](https://arxiv.org/pdf/2303.15904.pdf):
![image](https://github.com/agoel11/KEYS2023/assets/81878922/4d06848e-defb-41cf-ade2-5ecce8d517fa)
    1. Video Instance Segmentation (VIS) requires jointly detecting, tracking and segmenting all objects in a video from a given set of categories. State-of-the-art VIS models are trained with complete video annotations from VIS datasets. However, video annotation is costly, in particular regarding objectmask labels. Even coarse polygon-based mask annotation is multiple times slower than annotating video bounding boxes. This makes existing VIS benchmarks difficult to scale, limiting the number of object categories covered, especially for recent transformer based VIS models.
    2. Current box-supervised instance segmentation models are created for images and achieve low accuracy when applied to videos because they do not utilize temporal cues.
    3. Instead, the MaskFreeVIS method is proposed, for high performance VIS without any mask annotations. To leverage temporal mask consistency, the Temporal KNN-patch Loss (TK-Loss) is introduced. To find regions corresponding to the same underlying video object, TK-Loss first builds correspondences across frames by patch-wise matching. For each target patch, only the top K matches in the neighboring frame with high enough matching score are selected. A temporal consistency loss is then applied to all found matches to promote the mask consistency. Specifically, the surrogate objective function not only promotes the one-to-k matched regions to reach the same mask probabilities, but also commits their mask prediction to a confident foreground or background prediction by entropy minimization.
    4. TK-Loss simply replaces the conventional video mask losses in supervising video mask generation. To further enforce temporal consistency through the video clip, TK-Loss is employed in a cyclic manner instead of using dense frame-wise connections. This greatly reduces memory cost with negligible performance drop. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/9c828018-2e73-495c-af10-8cf47769216f)
    5. MaskFreeVIS achieves competitive VIS performance without using any video masks or even image mask labels on all datasets. Validated on various methods and backbones, MaskFreeVIS achieves 91.25% performance of its fully supervised counterparts, even outperforming a few recent fully-supervised methods on the popular YTVIS benchmark.  
![image](https://github.com/agoel11/KEYS2023/assets/81878922/f4397575-b1ec-4391-9a01-f75b07016ad1)    ![image](https://github.com/agoel11/KEYS2023/assets/81878922/0d8cc43a-678f-4189-a759-3ae10c4c7f3c) ![image](https://github.com/agoel11/KEYS2023/assets/81878922/1edd17e0-cdcf-4653-95bf-2241903d6b18)


##Matrix & Comparison
| Method | Use Case | Total Time to Test | Processing Time | Ease of Use | Efficiency of Singular-Human Segmentation | Efficiency of Multi-Human Segmentation | Efficiency of Singular-Animal Segmentation | Efficiency of Multi-Animal Segmentation |
|--------|--------|--------|--------|--------|--------|--------|--------|--------|
| DeepLabCut | Trained Pose Estimation/ Tracking |
| Track-Anything | Semi-supervised Segmentation/ Tracking|
| XMem | Trained Segmentation/ Tracking |
