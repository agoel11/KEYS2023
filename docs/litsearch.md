##Method Search & Software:

###Links:

[DeepLabCut Official](http://www.mackenziemathislab.org/deeplabcut-home)  
[DeepLabCut GitHub](https://github.com/DeepLabCut)

###DeepLabCut:
1. DeepLabCut is a software package designed for 2D and 3D markerless pose estimation based on transfer learning with deep neural networks. DeepLabCut is very accurate and efficient and requires minimal training data as well. The versatility of this framework is demonstrated by tracking various body parts in multiple species across a broad collection of behaviors. The package is open source, fast, robust, and can be used to compute 3D pose estimates or for multi-animals. This package is collaboratively developed by the Mathis Group & Mathis Lab at EPFL (releases prior to 2.1.9 were developed at Harvard University).
2. To use DeepLabCut you can use their own GUI, their Jupyter Notebook, their Google Colab, or your own terminal. They also provide lots of data that helps you demo the package and test installation.![image](https://github.com/agoel11/KEYS2023/assets/81878922/e87628ff-14ee-47bd-8a0d-3c8b2295feef)
3. DeepLabCut has been used for trail tracking, reaching in mice, and various Drosophila behaviours during egg-laying. But this toolbox has a variety of applications and it has already been applied to rats, humans, various fish species, bacteria, leeches, various robots, cheetahs, mouse whiskers and race horses. DeepLabCut utilizes the feature detectors (ResNets + readout layers) of one of the state-of-the-art algorithms for human pose estimation by Insafutdinov et al., called DeeperCut. They have improved the inference speed and provided both additional and novel augmentation methods, added real-time, and multi-animal support and currently provide state-of-the-art performance for animal pose estimation.
4. Because of transfer learning, the package requires little training data for multiple challenging behaviors. The feature detectors are robust to video compression. It allows 3D pose estimation with a single network and camera. It allows 3D pose estimation with a single network trained on data from multiple cameras together with standard triangulation methods. DeepLabCut is embedding in a larger open-source eco-system, providing behavioral tracking for neuroscience, ecology, medical, and technical applications. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/cd61a3ad-5a8f-415c-ab18-ec0fe46b77af)
5. ####[DeepLabCut: Markerless Pose Estimation of User-Defined Body Parts With Deep Learning](https://doi.org/10.1038/s41593-018-0209-y)
    1. New methods for markerless tracking using deep learning and neural networks. Excellent performance, comparable to human accuracy. Minimal training data also yields excellent results across various species and behaviors.
    2. Quantification of behavior essential for understanding brain. Computer vision much easier and more efficient than manual analysis. Markers, used traditionally, invade space and aren't cost effective and are distracting to subjects. Deep learning architecture greatly improve accuracy of pose estimation. Large datasets can be tackled by using transfer learning.
    3. Used feature detection architecture from DeeperCut (best pose estimation algorithms, can acheieve human-level labeling accuracy with minimal data). Result of transfer learning, feature detectors are based on extremely deep neural networks, which were pretrained on ImageNet, a massive dataset for object recognition.
    4. DeeperCut achieves outstanding performance on multi-human pose detection benchmarks, trained on thousands of labeled images. DeepLabCut is DeeperCut but focuses on feature detectors, which are variations of deep residual neural networks (ResNet) with readout layers that predict the location of a body part.
    5. DeepLabCut is a deep convolutional network combining two key ingredients from object recognition and semantic segmentation: pretrained ResNets and deconvolutional layers. The network consists of a variant of ResNets, whose weights were trained on a popular, large-scale object recognition benchmark called ImageNet. Deconvolutional layers are used to up-sample the visual information and produce spatial probability densities. For each body part, its probability density represents the ‘evidence’ that a body part is in a particular location. To fine-tune the network for a particular task, its weights are trained on labeled data, which consist of frames and the accompanying annotated body part locations. The weights are adjusted in an iterative fashion such that for a given frame the network assigns high probabilities to labeled body part locations and low probabilities elsewhere. The network is rewired and ‘learns’ feature detectors for the labeled body parts. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/028d8ebb-13f3-4bd4-8882-37e622e7711c)
    6. Average variability to ground truth was found to be very low and small. To quantidy accuracy, datasets were split and a certain percentage was used to train while the rest was used to test. When trained with 80% of
the data the algorithm achieved human-level accuracy. After varying training and testing percentages, it was found that even 100 frames were enough to achieve excellent generalization. Data augmentation (such as rotations or translations) also resulted in minimal differences, demonstrating the data-efficiency of DeepLabCut. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/6bf3d011-6145-458d-9e50-d91172aac2c4)
    7. The feature detectors were able to translate pretty well to novel mouse behaviors as well as videos with multiple mice. Although it wasn't error free, this can be improved by simply training on that data or on data for multiple mice. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/b9e8c1d2-aebe-410d-be88-d239c2f84903)
    8. End-to-end training allows the model to facilitate the localization of one body part based on other labeled body parts. The network that was trained with all body part labels simultaneously outperforms the specialized networks nearly twofold. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/88060d7d-89ba-44d2-8b36-90f7c40824f5)
    9. Temporal information could indeed be beneficial in certain contexts, challenges remain to using end-to-end-trained deep architectures for video data to extract postures. Because of the curse of dimensionality, deep architectures on videos must rely on input images with lower spatial resolution, and thus the best-performing action recognition algorithms still rely on frame-by-frame analysis with deep networks pretrained on ImageNet as a result of hardware limitations. Therefore currently, in situations where occlusions are very common, such as in social behaviors, pairwise interactions could also be added to improve performance.
6. ####[Using DeepLabCut for 3D Markerless Pose Estimation Across Species and Behaviors](https://doi.org/10.1101/476531)
    1. Transfer learning, the ability to take a network, which was trained on a task with a large supervised data set, and utilize it for another task with a small supervised data set, is beginning to allow users to broadly apply deep learning methods. DeepLabCut provides tools to create annotated training sets, train robust feature detectors, and utilize them to analyze novel behavioral videos.
    2. The major motivation for developing the DeepLabCut toolbox was to provide a reliable and efficient tool for high-throughput video analysis, where powerful feature detectors of user-defined body parts need to be learned for a specific situation. The toolbox is aimed to solve the problem of detecting body parts in dynamic visual environments where varying background, reflective walls or motion blur hinder the performance of common techniques, such as thresholding or regression based on visual features. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/4c483668-9292-4897-972a-1c9feebcf69f)
    3. The user starts by creating a new project based on a project and username as well as some (initial) videos, which are required to create the training dataset (additional videos can also be added after the creation of the project). Next, DeepLabCut extracts frames, which reflect the diversity of the behavior with respect to postures, animal identities, etc. Then the user can label the points of interest in the extracted frames. These
annotated frames can be visually checked for accuracy, and corrected if necessary. Eventually, a training dataset is created by merging all the extracted labeled frames and splitting them into subsets of test and train frames. Then, a pre-trained network (ResNet) is trained end-to-end to adapt its weights in order to predict the desired features. The performance of the trained network can then be evaluated on the training and test frames. The trained network can be used to analyze videos yielding extracted pose files.  In case the trained network does not generalize well to unseen data in the evaluation and analysis step, then additional frames with poor results can be extracted and the predicted labels can be manually shifted to their ideal location. This refinement step, if needed, creates an additional set of annotated images that can then be merged with the original training dataset to create a new training dataset. This larger training set can then be used to re-train the feature detectors for better results. This active learning loop can be done iteratively to robustly and accurately analyze videos with potentially large variability- i.e. experiments that include many individuals, and run over long time periods. Furthermore, the user can add additional body parts/labels at later stages during a project as well as correct user-defined labels. ![image](https://github.com/agoel11/KEYS2023/assets/81878922/d3a53dad-bbb0-47d8-b8ff-b2d939e9f728)
    4. However, if a user aims to track (adult) human poses, many excellent options exist, including DeeperCut, ArtTrack, DeepPose, OpenPose, and OpenPose-Plus (better for humans).
    5. DeepLabCut does not support occlusions, requires HPC and GPUs, and also workers faster with smaller images.
