# Logbook


####***Day 1 (4/10)***####: Met with Dr. Merchant and discussed current standing and overall goals for the summer. Discussed possible interests and options and what can be started right now. Dr. Merchant talked about write-ups and possible publication, and recommended exploring Docker. Discussed previous projects as well and considered possible projects and pathways.


***Day 2 (6/7)***: Second meeting with Dr. Merchant. Project was communicated over email along with lab agreement form. Possible project options were also discusses over email previously. Asked questions and clarified the project with Dr. Merchant as well as what could be done starting now. Learned about weekly meetings and standups as well as daily check-ins and check-outs and logs. Periodic demos were also mentioned. Dr. Merchant recommended learning UNIX and getting familiar with Linux Cl, Python, Jupyter Notebooks, Docker, and create a Git website. Prior research for video segmentation was also discussed for an understanding before the project.

***Day 3 (6/8)***: Created and worked on Git website with some struggle on the Actions tab and getting it to auto-update and re-build the website. Also worked on UNIX and Git Bash commands. Will continue to go deeper into that tomorrow.

***Day 4 (6/9)***: Finished up UNIX learning and testing as well as notes and documentation of learning on website. Learned version control with Git and took extensive notes on website, tried commands and created sample repository as well. Installed and started looking into Docker as well as Singularity and reading up on both softwares and learning the concept of containers. Concept of containerization and virtualization is still confusing and difference between VMs is unclear as well.

***Day 5 (6/12)***: Solidified basic understanding of containers and containerization including use cases, advantages, differences from VMs, container clustering, and container images. Used a variety of sources and recorded notes and important information on website. Understood basic inner-workings and uses of Docker. Explored some commands and different aspects of Docker such as client, daemon, API, registry, and object. Took notes and recorded important and useful information on website. Recapped deep learning and deep learning concepts as well as benefits and use cases. Recorded important information on website and prepared for prior fact finding for actual project content (video segmentation).

***Day 6 (6/13)***: Continued work on understanding video segmentation and the methods and models associated with it. Did not go into any specific research yet, but started gaining basic overview. Learned about VOS and VSS as well as the many different networks used in both methods. Finished learning and writing about VOS and main methods used in VOS. Started learning and working on VSS and got through one main method. Recorded learning on website with extensive notes as well as diagrams and images. Some technical concepts and technical words are still unclear, but are not needed for a basic understanding of methods. Can go into detail of the methods in the future if needed. Starting point and foundation has been established.

***Day 7 (6/14)***: Continued work on understanding video segmentation and the methods and models associated with it. Finished learning about VSS methods and recorded findings and learnings on website. Also worked through challenges associated with video segmentation models. Started looking into video segmentation models/ packages such as DeepLabCut. Also started looking into literature search for such packages and software as well as reading through current papers on the topic. Got basic understanding of what DeepLabCut is, will continue understanding how it works and testing it as well.

***Day 8 (6/15)***: Continued reading up on DeepLabCut and continued literature search. Started finding some papers and storing them for future study and possible use of their methods. Also researched what pipelines have been created in the past, if any, and what their use cases are. Also looked into point tracking and different methods for that. Started gathering questions for meeting with Dr. Merchant on Friday.

***Day 9 (6/16)***: Continued literature search and reading up on current research and methods for video segmentation. Met with Dr. Merchant early in the day to discuss progress so far as well as upcoming timeline. Got many questions answered on actual tasks and use cases as well as clarified projection of project and tasks associated with it. Also created Trello board for timeline and planning of upcoming days and progress. Started getting research question down and moving forward with information on website as well.

***Day 10 (6/20)***: Had to go back to DeepLabCut after conversation with Dr. Merchant. Read into it more and analyzed use cases as well as different research papers on the package. Understood how video segmentation (and software packages in general) work and how they can be used. Updated logbook with information and website with findings on DeepLabCut. 

***Day 11 (6/21)***: Finished reading both papers on DeepLabCut and gained working understanding of the package. Recorded everything on website along with visual models of how DeepLabCut works and what it does. Started searching for second and third method in order to research and learn more about. Found the package Track-Anything and started looking into it and seeing if it meets the needs of the project.

***Day 12 (6/22)***: Read all about Track-Anything and learned how it works and what it is. Learned about segment anything models and how they differ from traditional video segmentation models like DeepLabCut. Also learned how models can be compounded and built off each other. For example, Track-Anything is based off SAM (Segment Anything Model) and XMem and therefore creates its own new output. Read the paper on Track-Anything and recorded findings on website. Also updated Trello board and started planning for next week.

***Day 13 (6/23)***: Research the third package that could be used and evaluated against Track-Anything and DeepLabCut. Found a package called PaddleSeg and initially started reading through that, but realised it was only meant for image segmentation not video segmentation. Found another package called XMem (same one Track-Anything uses and is built off) and started researching and learning about it. Also asked Dr. Merchant some clarification questions and plans for next week after initial research has been done. Updated Trello board with plans for next week and shared link with Dr. Merchant. Finished reading and learning about XMem and its innovative memory allocation method and recorded all learnings on website.

***Day 14 (6/26)***: Researched another package after starting to build the amatrix and realising that another method would be helpful. Found MaskFreeVIS which is unsupervised video segmentation. Researched MaskFreeVIS and read about how it works using TK-Loss and recorded learnings as well as findings on website. Also created criteria for matrix in order to find best method to use. Created table with multiple criteria and also found videos to use for testing all 4 methods. Found videos of single and multiple humans dancing and single and multiple dogs as well. Also started playing with DeepLabCut demo in Jupyter Notebook.

***Day 15 (6/27)***: Started testing for all the criteria in the matrix. Tested DeepLabCut with all four videos and recorded all results in matrix as accurately as possible. Also repeated testing when unsure of something. Also tested Track-Anything and recorded all findings in matrix. Again, re-tested when in doubt of a recorded metric. So far, Track-Anything greatly outperformed DeepLabCut even though they have different ways of tracking (DeepLabCut has pose estimation, Track-Anything has traditional segmentation). Used DeepLabCut Model Zoo Jupyter Notebook for testing. Used HuggingFace demo of Track-Anything for testing. Also uploaded all 4 videos to website as well as segmented results of each video (segmented video from each method i.e. segmented dancing video that Track-Anything produced).

***Day 16 (6/28)***: Started testing XMem package on Google Colab demo. Tested with the singular human dancing video first, but had to create a mask for XMem to work. Created mask using image segmentation software on Hugging Face and then through image editing in order to get the background black and the objects that needed to be detected in one solid color. Once the mask was created, plugged it into the Colab file along with the video but it did not work. Tried again and again by refining the mask and trying to make it in different ways and different colors and trying different image segmentation methods. XMem software still did not work by end of day. Decided to move on and come back later.

***Day 17 (6/29)***: Tried to test MaskFreeVIS software today. Looked all through the GitHub repo but did not find any instructions on what to do and how to do it. Looked through all three different repos (PyTorch, Tensorflow, and Cuda) but to no avail. No instructions or references for testing were to be found. Looked through different websites and publications and the official MaskFreeVIS website as well. Also looked at each folder to try and see .txt files or if there are instructions in code files. Finally tried cloning the repo into Google Colab and trying something that way, but there were no instructions and ways to execute the code. Turned out to be a dead-end.

***Day 18 (6/30)***: Re-tried both methods after countless failures the past two days. Starting to get very frustrated now. Retried XMem with different videos and different masks. Tried the single dog video and mask with different colors for the dog and different refined and unrefined masks, but still did not work. Even tried to remove an object from the example mask and try it, but still did not work. Tried a simple rectangle and oval on the mask and even no object on the mask (just a black image) but still did not get any results. Tried to find some sort of instruction or reference for MaskFreeVIS as well, looking through different publications as well as searching the internet, but found nothing. Had a meeting with Dr. Merchant and discussed progress so far as well as future steps. Next steps are to test the current methods with Brian Carter's data as well as some mice data and see how well they work and then report results. Dr. Merchant instructed to not waste time on XMem and MaskFreeVIS anymore since they did not work, instead add YOLO to the matrix next week. Also discussed opportunity after KEYS.
