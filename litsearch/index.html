
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="KEYS internship 2023">
      
      
        <link rel="canonical" href="https://agoel11.github.io/KEYS2023/litsearch/">
      
      
        <meta name="author" content="Atharva Goel">
      
      <link rel="shortcut icon" href="../assets/logo.jpg">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-6.1.0">
    
    
      
        <title>Literature Search - KEYS Internship 2023</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.bc7e593a.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.ab28b872.min.css">
        
          
          
          <meta name="theme-color" content="#000000">
        
      
    
    
    
      
        
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans:300,400,400i,700%7CRegular&display=fallback">
        <style>body,input{font-family:"Google Sans",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Regular",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="red">
      
  
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#method-search-software" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href="https://agoel11.github.io/KEYS2023/" title="KEYS Internship 2023" class="md-header-nav__button md-logo" aria-label="KEYS Internship 2023">
      
  <img src="../assets/logo.jpg" alt="logo">

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            KEYS Internship 2023
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              Literature Search
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/agoel11/KEYS2023/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    KEYS2023
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="https://agoel11.github.io/KEYS2023/" title="KEYS Internship 2023" class="md-nav__button md-logo" aria-label="KEYS Internship 2023">
      
  <img src="../assets/logo.jpg" alt="logo">

    </a>
    KEYS Internship 2023
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/agoel11/KEYS2023/" title="Go to repository" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    KEYS2023
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." class="md-nav__link">
      Introduction
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../logbook/" class="md-nav__link">
      Logbook
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../keysassignments/" class="md-nav__link">
      Assignments
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../jupyter/" class="md-nav__link">
      Jupyter Notebooks
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      GitHub Actions
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="GitHub Actions" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        <span class="md-nav__icon md-icon"></span>
        GitHub Actions
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../githubed/" class="md-nav__link">
      GitHub Education Access
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-6" type="checkbox" id="nav-6">
    
    <label class="md-nav__link" for="nav-6">
      Project Notes
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Project Notes" data-md-level="1">
      <label class="md-nav__title" for="nav-6">
        <span class="md-nav__icon md-icon"></span>
        Project Notes
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../unixshell/" class="md-nav__link">
      UNIX Shell
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../vcwgit/" class="md-nav__link">
      Version Control with Git
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../containers/" class="md-nav__link">
      Containers
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-7" type="checkbox" id="nav-7" checked>
    
    <label class="md-nav__link" for="nav-7">
      KEYS 2023 Main Project
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="KEYS 2023 Main Project" data-md-level="1">
      <label class="md-nav__title" for="nav-7">
        <span class="md-nav__icon md-icon"></span>
        KEYS 2023 Main Project
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../myproject/" class="md-nav__link">
      My Project
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../priorresearch/" class="md-nav__link">
      Prior Research/ Fact Finding
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Literature Search
        <span class="md-nav__icon md-icon"></span>
      </label>
    
    <a href="./" class="md-nav__link md-nav__link--active">
      Literature Search
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#method-search-software" class="md-nav__link">
    Method Search &amp; Software:
  </a>
  
    <nav class="md-nav" aria-label="Method Search &amp; Software:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#links" class="md-nav__link">
    Links:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplabcut" class="md-nav__link">
    DeepLabCut:
  </a>
  
    <nav class="md-nav" aria-label="DeepLabCut:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deeplabcut-markerless-pose-estimation-of-user-defined-body-parts-with-deep-learning" class="md-nav__link">
    DeepLabCut: Markerless Pose Estimation of User-Defined Body Parts With Deep Learning:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-deeplabcut-for-3d-markerless-pose-estimation-across-species-and-behaviors" class="md-nav__link">
    Using DeepLabCut for 3D Markerless Pose Estimation Across Species and Behaviors:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#track-anything" class="md-nav__link">
    Track-Anything:
  </a>
  
    <nav class="md-nav" aria-label="Track-Anything:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#track-anything-segment-anything-meets-videos" class="md-nav__link">
    Track Anything: Segment Anything Meets Videos:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#xmem" class="md-nav__link">
    XMem:
  </a>
  
    <nav class="md-nav" aria-label="XMem:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#xmem-long-term-video-object-segmentation-with-an-atkinson-shiffrin-memory-model" class="md-nav__link">
    XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maskfreevis" class="md-nav__link">
    MaskFreeVIS:
  </a>
  
    <nav class="md-nav" aria-label="MaskFreeVIS:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mask-free-video-instance-segmentation" class="md-nav__link">
    Mask-Free Video Instance Segmentation:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#yolo8" class="md-nav__link">
    YOLO8:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#matrix-comparison" class="md-nav__link">
    Matrix &amp; Comparison
  </a>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-8" type="checkbox" id="nav-8">
    
    <label class="md-nav__link" for="nav-8">
      Results
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Results" data-md-level="1">
      <label class="md-nav__title" for="nav-8">
        <span class="md-nav__icon md-icon"></span>
        Results
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../poster/" class="md-nav__link">
      Poster
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../references/" class="md-nav__link">
      References
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#method-search-software" class="md-nav__link">
    Method Search &amp; Software:
  </a>
  
    <nav class="md-nav" aria-label="Method Search &amp; Software:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#links" class="md-nav__link">
    Links:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deeplabcut" class="md-nav__link">
    DeepLabCut:
  </a>
  
    <nav class="md-nav" aria-label="DeepLabCut:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deeplabcut-markerless-pose-estimation-of-user-defined-body-parts-with-deep-learning" class="md-nav__link">
    DeepLabCut: Markerless Pose Estimation of User-Defined Body Parts With Deep Learning:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-deeplabcut-for-3d-markerless-pose-estimation-across-species-and-behaviors" class="md-nav__link">
    Using DeepLabCut for 3D Markerless Pose Estimation Across Species and Behaviors:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#track-anything" class="md-nav__link">
    Track-Anything:
  </a>
  
    <nav class="md-nav" aria-label="Track-Anything:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#track-anything-segment-anything-meets-videos" class="md-nav__link">
    Track Anything: Segment Anything Meets Videos:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#xmem" class="md-nav__link">
    XMem:
  </a>
  
    <nav class="md-nav" aria-label="XMem:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#xmem-long-term-video-object-segmentation-with-an-atkinson-shiffrin-memory-model" class="md-nav__link">
    XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maskfreevis" class="md-nav__link">
    MaskFreeVIS:
  </a>
  
    <nav class="md-nav" aria-label="MaskFreeVIS:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mask-free-video-instance-segmentation" class="md-nav__link">
    Mask-Free Video Instance Segmentation:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#yolo8" class="md-nav__link">
    YOLO8:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#matrix-comparison" class="md-nav__link">
    Matrix &amp; Comparison
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/agoel11/KEYS2023/edit/main/docs/litsearch.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                  <h1>Literature Search</h1>
                
                <h2 id="method-search-software">Method Search &amp; Software:<a class="headerlink" href="#method-search-software" title="Permanent link">&para;</a></h2>
<h3 id="links">Links:<a class="headerlink" href="#links" title="Permanent link">&para;</a></h3>
<p><a href="http://www.mackenziemathislab.org/deeplabcut-home">DeepLabCut Official</a><br />
<a href="https://github.com/DeepLabCut">DeepLabCut GitHub</a><br />
<a href="https://github.com/gaomingqi/Track-Anything/blob/master/README.md">Track-Anything GitHub</a><br />
<a href="https://github.com/hkchengrex/XMem">XMem GitHub</a><br />
<a href="https://www.vis.xyz/pub/maskfreevis/">MaskFreeVIS Official</a><br />
<a href="https://github.com/SysCV/MaskFreeVIS">MaskFreeVIS GitHub</a><br />
<a href="https://docs.ultralytics.com/">YOLO8 Official</a>  </p>
<h3 id="deeplabcut">DeepLabCut:<a class="headerlink" href="#deeplabcut" title="Permanent link">&para;</a></h3>
<ol>
<li>DeepLabCut is a software package designed for 2D and 3D markerless pose estimation based on transfer learning with deep neural networks. DeepLabCut is very accurate and efficient and requires minimal training data as well. The versatility of this framework is demonstrated by tracking various body parts in multiple species across a broad collection of behaviors. The package is open source, fast, robust, and can be used to compute 3D pose estimates or for multi-animals. This package is collaboratively developed by the Mathis Group &amp; Mathis Lab at EPFL (releases prior to 2.1.9 were developed at Harvard University).</li>
<li>To use DeepLabCut you can use their own GUI, their Jupyter Notebook, their Google Colab, or your own terminal. They also provide lots of data that helps you demo the package and test installation.<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/e87628ff-14ee-47bd-8a0d-3c8b2295feef" /></li>
<li>DeepLabCut has been used for trail tracking, reaching in mice, and various Drosophila behaviours during egg-laying. But this toolbox has a variety of applications and it has already been applied to rats, humans, various fish species, bacteria, leeches, various robots, cheetahs, mouse whiskers and race horses. DeepLabCut utilizes the feature detectors (ResNets + readout layers) of one of the state-of-the-art algorithms for human pose estimation by Insafutdinov et al., called DeeperCut. They have improved the inference speed and provided both additional and novel augmentation methods, added real-time, and multi-animal support and currently provide state-of-the-art performance for animal pose estimation.</li>
<li>Because of transfer learning, the package requires little training data for multiple challenging behaviors. The feature detectors are robust to video compression. It allows 3D pose estimation with a single network and camera. It allows 3D pose estimation with a single network trained on data from multiple cameras together with standard triangulation methods. DeepLabCut is embedding in a larger open-source eco-system, providing behavioral tracking for neuroscience, ecology, medical, and technical applications. <img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/cd61a3ad-5a8f-415c-ab18-ec0fe46b77af" /></li>
<li>
<h4 id="deeplabcut-markerless-pose-estimation-of-user-defined-body-parts-with-deep-learning"><a href="https://doi.org/10.1038/s41593-018-0209-y">DeepLabCut: Markerless Pose Estimation of User-Defined Body Parts With Deep Learning</a>:<a class="headerlink" href="#deeplabcut-markerless-pose-estimation-of-user-defined-body-parts-with-deep-learning" title="Permanent link">&para;</a></h4>
<ol>
<li>New methods for markerless tracking using deep learning and neural networks. Excellent performance, comparable to human accuracy. Minimal training data also yields excellent results across various species and behaviors.</li>
<li>Quantification of behavior essential for understanding brain. Computer vision much easier and more efficient than manual analysis. Markers, used traditionally, invade space and aren't cost effective and are distracting to subjects. Deep learning architecture greatly improve accuracy of pose estimation. Large datasets can be tackled by using transfer learning.</li>
<li>Used feature detection architecture from DeeperCut (best pose estimation algorithms, can acheieve human-level labeling accuracy with minimal data). Result of transfer learning, feature detectors are based on extremely deep neural networks, which were pretrained on ImageNet, a massive dataset for object recognition.</li>
<li>DeeperCut achieves outstanding performance on multi-human pose detection benchmarks, trained on thousands of labeled images. DeepLabCut is DeeperCut but focuses on feature detectors, which are variations of deep residual neural networks (ResNet) with readout layers that predict the location of a body part.</li>
<li>DeepLabCut is a deep convolutional network combining two key ingredients from object recognition and semantic segmentation: pretrained ResNets and deconvolutional layers. The network consists of a variant of ResNets, whose weights were trained on a popular, large-scale object recognition benchmark called ImageNet. Deconvolutional layers are used to up-sample the visual information and produce spatial probability densities. For each body part, its probability density represents the ‘evidence’ that a body part is in a particular location. To fine-tune the network for a particular task, its weights are trained on labeled data, which consist of frames and the accompanying annotated body part locations. The weights are adjusted in an iterative fashion such that for a given frame the network assigns high probabilities to labeled body part locations and low probabilities elsewhere. The network is rewired and ‘learns’ feature detectors for the labeled body parts. <img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/028d8ebb-13f3-4bd4-8882-37e622e7711c" /></li>
<li>Average variability to ground truth was found to be very low and small. To quantidy accuracy, datasets were split and a certain percentage was used to train while the rest was used to test. When trained with 80% of
the data the algorithm achieved human-level accuracy. After varying training and testing percentages, it was found that even 100 frames were enough to achieve excellent generalization. Data augmentation (such as rotations or translations) also resulted in minimal differences, demonstrating the data-efficiency of DeepLabCut. <img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/6bf3d011-6145-458d-9e50-d91172aac2c4" /></li>
<li>The feature detectors were able to translate pretty well to novel mouse behaviors as well as videos with multiple mice. Although it wasn't error free, this can be improved by simply training on that data or on data for multiple mice. <img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/b9e8c1d2-aebe-410d-be88-d239c2f84903" /></li>
<li>End-to-end training allows the model to facilitate the localization of one body part based on other labeled body parts. The network that was trained with all body part labels simultaneously outperforms the specialized networks nearly twofold. <img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/88060d7d-89ba-44d2-8b36-90f7c40824f5" /></li>
<li>Temporal information could indeed be beneficial in certain contexts, challenges remain to using end-to-end-trained deep architectures for video data to extract postures. Because of the curse of dimensionality, deep architectures on videos must rely on input images with lower spatial resolution, and thus the best-performing action recognition algorithms still rely on frame-by-frame analysis with deep networks pretrained on ImageNet as a result of hardware limitations. Therefore currently, in situations where occlusions are very common, such as in social behaviors, pairwise interactions could also be added to improve performance.</li>
</ol>
</li>
<li>
<h4 id="using-deeplabcut-for-3d-markerless-pose-estimation-across-species-and-behaviors"><a href="https://doi.org/10.1101/476531">Using DeepLabCut for 3D Markerless Pose Estimation Across Species and Behaviors</a>:<a class="headerlink" href="#using-deeplabcut-for-3d-markerless-pose-estimation-across-species-and-behaviors" title="Permanent link">&para;</a></h4>
<ol>
<li>Transfer learning, the ability to take a network, which was trained on a task with a large supervised data set, and utilize it for another task with a small supervised data set, is beginning to allow users to broadly apply deep learning methods. DeepLabCut provides tools to create annotated training sets, train robust feature detectors, and utilize them to analyze novel behavioral videos.</li>
<li>The major motivation for developing the DeepLabCut toolbox was to provide a reliable and efficient tool for high-throughput video analysis, where powerful feature detectors of user-defined body parts need to be learned for a specific situation. The toolbox is aimed to solve the problem of detecting body parts in dynamic visual environments where varying background, reflective walls or motion blur hinder the performance of common techniques, such as thresholding or regression based on visual features. <img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/4c483668-9292-4897-972a-1c9feebcf69f" /></li>
<li>The user starts by creating a new project based on a project and username as well as some (initial) videos, which are required to create the training dataset (additional videos can also be added after the creation of the project). Next, DeepLabCut extracts frames, which reflect the diversity of the behavior with respect to postures, animal identities, etc. Then the user can label the points of interest in the extracted frames. These
annotated frames can be visually checked for accuracy, and corrected if necessary. Eventually, a training dataset is created by merging all the extracted labeled frames and splitting them into subsets of test and train frames. Then, a pre-trained network (ResNet) is trained end-to-end to adapt its weights in order to predict the desired features. The performance of the trained network can then be evaluated on the training and test frames. The trained network can be used to analyze videos yielding extracted pose files.  In case the trained network does not generalize well to unseen data in the evaluation and analysis step, then additional frames with poor results can be extracted and the predicted labels can be manually shifted to their ideal location. This refinement step, if needed, creates an additional set of annotated images that can then be merged with the original training dataset to create a new training dataset. This larger training set can then be used to re-train the feature detectors for better results. This active learning loop can be done iteratively to robustly and accurately analyze videos with potentially large variability- i.e. experiments that include many individuals, and run over long time periods. Furthermore, the user can add additional body parts/labels at later stages during a project as well as correct user-defined labels. <img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/d3a53dad-bbb0-47d8-b8ff-b2d939e9f728" /></li>
<li>However, if a user aims to track (adult) human poses, many excellent options exist, including DeeperCut, ArtTrack, DeepPose, OpenPose, and OpenPose-Plus (better for humans).</li>
<li>DeepLabCut does not support occlusions, requires HPC and GPUs, and also workers faster with smaller images.</li>
</ol>
</li>
</ol>
<h3 id="track-anything">Track-Anything:<a class="headerlink" href="#track-anything" title="Permanent link">&para;</a></h3>
<ol>
<li>Track-Anything is a flexible and interactive tool for video object tracking and segmentation. It is developed upon Segment Anything, can specify anything to track and segment via user clicks only. During tracking, users can flexibly change the objects they wanna track or correct the region of interest if there are any ambiguities.</li>
<li>Track-Anything is suitable for video object tracking and segmentation with shot changes, visualized development and data annotation for video object tracking and segmentation, object-centric downstream video tasks (such as video inpainting and editing).</li>
<li>
<h4 id="track-anything-segment-anything-meets-videos"><a href="https://arxiv.org/pdf/2304.11968.pdf">Track Anything: Segment Anything Meets Videos</a>:<a class="headerlink" href="#track-anything-segment-anything-meets-videos" title="Permanent link">&para;</a></h4>
<ol>
<li>Segment Anything Model (SAM) displays impressive performance on images, but it performs poorly on video segmentation. The proposed Track Anything Model (TAM), achieves high-performance interactive tracking and segmentation in videos.</li>
<li>Current state-of-the-art video trackers/segmenters are trained on large-scale manually-annotated datasets and initialized by a bounding box or a segmentation mask. Moreover, current initialization settings, especially the semi-supervised VOS, need specific object mask groundtruth for model initialization. Large amouns of human labor is also required for huge amounts of annotated and labeled data.</li>
<li>Recently, Segment-Anything Model (SAM) has been proposed, which is a large foundation model for image segmentation. It supports flexible prompts and computes masks in real-time, thus allowing interactive use. Trained on 11 million images and 1.1 billion masks, SAM can produce high-quality masks and do zero-shot segmentation in generic scenarios. With input user-friendly prompts of points, boxes, or language, SAM can give satisfactory segmentation masks on specific image areas. However, using SAM in videos directly does not give an impressive performance due to its deficiency in temporal correspondence. But tracking or segmenting in videos faces challenges from scale variation, target deformation, motion blur, camera motion, similar objects, and so on.</li>
<li>In this paper, the Track-Anything toolkit is proposed for high-performance object tracking and segmentation in videos. With a user-friendly interface, the Track Anything Model (TAM) can track and segment any objects in a given video with only one-pass inference. TAM combines SAM, a large segmentation model, and XMem, an advanced VOS model. Firstly, users can interactively initialize the SAM, i.e., clicking on the object, to define a target object; then, XMem is used to give a mask prediction of the object in the next frame according to both temporal and spatial correspondence; next, SAM is utilized to give a more precise mask description; during the tracking process, users can pause and correct as soon as they notice tracking failures.</li>
<li>TAM is able to promote the SAM applications to the video level to achieve interactive video object tracking and segmentation. Rather than separately using SAM per frame, it integrates SAM into the process of temporal correspondence construction. The Track Anything task aims for flexible object tracking in arbitrary videos. The target objects can be flexibly selected, added, or removed in any way according to the users’ interests. Also, the video length and types can be arbitrary rather than limited to trimmed or natural videos. With such settings, diverse downstream tasks can be achieved, including single/multiple object tracking, short-/long-term object tracking, unsupervised VOS, semi-supervised VOS, referring VOS, interactive VOS, long-term VOS, and more. <img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/88c0ad4d-b6e1-4afd-b6f3-cfd964cdd4d7" /></li>
<li>As a foundation model for image segmentation (and the genesis for TAM), SAM is based on ViT and trained on the large-scale dataset SA-1B. Obviously, SAM shows promising segmentation ability on images, especially on zero-shot segmentation tasks. Unfortunately, SAM only shows superior performance on image segmentation, while it cannot deal with complex video segmentation.</li>
<li>Given the mask description of the target object at the first frame, XMem can track the object and generate corresponding masks in the subsequent frames. Inspired by the Atkinson-Shiffrin memory model, it aims to solve the difficulties in long-term videos with unified feature memory stores. The drawbacks of XMem are also obvious: as a semi-supervised VOS model, it requires a precise mask to initialize; for long videos, it is difficult for XMem to recover from tracking or segmentation failure. In this paper, we solve both difficulties by importing interactive tracking with SAM.</li>
<li>The Track-Anything process is divided into the following processes:<ol>
<li>Initialization with SAM - As SAM provides an opportunity to segment a region of interest with weak prompts, e.g., points, and bounding boxes, we use it to give an initial mask of the target object. Following SAM, users can get a mask description of the interested object by a click or modify the object mask with several clicks to get a satisfactory initialization.</li>
<li>Tracking with XMem - Given the initialized mask, XMem performs semi-supervised VOS on the following frames. Since XMem is an advanced VOS method that can output satisfactory results on simple scenarios, we output the predicted masks of XMem on most occasions. When the mask quality is not as good, we save the XMem predictions and corresponding intermediate parameters, i.e., probes and affinities, and skip to the next step.</li>
<li>Refinement with SAM - During the inference of VOS models, keep predicting consistent and precise masks are challenging. In fact, most state-of-the-art VOS models tend to segment more and more coarsely over time during inference. Therefore, we utilize SAM to refine the masks predicted by XMem when its quality assessment is not satisfactory. Specifically, we project the probes and affinities to be point prompts for SAM, and the predicted mask from Step 2 is used as a mask prompt for SAM. Then, with these prompts, SAM is able to produce a refined segmentation mask. Such refined masks will also be added to the temporal correspondence of XMem to refine all subsequent object discrimination.</li>
<li>Correction with human participation - After the above three steps, the TAM can now successfully solve some common challenges and predict segmentation masks. However, it is still difficult to accurately distinguish the objects in some extremely challenging scenarios, especially when processing long videos. Therefore, we propose to add human correction during inference, which can bring a qualitative leap in performance with only very small human efforts. In detail, users can compulsively stop the TAM process and correct the mask of the current frame with positive and negative clicks. <img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/8d5b3478-e24a-4293-95fb-9c6b37b50c68" /></li>
<li>TAM can handle multi-object separation, target deformation, scale change, and camera motion well, which demonstrates its superior tracking and segmentation abilities within only click initialization and one-round inference. <img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/4c07b326-bb83-488f-9a4d-f81e9c8bb95a" /></li>
<li>Some failed cases: <img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/1f2f0236-5846-472a-bdbc-e4a914d97059" /></li>
<li>The failed cases typically appear on the following two occasions. Current VOS models are mostly designed for short videos, which focus more on maintaining short-term memory rather than long-term memory. This leads to mask shrinkage or lacking refinement in long-term videos, as shown in seq (a). Essentially, this is solved in step 3 by the refinement ability of SAM, while its effectiveness is lower than
expected in realistic applications. It indicates that the ability of SAM refinement based on multiple prompts can be further improved in the future. On the other hand, human participation/interaction in TAM can be an approach to solving such difficulties, while too much interaction will also result in low efficiency. Thus, the mechanism of long-term memory preserving and transient memory updating is still important.</li>
<li>When the object structure is complex, e.g., the bicycle wheels in seq (b) contain many cavities in groundtruth masks. It is very difficult to get a fine-grained initialized mask by propagating the clicks. Thus, the coarse initialized masks may have side effects on the subsequent frames and lead to poor predictions. This suggests that SAM is still struggling with complex and precision structures.</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="xmem">XMem:<a class="headerlink" href="#xmem" title="Permanent link">&para;</a></h3>
<ol>
<li>XMem frames Video Object Segmentation (VOS) as a memory problem. Prior works mostly use a single type of feature memory. This can be in the form of network weights (i.e., online learning), last frame segmentation (e.g., MaskTrack), spatial hidden representation (e.g., Conv-RNN-based methods), spatial-attentional features (e.g., STM, STCN, AOT), or some sort of long-term compact features (e.g., AFB-URR).</li>
<li>Methods with a short memory span are not robust to changes, while those with a large memory bank are subject to a catastrophic increase in computation and GPU memory usage. Attempts at long-term attentional VOS like AFB-URR compress features eagerly as soon as they are generated, leading to a loss of feature resolution.</li>
<li>XMem is inspired by the Atkinson-Shiffrin human memory model, which has a sensory memory, a working memory, and a long-term memory. These memory stores have different temporal scales and complement each other in our memory reading mechanism. It performs well in both short-term and long-term video datasets, handling videos with more than 10,000 frames with ease.</li>
<li>
<h4 id="xmem-long-term-video-object-segmentation-with-an-atkinson-shiffrin-memory-model"><a href="https://arxiv.org/pdf/2207.07115.pdf">XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model</a>:<a class="headerlink" href="#xmem-long-term-video-object-segmentation-with-an-atkinson-shiffrin-memory-model" title="Permanent link">&para;</a></h4>
<ol>
<li>XMem is a video object segmentation architecture for long videos with unified feature memory stores inspired by the Atkinson-Shiffrin memory model. Prior work on video object segmentation typically only uses one type of feature memory. For videos longer than a minute, a single feature memory model tightly links memory consumption and accuracy.</li>
<li>XMem uses an architecture that incorporates multiple independent yet deeply-connected feature memory stores: a rapidly updated sensory memory, a high-resolution working memory, and a compact thus sustained long-term memory. A memory potentiation algorithm that routinely consolidates actively used working memory elements into the long-term memory, which avoids memory explosion and minimizes performance decay for long-term prediction is developed. Combined with a new memory reading mechanism, XMem greatly exceeds state-of-the-art performance on long-video datasets while being on par with state-of-theart methods (that do not work on long videos) on short-video datasets.</li>
<li>XMem relies on semi-supervised VOS by using a first-frame annotation, provided by the user, and segments objects in all other frames as accurately as possible while preferably running in real-time, online, and while having a small memory footprint even when processing long videos.</li>
<li>VOS methods employ a feature memory to store relevant deep-net representations of an object to propogate annotations to other frames. Online learning methods use the weights of a network as their feature memory and recurrent methods propagate information often from the most recent frames, either via a mask or via a hidden representation. But these methods are prone to drifting and struggle with occlusions. Recent VOS methods store representations of past frames in the feature memory with features extracted from the newly observed query frame which needs to be segmented. Although these methods are high-performance, they require a large amount of GPU memory and struggle to handle videos longer  than a minute. Methods designed for longer videos, however, often sacrifice on segmentation quality.</li>
<li>It is proposed that this connection of performance and GPU memory consumption is a direct consequence of using a single feature memory type. Instead a unified memory architecture, dubbed XMem, is proposed. XMem maintains three independent yet deeply-connected feature memory stores: a rapidly updated sensory memory, a high-resolution working memory, and a compact thus sustained long-term memory. The sensory memory corresponds to the hidden representation of a GRU which is updated every frame. It provides temporal smoothness but fails for long-term prediction due to representation drift. To complement, the working memory is agglomerated from a subset of historical frames and considers them equally without drifting over time. To control the size of the working memory, XMem routinely consolidates its representations into the long-term memory, inspired by the consolidation mechanism in the human memory. XMem stores long-term memory as a set of highly compact prototypes. For this, they develop a memory potentiation algorithm that aggregates richer information into these prototypes to prevent aliasing due to sub-sampling. To read from the working and long-term memory, they devise a space-time memory reading operation. The three feature memory stores combined permit handling long videos with high accuracy while keeping GPU memory usage low.<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/1ec9b7e8-8200-4a2a-852a-bd6a558cae0d" /></li>
<li>Given the image and target object mask at the first frame, XMem tracks the object and generates corresponding masks for subsequent query frames. For this, it first initializes the different feature memory stores using the inputs. For each subsequent query frame, it performs memory reading from long-term memory, working memory, and sensory memory respectively. The readout features are used to generate a segmentation mask. Then, it updates each of the feature memory stores at different frequencies. It updates the sensory memory every frame and inserts features into the working memory at every r-th frame. When the working memory reaches a pre-defined maximum of Tmax frames, it consolidates features from the working memory into the long-term memory in a highly compact form. When the long-term memory is also full (which only happens after processing thousands of frames), it discards obsolete features to bound the maximum GPU memory usage. These feature memory stores work in conjunction to provide high-quality features with low GPU memory usage even for very long videos. <img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/d7b6f519-ce0f-4bcb-9c8e-7beb2dcf6ef3" /></li>
<li>XMem consists of three end-to-end trainable convolutional networks as shown in Figure 3: a query encoder that extracts query-specific image features, a decoder that takes the output of the memory reading step to generate an object mask, and a value encoder that combines the image with the object mask to extract new memory features. See Section 3.6 for details of these networks. In the following, we will first describe the memory reading operation before discussing each feature memory store in detail.</li>
<li>The method sometimes fails when the target object moves too quickly or has severe motion blur as even the fastest updating sensory memory cannot catch up.  </li>
<li>Comparisons:<br />
<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/107a01c1-cd91-4cac-a5f7-d6eb4b783d43" /><br />
<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/f1fc5dbd-09c0-478c-8596-7ee4f139e47c" /><br />
<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/c5794ae8-48da-4eed-8f5a-c06fa5f97603" /></li>
<li>Failure Cases: <img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/58af7de8-c761-409e-976d-5a3705c95823" /></li>
</ol>
</li>
</ol>
<h3 id="maskfreevis">MaskFreeVIS:<a class="headerlink" href="#maskfreevis" title="Permanent link">&para;</a></h3>
<ol>
<li>The recent advancement in Video Instance Segmentation (VIS) has largely been driven by the use of deeper and increasingly data-hungry transformer-based models. However, video masks are tedious and expensive to annotate, limiting the scale and diversity of existing VIS datasets. The solution proposed is MaskFreeVIS, which aims to remove the mask-annotation requirement, achieving highly competitive VIS performance, while only using bounding box annotations for the object state. It leverages the rich temporal mask consistency constraints in videos by introducing the Temporal KNN-patch Loss (TK-Loss), providing strong mask supervision without any labels. TK-Loss finds one-to-many matches across frames, through an efficient patch-matching step followed by a K-nearest neighbor selection. A consistency loss is then enforced on the found matches. The mask-free objective is simple to implement, has no trainable parameters, is computationally efficient, yet outperforms baselines employing, e.g., state-of-the-art optical flow to enforce temporal mask consistency. MaskFreeVIS is validated on the YouTube-VIS 2019/2021, OVIS and BDD100K MOTS benchmarks. The results clearly demonstrate the efficacy of the method by drastically narrowing the gap between fully and weakly-supervised VIS performance.</li>
<li>MaskFreeVIS has high-performing video instance segmentation without using any video masks or even image mask labels. Using SwinL and built on Mask2Former, MaskFreeVIS achieved 56.0 AP on YTVIS without using any video masks labels. Using ResNet-101, MaskFreeVIS achieves 49.1 AP without using video masks, and 47.3 AP only using COCO mask initialized model.</li>
<li>A new parameter-free Temporal KNN-patch Loss (TK-Loss), which leverages temporal masks consistency using unsupervised one-to-k patch correspondence is what MaskFreeVIS uses. TK-Loss is flexible to intergrated with state-of-the-art transformer-based VIS models, with no trainable parameters.</li>
<li>
<h4 id="mask-free-video-instance-segmentation"><a href="https://arxiv.org/pdf/2303.15904.pdf">Mask-Free Video Instance Segmentation</a>:<a class="headerlink" href="#mask-free-video-instance-segmentation" title="Permanent link">&para;</a></h4>
<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/4d06848e-defb-41cf-ade2-5ecce8d517fa" /><ol>
<li>Video Instance Segmentation (VIS) requires jointly detecting, tracking and segmenting all objects in a video from a given set of categories. State-of-the-art VIS models are trained with complete video annotations from VIS datasets. However, video annotation is costly, in particular regarding objectmask labels. Even coarse polygon-based mask annotation is multiple times slower than annotating video bounding boxes. This makes existing VIS benchmarks difficult to scale, limiting the number of object categories covered, especially for recent transformer based VIS models.</li>
<li>Current box-supervised instance segmentation models are created for images and achieve low accuracy when applied to videos because they do not utilize temporal cues.</li>
<li>Instead, the MaskFreeVIS method is proposed, for high performance VIS without any mask annotations. To leverage temporal mask consistency, the Temporal KNN-patch Loss (TK-Loss) is introduced. To find regions corresponding to the same underlying video object, TK-Loss first builds correspondences across frames by patch-wise matching. For each target patch, only the top K matches in the neighboring frame with high enough matching score are selected. A temporal consistency loss is then applied to all found matches to promote the mask consistency. Specifically, the surrogate objective function not only promotes the one-to-k matched regions to reach the same mask probabilities, but also commits their mask prediction to a confident foreground or background prediction by entropy minimization.</li>
<li>TK-Loss simply replaces the conventional video mask losses in supervising video mask generation. To further enforce temporal consistency through the video clip, TK-Loss is employed in a cyclic manner instead of using dense frame-wise connections. This greatly reduces memory cost with negligible performance drop. <img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/9c828018-2e73-495c-af10-8cf47769216f" /></li>
<li>MaskFreeVIS achieves competitive VIS performance without using any video masks or even image mask labels on all datasets. Validated on various methods and backbones, MaskFreeVIS achieves 91.25% performance of its fully supervised counterparts, even outperforming a few recent fully-supervised methods on the popular YTVIS benchmark.<br />
<img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/f4397575-b1ec-4391-9a01-f75b07016ad1" />    <img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/0d8cc43a-678f-4189-a759-3ae10c4c7f3c" /> <img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/1edd17e0-cdcf-4653-95bf-2241903d6b18" /></li>
</ol>
</li>
</ol>
<h3 id="yolo8">YOLO8:<a class="headerlink" href="#yolo8" title="Permanent link">&para;</a></h3>
<ol>
<li>Ultralytics YOLOv8 is the latest version of the acclaimed real-time object detection and image segmentation model. YOLOv8 is built on cutting-edge advancements in deep learning and computer vision, offering unparalleled performance in terms of speed and accuracy. Its streamlined design makes it suitable for various applications and easily adaptable to different hardware platforms, from edge devices to cloud APIs.</li>
<li>Using YOLOv8 is as simple as installing <code>ultralytics</code> with pip and get it up and running in minutes. YOLOv8 helps you predict new images and videos. You can also train a new YOLOv8 model on your own custom dataset or use a pre-trained model. YOLOv8 also provides tasks like segment, classify, pose and track.</li>
<li>YOLO's History:<ol>
<li>YOLO (You Only Look Once), a popular object detection and image segmentation model, was developed by Joseph Redmon and Ali Farhadi at the University of Washington. Launched in 2015, YOLO quickly gained popularity     for its high speed and accuracy.</li>
<li>YOLOv2, released in 2016, improved the original model by incorporating batch normalization, anchor boxes, and dimension clusters.</li>
<li>YOLOv3, launched in 2018, further enhanced the model's performance using a more efficient backbone network, multiple anchors and spatial pyramid pooling.</li>
<li>YOLOv4 was released in 2020, introducing innovations like Mosaic data augmentation, a new anchor-free detection head, and a new loss function.</li>
<li>YOLOv5 further improved the model's performance and added new features such as hyperparameter optimization, integrated experiment tracking and automatic export to popular export formats.</li>
<li>YOLOv6 was open-sourced by Meituan in 2022 and is in use in many of the company's autonomous delivery robots.</li>
<li>YOLOv7 added additional tasks such as pose estimation on the COCO keypoints dataset.</li>
<li>YOLOv8 is the latest version of YOLO by Ultralytics. As a cutting-edge, state-of-the-art (SOTA) model, YOLOv8 builds on the success of previous versions, introducing new features and improvements for enhanced         performance, flexibility, and efficiency. YOLOv8 supports a full range of vision AI tasks, including detection, segmentation, pose estimation, tracking, and classification. This versatility allows users to leverage      YOLOv8's capabilities across diverse applications and domains.</li>
</ol>
</li>
<li>YOLOv8 is an AI framework that supports multiple computer vision tasks. The framework can be used to perform detection, segmentation, classification, and pose estimation. Each of these tasks has a different objective and use case. <img alt="image" src="https://github.com/agoel11/KEYS2023/assets/81878922/4d4d480d-5f49-4cfa-b095-3b1c33770006" /><ol>
<li>Detection is the primary task supported by YOLOv8. It involves detecting objects in an image or video frame and drawing bounding boxes around them. The detected objects are classified into different categories        based on their features. YOLOv8 can detect multiple objects in a single image or video frame with high accuracy and speed.</li>
<li>Segmentation is a task that involves segmenting an image into different regions based on the content of the image. Each region is assigned a label based on its content. This task is useful in applications such as     image segmentation and medical imaging. YOLOv8 uses a variant of the U-Net architecture to perform segmentation.</li>
<li>Classification is a task that involves classifying an image into different categories. YOLOv8 can be used to classify images based on their content. It uses a variant of the EfficientNet architecture to perform       classification.</li>
<li>Pose/keypoint detection is a task that involves detecting specific points in an image or video frame. These points are referred to as keypoints and are used to track movement or pose estimation. YOLOv8 can detect     keypoints in an image or video frame with high accuracy and speed.</li>
</ol>
</li>
</ol>
<h2 id="matrix-comparison">Matrix &amp; Comparison<a class="headerlink" href="#matrix-comparison" title="Permanent link">&para;</a></h2>
<p>All qualitative fields will be marked on a scale of 1-5 with 1 representing a worse rating and 5 representing a great rating. The videos used to test are uploaded below. </p>
<ol>
<li><a href="https://github.com/agoel11/KEYS2023/assets/81878922/b17514ec-2046-4e6c-a591-e2825c89a488">Single-Human Video</a> (SH, 2.84 MB, 28 sec.)<ol>
<li><a href="https://github.com/agoel11/KEYS2023/assets/81878922/ae326e95-2013-4409-9c7b-f78da93db1ba">DeepLabCut</a></li>
<li><a href="https://github.com/agoel11/KEYS2023/assets/81878922/aaaf21c3-f634-40cd-8093-ba3be816340d">Track-Anything</a></li>
<li><a href="https://github.com/agoel11/KEYS2023/assets/81878922/3d7527fa-eeae-4ddb-a962-1410be1ea4f3">YOLOv8</a></li>
</ol>
</li>
<li><a href="https://github.com/agoel11/KEYS2023/assets/81878922/79b3624b-be38-4f2f-9d06-f1989ccf3e52">Multi-Human Video</a> (MH, 2.07 MB, 14 sec.)<ol>
<li><a href="https://github.com/agoel11/KEYS2023/assets/81878922/dd71a0ae-a10c-4306-a3ce-1801d2b3960d">DeepLabCut</a></li>
<li><a href="https://github.com/agoel11/KEYS2023/assets/81878922/4c182174-967c-4090-b240-406643003cd1">Track-Anything</a></li>
<li><a href="https://github.com/agoel11/KEYS2023/assets/81878922/8cab6b57-08b6-42c9-b783-d50e446774f1">YOLOv8</a></li>
</ol>
</li>
<li><a href="https://github.com/agoel11/KEYS2023/assets/81878922/92f3eba9-1a4f-40ab-9cea-bd5c85e92a6d">Single-Animal Video</a> (SA, 742 KB, 12 sec.)<ol>
<li><a href="https://github.com/agoel11/KEYS2023/assets/81878922/865add2f-2b3e-4bb5-9fe3-6627e3ef1f34">DeepLabCut</a></li>
<li><a href="https://github.com/agoel11/KEYS2023/assets/81878922/dee56854-13c9-494a-a9be-613d1917b5e1">Track-Anything</a></li>
<li><a href="https://github.com/agoel11/KEYS2023/assets/81878922/d72d9d80-feaf-4d0d-9428-bd06c6592de2">YOLOv8</a></li>
</ol>
</li>
<li><a href="https://github.com/agoel11/KEYS2023/assets/81878922/df2f5f22-2d05-4c7e-9509-49c46db1779f">Multi-Animal Video</a> (MA, 6.35 MB, 13 sec.)<ol>
<li><a href="https://github.com/agoel11/KEYS2023/assets/81878922/1645742d-691d-4647-8324-04f29499d519">DeepLabCut</a></li>
<li><a href="https://github.com/agoel11/KEYS2023/assets/81878922/b33a73da-1d03-4a89-b7c3-203d997615fc">Track-Anything</a></li>
<li><a href="https://github.com/agoel11/KEYS2023/assets/81878922/1bf8f0ad-b8f1-4654-afd8-c8e24e68da6d">YOLOv8</a></li>
</ol>
</li>
</ol>
<table>
<thead>
<tr>
<th>Method</th>
<th>Use Case</th>
<th>Test Link</th>
<th>Number of Papers Using Tool</th>
<th>Activeness</th>
<th>Total Time to Test</th>
<th>Processing Time</th>
<th>Ease of Use</th>
<th>Efficiency of Single-Human Segmentation</th>
<th>Efficiency of Multi-Human Segmentation</th>
<th>Efficiency of Single-Animal Segmentation</th>
<th>Efficiency of Multi-Animal Segmentation</th>
</tr>
</thead>
<tbody>
<tr>
<td>DeepLabCut</td>
<td>Trained Pose Estimation/ Tracking</td>
<td><a href="http://bit.ly/DLCMZ">http://bit.ly/DLCMZ</a></td>
<td>3310</td>
<td>5</td>
<td>SH - 6m <br />MH - 5m <br />SA - 5m <br />MA - 5m</td>
<td>SH - 1m <br />MH - 1m <br />SA - 56s <br />MA - 56s</td>
<td>4</td>
<td>4</td>
<td>2</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>Track-Anything</td>
<td>Semi-supervised Segmentation/ Tracking</td>
<td><a href="https://bit.ly/TrAn">https://bit.ly/TrAn</a></td>
<td>3</td>
<td>4</td>
<td>SH - 3m <br />MH - 2m <br />SA - 2m <br />MA - 3m</td>
<td>SH - 2m <br />MH - 1m <br />SA - 52s <br />MA - 1m</td>
<td>5</td>
<td>4</td>
<td>5</td>
<td>5</td>
<td>4</td>
</tr>
<tr>
<td>XMem</td>
<td>Trained Segmentation/ Tracking</td>
<td><a href="https://xxxxxx/xxxx">https://xxxxxx/xxxx</a></td>
<td>1500</td>
<td>1</td>
<td>SH - 0m <br />MH - 0m <br />SA - 0m <br />MA - 0m</td>
<td>SH - 0m <br />MH - 0m <br />SA - 0m <br />MA - 0m</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>MaskFreeVIS</td>
<td>Trained Segmentation/ Tracking</td>
<td><a href="https://xxxxxx/xxxx">https://xxxxxx/xxxx</a></td>
<td>2</td>
<td>1</td>
<td>SH - 0m <br />MH - 0m <br />SA - 0m <br />MA - 0m</td>
<td>SH - 0m <br />MH - 0m <br />SA - 0m <br />MA - 0m</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>YOLOv8</td>
<td>Trained Pose Estimation/ Tracking</td>
<td><a href="https://bit.ly/YOLO8">https://bit.ly/YOLO8</a></td>
<td>382</td>
<td>5</td>
<td>SH - 2m <br />MH - 2m <br />SA - 3m <br />MA - 5m</td>
<td>SH - 1m <br />MH - 45s <br />SA - 1m <br />MA - 1m</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>2</td>
<td>1</td>
</tr>
</tbody>
</table>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="Footer">
        
          <a href="../priorresearch/" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Prior Research/ Fact Finding
              </div>
            </div>
          </a>
        
        
          <a href="../poster/" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Poster
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2023 - 2024 Atharva Goel
          </div>
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
  <div class="md-footer-social">
    
      
      
        
        
      
      <a href="https://github.com/agoel11/KEYS2023.git" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/vendor.6a3d08fc.min.js"></script>
      <script src="../assets/javascripts/bundle.71201edf.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}</script>
      
      <script>
        app = initialize({
          base: "..",
          features: [],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.4ac00218.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
    
  </body>
</html>